
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>pyspark.sql module &mdash; PySpark 1.2-SNAPSHOT documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '1.2-SNAPSHOT',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="PySpark 1.2-SNAPSHOT documentation" href="index.html" />
    <link rel="next" title="pyspark.streaming module" href="pyspark.streaming.html" />
    <link rel="prev" title="pyspark.mllib package" href="pyspark.mllib.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="pyspark.streaming.html" title="pyspark.streaming module"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="pyspark.mllib.html" title="pyspark.mllib package"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">PySpark 1.2-SNAPSHOT documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="pyspark-sql-module">
<h1>pyspark.sql module<a class="headerlink" href="#pyspark-sql-module" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-pyspark.sql">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-pyspark.sql" title="Permalink to this headline">¶</a></h2>
<p>public classes of Spark SQL:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#pyspark.sql.SQLContext" title="pyspark.sql.SQLContext"><tt class="xref py py-class docutils literal"><span class="pre">SQLContext</span></tt></a>
Main entry point for SQL functionality.</li>
<li><a class="reference internal" href="#pyspark.sql.SchemaRDD" title="pyspark.sql.SchemaRDD"><tt class="xref py py-class docutils literal"><span class="pre">SchemaRDD</span></tt></a>
A Resilient Distributed Dataset (RDD) with Schema information for the data contained. In
addition to normal RDD operations, SchemaRDDs also support SQL.</li>
<li><a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>
A Row of data returned by a Spark SQL query.</li>
<li><a class="reference internal" href="#pyspark.sql.HiveContext" title="pyspark.sql.HiveContext"><tt class="xref py py-class docutils literal"><span class="pre">HiveContext</span></tt></a>
Main entry point for accessing data stored in Apache Hive..</li>
</ul>
</div></blockquote>
<dl class="class">
<dt id="pyspark.sql.StringType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">StringType</tt><a class="reference internal" href="_modules/pyspark/sql.html#StringType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.StringType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.PrimitiveType</span></tt></p>
<p>Spark SQL StringType</p>
<p>The data type representing string values.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.BinaryType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">BinaryType</tt><a class="reference internal" href="_modules/pyspark/sql.html#BinaryType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.BinaryType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.PrimitiveType</span></tt></p>
<p>Spark SQL BinaryType</p>
<p>The data type representing bytearray values.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.BooleanType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">BooleanType</tt><a class="reference internal" href="_modules/pyspark/sql.html#BooleanType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.BooleanType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.PrimitiveType</span></tt></p>
<p>Spark SQL BooleanType</p>
<p>The data type representing bool values.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DateType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DateType</tt><a class="reference internal" href="_modules/pyspark/sql.html#DateType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DateType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.PrimitiveType</span></tt></p>
<p>Spark SQL DateType</p>
<p>The data type representing datetime.date values.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.TimestampType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">TimestampType</tt><a class="reference internal" href="_modules/pyspark/sql.html#TimestampType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.TimestampType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.PrimitiveType</span></tt></p>
<p>Spark SQL TimestampType</p>
<p>The data type representing datetime.datetime values.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DecimalType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DecimalType</tt><big>(</big><em>precision=None</em>, <em>scale=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#DecimalType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DecimalType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataType</span></tt></p>
<p>Spark SQL DecimalType</p>
<p>The data type representing decimal.Decimal values.</p>
<dl class="method">
<dt id="pyspark.sql.DecimalType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#DecimalType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DecimalType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DoubleType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DoubleType</tt><a class="reference internal" href="_modules/pyspark/sql.html#DoubleType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.DoubleType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.PrimitiveType</span></tt></p>
<p>Spark SQL DoubleType</p>
<p>The data type representing float values.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.FloatType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">FloatType</tt><a class="reference internal" href="_modules/pyspark/sql.html#FloatType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.FloatType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.PrimitiveType</span></tt></p>
<p>Spark SQL FloatType</p>
<p>The data type representing single precision floating-point values.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.ByteType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">ByteType</tt><a class="reference internal" href="_modules/pyspark/sql.html#ByteType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.ByteType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.PrimitiveType</span></tt></p>
<p>Spark SQL ByteType</p>
<p>The data type representing int values with 1 singed byte.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.IntegerType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">IntegerType</tt><a class="reference internal" href="_modules/pyspark/sql.html#IntegerType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.IntegerType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.PrimitiveType</span></tt></p>
<p>Spark SQL IntegerType</p>
<p>The data type representing int values.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.LongType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">LongType</tt><a class="reference internal" href="_modules/pyspark/sql.html#LongType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.LongType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.PrimitiveType</span></tt></p>
<p>Spark SQL LongType</p>
<p>The data type representing long values. If the any value is
beyond the range of [-9223372036854775808, 9223372036854775807],
please use DecimalType.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.ShortType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">ShortType</tt><a class="reference internal" href="_modules/pyspark/sql.html#ShortType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.ShortType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.PrimitiveType</span></tt></p>
<p>Spark SQL ShortType</p>
<p>The data type representing int values with 2 signed bytes.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.ArrayType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">ArrayType</tt><big>(</big><em>elementType</em>, <em>containsNull=True</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#ArrayType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.ArrayType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataType</span></tt></p>
<p>Spark SQL ArrayType</p>
<p>The data type representing list values. An ArrayType object
comprises two fields, elementType (a DataType) and containsNull (a bool).
The field of elementType is used to specify the type of array elements.
The field of containsNull is used to specify if the array has None values.</p>
<dl class="classmethod">
<dt id="pyspark.sql.ArrayType.fromJson">
<em class="property">classmethod </em><tt class="descname">fromJson</tt><big>(</big><em>json</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#ArrayType.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.ArrayType.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.ArrayType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#ArrayType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.ArrayType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.MapType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">MapType</tt><big>(</big><em>keyType</em>, <em>valueType</em>, <em>valueContainsNull=True</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#MapType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.MapType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataType</span></tt></p>
<p>Spark SQL MapType</p>
<p>The data type representing dict values. A MapType object comprises
three fields, keyType (a DataType), valueType (a DataType) and
valueContainsNull (a bool).</p>
<p>The field of keyType is used to specify the type of keys in the map.
The field of valueType is used to specify the type of values in the map.
The field of valueContainsNull is used to specify if values of this
map has None values.</p>
<p>For values of a MapType column, keys are not allowed to have None values.</p>
<dl class="classmethod">
<dt id="pyspark.sql.MapType.fromJson">
<em class="property">classmethod </em><tt class="descname">fromJson</tt><big>(</big><em>json</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#MapType.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.MapType.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.MapType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#MapType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.MapType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.StructField">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">StructField</tt><big>(</big><em>name</em>, <em>dataType</em>, <em>nullable=True</em>, <em>metadata=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#StructField"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.StructField" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataType</span></tt></p>
<p>Spark SQL StructField</p>
<p>Represents a field in a StructType.
A StructField object comprises three fields, name (a string),
dataType (a DataType) and nullable (a bool). The field of name
is the name of a StructField. The field of dataType specifies
the data type of a StructField.</p>
<p>The field of nullable specifies if values of a StructField can
contain None values.</p>
<dl class="classmethod">
<dt id="pyspark.sql.StructField.fromJson">
<em class="property">classmethod </em><tt class="descname">fromJson</tt><big>(</big><em>json</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#StructField.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.StructField.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.StructField.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#StructField.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.StructField.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.StructType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">StructType</tt><big>(</big><em>fields</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#StructType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.StructType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataType</span></tt></p>
<p>Spark SQL StructType</p>
<p>The data type representing rows.
A StructType object comprises a list of <a class="reference internal" href="#pyspark.sql.StructField" title="pyspark.sql.StructField"><tt class="xref py py-class docutils literal"><span class="pre">StructField</span></tt></a>.</p>
<dl class="classmethod">
<dt id="pyspark.sql.StructType.fromJson">
<em class="property">classmethod </em><tt class="descname">fromJson</tt><big>(</big><em>json</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#StructType.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.StructType.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.StructType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#StructType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.StructType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.SQLContext">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">SQLContext</tt><big>(</big><em>sparkContext</em>, <em>sqlContext=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SQLContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>Main entry point for Spark SQL functionality.</p>
<p>A SQLContext can be used create <a class="reference internal" href="#pyspark.sql.SchemaRDD" title="pyspark.sql.SchemaRDD"><tt class="xref py py-class docutils literal"><span class="pre">SchemaRDD</span></tt></a>, register <a class="reference internal" href="#pyspark.sql.SchemaRDD" title="pyspark.sql.SchemaRDD"><tt class="xref py py-class docutils literal"><span class="pre">SchemaRDD</span></tt></a> as
tables, execute SQL over tables, cache tables, and read parquet files.</p>
<dl class="method">
<dt id="pyspark.sql.SQLContext.applySchema">
<tt class="descname">applySchema</tt><big>(</big><em>rdd</em>, <em>schema</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SQLContext.applySchema"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.applySchema" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the given schema to the given RDD of <tt class="xref py py-class docutils literal"><span class="pre">tuple</span></tt> or <tt class="xref py py-class docutils literal"><span class="pre">list</span></tt>.</p>
<p>These tuples or lists can contain complex nested structures like
lists, maps or nested rows.</p>
<p>The schema should be a StructType.</p>
<p>It is important that the schema matches the types of the objects
in each row or exceptions could be thrown at runtime.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd2</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="s">&quot;row1&quot;</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s">&quot;row2&quot;</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s">&quot;row3&quot;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s">&quot;field1&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;field2&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">applySchema</span><span class="p">(</span><span class="n">rdd2</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerRDDAsTable</span><span class="p">(</span><span class="n">srdd</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd2</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT * from table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd2</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(field1=1, field2=u&#39;row1&#39;),..., Row(field1=3, field2=u&#39;row3&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">date</span><span class="p">,</span> <span class="n">datetime</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([(</span><span class="mi">127</span><span class="p">,</span> <span class="o">-</span><span class="il">128L</span><span class="p">,</span> <span class="o">-</span><span class="mi">32768</span><span class="p">,</span> <span class="mi">32767</span><span class="p">,</span> <span class="il">2147483647L</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">date</span><span class="p">(</span><span class="mi">2010</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">datetime</span><span class="p">(</span><span class="mi">2010</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s">&quot;a&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="bp">None</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;byte1&quot;</span><span class="p">,</span> <span class="n">ByteType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;byte2&quot;</span><span class="p">,</span> <span class="n">ByteType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;short1&quot;</span><span class="p">,</span> <span class="n">ShortType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;short2&quot;</span><span class="p">,</span> <span class="n">ShortType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;int&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;float&quot;</span><span class="p">,</span> <span class="n">FloatType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;date&quot;</span><span class="p">,</span> <span class="n">DateType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;time&quot;</span><span class="p">,</span> <span class="n">TimestampType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;map&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">MapType</span><span class="p">(</span><span class="n">StringType</span><span class="p">(),</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;struct&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s">&quot;b&quot;</span><span class="p">,</span> <span class="n">ShortType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">)]),</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;list&quot;</span><span class="p">,</span> <span class="n">ArrayType</span><span class="p">(</span><span class="n">ByteType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;null&quot;</span><span class="p">,</span> <span class="n">DoubleType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">applySchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span> <span class="o">=</span> <span class="n">srdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
<span class="gp">... </span>    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">byte1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">byte2</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">short1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">short2</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">date</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">x</span><span class="o">.</span><span class="n">time</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">map</span><span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">struct</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">list</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">null</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">results</span><span class="o">.</span><span class="n">collect</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> 
<span class="go">(127, -128, -32768, 32767, 2147483647, 1.0, datetime.date(2010, 1, 1),</span>
<span class="go">     datetime.datetime(2010, 1, 1, 1, 1, 1), 1, 2, [1, 2, 3], None)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s">&quot;table2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span>
<span class="gp">... </span>  <span class="s">&quot;SELECT byte1 - 1 AS byte1, byte2 + 1 AS byte2, &quot;</span> <span class="o">+</span>
<span class="gp">... </span>    <span class="s">&quot;short1 + 1 AS short1, short2 - 1 AS short2, int - 1 AS int, &quot;</span> <span class="o">+</span>
<span class="gp">... </span>    <span class="s">&quot;float + 1.5 as float FROM table2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(byte1=126, byte2=-127, short1=-32767, short2=32766, int=2147483646, float=2.5)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([(</span><span class="mi">127</span><span class="p">,</span> <span class="o">-</span><span class="mi">32768</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">datetime</span><span class="p">(</span><span class="mi">2010</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s">&quot;a&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">abstract</span> <span class="o">=</span> <span class="s">&quot;byte short float time map{} struct(b) list[]&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">_parse_schema_abstract</span><span class="p">(</span><span class="n">abstract</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">typedSchema</span> <span class="o">=</span> <span class="n">_infer_schema_type</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">first</span><span class="p">(),</span> <span class="n">schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">applySchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">typedSchema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(byte=127, short=-32768, float=1.0, time=..., list=[1, 2, 3])]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.cacheTable">
<tt class="descname">cacheTable</tt><big>(</big><em>tableName</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SQLContext.cacheTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.cacheTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Caches the specified table in-memory.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.inferSchema">
<tt class="descname">inferSchema</tt><big>(</big><em>rdd</em>, <em>samplingRatio=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SQLContext.inferSchema"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.inferSchema" title="Permalink to this definition">¶</a></dt>
<dd><p>Infer and apply a schema to an RDD of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>.</p>
<p>When samplingRatio is specified, the schema is inferred by looking
at the types of each row in the sampled dataset. Otherwise, the
first 100 rows of the RDD are inspected. Nested collections are
supported, which can include array, dict, list, Row, tuple,
namedtuple, or object.</p>
<p>Each row could be <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.Row</span></tt></a> object or namedtuple or objects.
Using top level dicts is deprecated, as dict is used to represent Maps.</p>
<p>If a single column has multiple distinct inferred types, it may cause
runtime exceptions.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="n">Row</span><span class="p">(</span><span class="n">field1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">field2</span><span class="o">=</span><span class="s">&quot;row1&quot;</span><span class="p">),</span>
<span class="gp">... </span>     <span class="n">Row</span><span class="p">(</span><span class="n">field1</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">field2</span><span class="o">=</span><span class="s">&quot;row2&quot;</span><span class="p">),</span>
<span class="gp">... </span>     <span class="n">Row</span><span class="p">(</span><span class="n">field1</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">field2</span><span class="o">=</span><span class="s">&quot;row3&quot;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">collect</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">Row(field1=1, field2=u&#39;row1&#39;)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">NestedRow</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s">&quot;f1&quot;</span><span class="p">,</span> <span class="s">&quot;f2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nestedRdd1</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">NestedRow</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="s">&#39;i&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="p">{</span><span class="s">&quot;row1&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}),</span>
<span class="gp">... </span>    <span class="n">NestedRow</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="s">&#39;i&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="p">{</span><span class="s">&quot;row2&quot;</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">})])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">nestedRdd1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f1=[1, 2], f2={u&#39;row1&#39;: 1.0}), ..., f2={u&#39;row2&#39;: 2.0})]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">nestedRdd2</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">NestedRow</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
<span class="gp">... </span>    <span class="n">NestedRow</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">nestedRdd2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f1=[[1, 2], [2, 3]], f2=[1, 2]), ..., f2=[2, 3])]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.jsonFile">
<tt class="descname">jsonFile</tt><big>(</big><em>path</em>, <em>schema=None</em>, <em>samplingRatio=1.0</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SQLContext.jsonFile"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.jsonFile" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a text file storing one JSON object per line as a
<a class="reference internal" href="#pyspark.sql.SchemaRDD" title="pyspark.sql.SchemaRDD"><tt class="xref py py-class docutils literal"><span class="pre">SchemaRDD</span></tt></a>.</p>
<p>If the schema is provided, applies the given schema to this
JSON dataset.</p>
<p>Otherwise, it samples the dataset with ratio <cite>samplingRatio</cite> to
determine the schema.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tempfile</span><span class="o">,</span> <span class="nn">shutil</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jsonFile</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">jsonFile</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ofn</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">jsonFile</span><span class="p">,</span> <span class="s">&#39;w&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">json</span> <span class="ow">in</span> <span class="n">jsonStrings</span><span class="p">:</span>
<span class="gp">... </span>  <span class="k">print</span><span class="o">&gt;&gt;</span><span class="n">ofn</span><span class="p">,</span> <span class="n">json</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ofn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd1</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">jsonFile</span><span class="p">(</span><span class="n">jsonFile</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerRDDAsTable</span><span class="p">(</span><span class="n">srdd1</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd2</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span>
<span class="gp">... </span>  <span class="s">&quot;SELECT field1 AS f1, field2 as f2, field3 as f3, &quot;</span>
<span class="gp">... </span>  <span class="s">&quot;field6 as f4 from table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">srdd2</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">r</span>
<span class="go">Row(f1=1, f2=u&#39;row1&#39;, f3=Row(field4=11, field5=None), f4=None)</span>
<span class="go">Row(f1=2, f2=None, f3=Row(field4=22,..., f4=[Row(field7=u&#39;row2&#39;)])</span>
<span class="go">Row(f1=None, f2=u&#39;row3&#39;, f3=Row(field4=33, field5=[]), f4=None)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd3</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">jsonFile</span><span class="p">(</span><span class="n">jsonFile</span><span class="p">,</span> <span class="n">srdd1</span><span class="o">.</span><span class="n">schema</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerRDDAsTable</span><span class="p">(</span><span class="n">srdd3</span><span class="p">,</span> <span class="s">&quot;table2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd4</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span>
<span class="gp">... </span>  <span class="s">&quot;SELECT field1 AS f1, field2 as f2, field3 as f3, &quot;</span>
<span class="gp">... </span>  <span class="s">&quot;field6 as f4 from table2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">srdd4</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
<span class="gp">... </span>   <span class="k">print</span> <span class="n">r</span>
<span class="go">Row(f1=1, f2=u&#39;row1&#39;, f3=Row(field4=11, field5=None), f4=None)</span>
<span class="go">Row(f1=2, f2=None, f3=Row(field4=22,..., f4=[Row(field7=u&#39;row2&#39;)])</span>
<span class="go">Row(f1=None, f2=u&#39;row3&#39;, f3=Row(field4=33, field5=[]), f4=None)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;field2&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;field3&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">StructType</span><span class="p">([</span>
<span class="gp">... </span>            <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;field5&quot;</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">ArrayType</span><span class="p">(</span><span class="n">IntegerType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span> <span class="bp">True</span><span class="p">)]),</span> <span class="bp">False</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd5</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">jsonFile</span><span class="p">(</span><span class="n">jsonFile</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerRDDAsTable</span><span class="p">(</span><span class="n">srdd5</span><span class="p">,</span> <span class="s">&quot;table3&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd6</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span>
<span class="gp">... </span>  <span class="s">&quot;SELECT field2 AS f1, field3.field5 as f2, &quot;</span>
<span class="gp">... </span>  <span class="s">&quot;field3.field5[0] as f3 from table3&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd6</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f1=u&#39;row1&#39;, f2=None, f3=None)...Row(f1=u&#39;row3&#39;, f2=[], f3=None)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.jsonRDD">
<tt class="descname">jsonRDD</tt><big>(</big><em>rdd</em>, <em>schema=None</em>, <em>samplingRatio=1.0</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SQLContext.jsonRDD"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.jsonRDD" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads an RDD storing one JSON object per string as a <a class="reference internal" href="#pyspark.sql.SchemaRDD" title="pyspark.sql.SchemaRDD"><tt class="xref py py-class docutils literal"><span class="pre">SchemaRDD</span></tt></a>.</p>
<p>If the schema is provided, applies the given schema to this
JSON dataset.</p>
<p>Otherwise, it samples the dataset with ratio <cite>samplingRatio</cite> to
determine the schema.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd1</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">jsonRDD</span><span class="p">(</span><span class="n">json</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerRDDAsTable</span><span class="p">(</span><span class="n">srdd1</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd2</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span>
<span class="gp">... </span>  <span class="s">&quot;SELECT field1 AS f1, field2 as f2, field3 as f3, &quot;</span>
<span class="gp">... </span>  <span class="s">&quot;field6 as f4 from table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">srdd2</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">r</span>
<span class="go">Row(f1=1, f2=u&#39;row1&#39;, f3=Row(field4=11, field5=None), f4=None)</span>
<span class="go">Row(f1=2, f2=None, f3=Row(field4=22..., f4=[Row(field7=u&#39;row2&#39;)])</span>
<span class="go">Row(f1=None, f2=u&#39;row3&#39;, f3=Row(field4=33, field5=[]), f4=None)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd3</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">jsonRDD</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">srdd1</span><span class="o">.</span><span class="n">schema</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerRDDAsTable</span><span class="p">(</span><span class="n">srdd3</span><span class="p">,</span> <span class="s">&quot;table2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd4</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span>
<span class="gp">... </span>  <span class="s">&quot;SELECT field1 AS f1, field2 as f2, field3 as f3, &quot;</span>
<span class="gp">... </span>  <span class="s">&quot;field6 as f4 from table2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">srdd4</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">r</span>
<span class="go">Row(f1=1, f2=u&#39;row1&#39;, f3=Row(field4=11, field5=None), f4=None)</span>
<span class="go">Row(f1=2, f2=None, f3=Row(field4=22..., f4=[Row(field7=u&#39;row2&#39;)])</span>
<span class="go">Row(f1=None, f2=u&#39;row3&#39;, f3=Row(field4=33, field5=[]), f4=None)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;field2&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;field3&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">StructType</span><span class="p">([</span>
<span class="gp">... </span>            <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;field5&quot;</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">ArrayType</span><span class="p">(</span><span class="n">IntegerType</span><span class="p">(),</span> <span class="bp">False</span><span class="p">),</span> <span class="bp">True</span><span class="p">)]),</span> <span class="bp">False</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd5</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">jsonRDD</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerRDDAsTable</span><span class="p">(</span><span class="n">srdd5</span><span class="p">,</span> <span class="s">&quot;table3&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd6</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span>
<span class="gp">... </span>  <span class="s">&quot;SELECT field2 AS f1, field3.field5 as f2, &quot;</span>
<span class="gp">... </span>  <span class="s">&quot;field3.field5[0] as f3 from table3&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd6</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f1=u&#39;row1&#39;, f2=None,...Row(f1=u&#39;row3&#39;, f2=[], f3=None)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">jsonRDD</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="s">&#39;{}&#39;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s">&#39;{&quot;key0&quot;: {&quot;key1&quot;: &quot;value1&quot;}}&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(key0=None), Row(key0=Row(key1=u&#39;value1&#39;))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">jsonRDD</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="s">&#39;{&quot;key0&quot;: null}&#39;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s">&#39;{&quot;key0&quot;: {&quot;key1&quot;: &quot;value1&quot;}}&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(key0=None), Row(key0=Row(key1=u&#39;value1&#39;))]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.parquetFile">
<tt class="descname">parquetFile</tt><big>(</big><em>path</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SQLContext.parquetFile"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.parquetFile" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a Parquet file, returning the result as a <a class="reference internal" href="#pyspark.sql.SchemaRDD" title="pyspark.sql.SchemaRDD"><tt class="xref py py-class docutils literal"><span class="pre">SchemaRDD</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tempfile</span><span class="o">,</span> <span class="nn">shutil</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parquetFile</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">parquetFile</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">saveAsParquetFile</span><span class="p">(</span><span class="n">parquetFile</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd2</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">parquetFile</span><span class="p">(</span><span class="n">parquetFile</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">srdd</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">srdd2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.registerFunction">
<tt class="descname">registerFunction</tt><big>(</big><em>name</em>, <em>f</em>, <em>returnType=StringType</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SQLContext.registerFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.registerFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a lambda function as a UDF so it can be used in SQL statements.</p>
<p>In addition to a name and the function itself, the return type can be optionally specified.
When the return type is not given it default to a string and conversion will automatically
be done.  For any other return type, the produced object must match the specified type.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerFunction</span><span class="p">(</span><span class="s">&quot;stringLengthString&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT stringLengthString(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c0=u&#39;4&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerFunction</span><span class="p">(</span><span class="s">&quot;stringLengthInt&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT stringLengthInt(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c0=4)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.registerRDDAsTable">
<tt class="descname">registerRDDAsTable</tt><big>(</big><em>rdd</em>, <em>tableName</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SQLContext.registerRDDAsTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.registerRDDAsTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers the given RDD as a temporary table in the catalog.</p>
<p>Temporary tables exist only during the lifetime of this instance of
SQLContext.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerRDDAsTable</span><span class="p">(</span><span class="n">srdd</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.sql">
<tt class="descname">sql</tt><big>(</big><em>sqlQuery</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SQLContext.sql"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.sql" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#pyspark.sql.SchemaRDD" title="pyspark.sql.SchemaRDD"><tt class="xref py py-class docutils literal"><span class="pre">SchemaRDD</span></tt></a> representing the result of the given query.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerRDDAsTable</span><span class="p">(</span><span class="n">srdd</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd2</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT field1 AS f1, field2 as f2 from table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd2</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f1=1, f2=u&#39;row1&#39;), Row(f1=2, f2=u&#39;row2&#39;), Row(f1=3, f2=u&#39;row3&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.table">
<tt class="descname">table</tt><big>(</big><em>tableName</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SQLContext.table"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.table" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the specified table as a <a class="reference internal" href="#pyspark.sql.SchemaRDD" title="pyspark.sql.SchemaRDD"><tt class="xref py py-class docutils literal"><span class="pre">SchemaRDD</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerRDDAsTable</span><span class="p">(</span><span class="n">srdd</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd2</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">srdd</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">srdd2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.uncacheTable">
<tt class="descname">uncacheTable</tt><big>(</big><em>tableName</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SQLContext.uncacheTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SQLContext.uncacheTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the specified table from the in-memory cache.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.HiveContext">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">HiveContext</tt><big>(</big><em>sparkContext</em>, <em>hiveContext=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#HiveContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.HiveContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pyspark.sql.SQLContext" title="pyspark.sql.SQLContext"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.SQLContext</span></tt></a></p>
<p>A variant of Spark SQL that integrates with data stored in Hive.</p>
<p>Configuration for Hive is read from hive-site.xml on the classpath.
It supports running both SQL and HiveQL commands.</p>
<dl class="method">
<dt id="pyspark.sql.HiveContext.hiveql">
<tt class="descname">hiveql</tt><big>(</big><em>hqlQuery</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#HiveContext.hiveql"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.HiveContext.hiveql" title="Permalink to this definition">¶</a></dt>
<dd><p>DEPRECATED: Use sql()</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.HiveContext.hql">
<tt class="descname">hql</tt><big>(</big><em>hqlQuery</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#HiveContext.hql"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.HiveContext.hql" title="Permalink to this definition">¶</a></dt>
<dd><p>DEPRECATED: Use sql()</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.SchemaRDD">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">SchemaRDD</tt><big>(</big><em>jschema_rdd</em>, <em>sql_ctx</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">pyspark.rdd.RDD</span></tt></p>
<p>An RDD of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a> objects that has an associated schema.</p>
<p>The underlying JVM object is a SchemaRDD, not a PythonRDD, so we can
utilize the relational query api exposed by Spark SQL.</p>
<p>For normal <tt class="xref py py-class docutils literal"><span class="pre">RDD</span></tt> operations (map, count, etc.) the
<a class="reference internal" href="#pyspark.sql.SchemaRDD" title="pyspark.sql.SchemaRDD"><tt class="xref py py-class docutils literal"><span class="pre">SchemaRDD</span></tt></a> is not operated on directly, as it&#8217;s underlying
implementation is an RDD composed of Java objects. Instead it is
converted to a PythonRDD in the JVM, on which Python operations can
be done.</p>
<p>This class receives raw tuples from Java but assigns a class to it in
all its data-collection methods (mapPartitionsWithIndex, collect, take,
etc) so that PySpark sees them as Row objects with named fields.</p>
<dl class="method">
<dt id="pyspark.sql.SchemaRDD.cache">
<tt class="descname">cache</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.cache"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.cache" title="Permalink to this definition">¶</a></dt>
<dd><p>Persist this RDD with the default storage level (<tt class="xref py py-class docutils literal"><span class="pre">MEMORY_ONLY_SER</span></tt>).</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.checkpoint">
<tt class="descname">checkpoint</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.checkpoint"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Mark this RDD for checkpointing. It will be saved to a file inside the
checkpoint directory set with <tt class="xref py py-class docutils literal"><span class="pre">SparkContext.setCheckpointDir()</span></tt> and
all references to its parent RDDs will be removed. This function must
be called before any job has been executed on this RDD. It is strongly
recommended that this RDD is persisted in memory, otherwise saving it
on a file will require recomputation.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.coalesce">
<tt class="descname">coalesce</tt><big>(</big><em>numPartitions</em>, <em>shuffle=False</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.coalesce"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.coalesce" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new RDD that is reduced into <cite>numPartitions</cite> partitions.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">glom</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[[1], [2, 3], [4, 5]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">coalesce</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">glom</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[[1, 2, 3, 4, 5]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.collect">
<tt class="descname">collect</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.collect"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.collect" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list that contains all of the rows in this RDD.</p>
<p>Each object in the list is a Row, the fields can be accessed as
attributes.</p>
<p>Unlike the base RDD implementation of collect, this implementation
leverages the query optimizer to perform a collect on the SchemaRDD,
which supports features such as filter pushdown.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(field1=1, field2=u&#39;row1&#39;), ..., Row(field1=3, field2=u&#39;row3&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.count">
<tt class="descname">count</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.count"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the number of elements in this RDD.</p>
<p>Unlike the base RDD implementation of count, this implementation
leverages the query optimizer to compute the count on the SchemaRDD,
which supports features such as filter pushdown.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">3L</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">count</span><span class="p">()</span> <span class="o">==</span> <span class="n">srdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.distinct">
<tt class="descname">distinct</tt><big>(</big><em>numPartitions=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.distinct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.distinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new RDD containing the distinct elements in this RDD.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">distinct</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[1, 2, 3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.getCheckpointFile">
<tt class="descname">getCheckpointFile</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.getCheckpointFile"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.getCheckpointFile" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the name of the file to which this RDD was checkpointed</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.id">
<tt class="descname">id</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.id"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.id" title="Permalink to this definition">¶</a></dt>
<dd><p>A unique ID for this RDD (within its SparkContext).</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.insertInto">
<tt class="descname">insertInto</tt><big>(</big><em>tableName</em>, <em>overwrite=False</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.insertInto"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.insertInto" title="Permalink to this definition">¶</a></dt>
<dd><p>Inserts the contents of this SchemaRDD into the specified table.</p>
<p>Optionally overwriting any existing data.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.intersection">
<tt class="descname">intersection</tt><big>(</big><em>other</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.intersection"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.intersection" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the intersection of this RDD and another one. The output will
not contain any duplicate elements, even if the input RDDs did.</p>
<p>Note that this method performs a shuffle internally.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd1</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd2</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd1</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">rdd2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[1, 2, 3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.isCheckpointed">
<tt class="descname">isCheckpointed</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.isCheckpointed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.isCheckpointed" title="Permalink to this definition">¶</a></dt>
<dd><p>Return whether this RDD has been checkpointed or not</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.limit">
<tt class="descname">limit</tt><big>(</big><em>num</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.limit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.limit" title="Permalink to this definition">¶</a></dt>
<dd><p>Limit the result count to the number specified.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(field1=1, field2=u&#39;row1&#39;), Row(field1=2, field2=u&#39;row2&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.mapPartitionsWithIndex">
<tt class="descname">mapPartitionsWithIndex</tt><big>(</big><em>f</em>, <em>preservesPartitioning=False</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.mapPartitionsWithIndex"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.mapPartitionsWithIndex" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new RDD by applying a function to each partition of this RDD,
while tracking the index of the original partition.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">splitIndex</span><span class="p">,</span> <span class="n">iterator</span><span class="p">):</span> <span class="k">yield</span> <span class="n">splitIndex</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitionsWithIndex</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="go">6</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.persist">
<tt class="descname">persist</tt><big>(</big><em>storageLevel=StorageLevel(False</em>, <em>True</em>, <em>False</em>, <em>False</em>, <em>1)</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.persist"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.persist" title="Permalink to this definition">¶</a></dt>
<dd><p>Set this RDD&#8217;s storage level to persist its values across operations
after the first time it is computed. This can only be used to assign
a new storage level if the RDD does not have a storage level set yet.
If no storage level is specified defaults to (<tt class="xref py py-class docutils literal"><span class="pre">MEMORY_ONLY_SER</span></tt>).</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="s">&quot;b&quot;</span><span class="p">,</span> <span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;c&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span><span class="o">.</span><span class="n">persist</span><span class="p">()</span><span class="o">.</span><span class="n">is_cached</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.printSchema">
<tt class="descname">printSchema</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.printSchema"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.printSchema" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints out the schema in the tree format.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.registerAsTable">
<tt class="descname">registerAsTable</tt><big>(</big><em>name</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.registerAsTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.registerAsTable" title="Permalink to this definition">¶</a></dt>
<dd><p>DEPRECATED: use registerTempTable() instead</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.registerTempTable">
<tt class="descname">registerTempTable</tt><big>(</big><em>name</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.registerTempTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.registerTempTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers this RDD as a temporary table using the given name.</p>
<p>The lifetime of this temporary table is tied to the <a class="reference internal" href="#pyspark.sql.SQLContext" title="pyspark.sql.SQLContext"><tt class="xref py py-class docutils literal"><span class="pre">SQLContext</span></tt></a>
that was used to create this SchemaRDD.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s">&quot;test&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd2</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;select * from test&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">srdd</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">srdd2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.repartition">
<tt class="descname">repartition</tt><big>(</big><em>numPartitions</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.repartition"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.repartition" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new RDD that has exactly numPartitions partitions.</p>
<p>Can increase or decrease the level of parallelism in this RDD.
Internally, this uses a shuffle to redistribute data.
If you are decreasing the number of partitions in this RDD, consider
using <cite>coalesce</cite>, which can avoid performing a shuffle.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">],</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">glom</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[[1], [2, 3], [4, 5], [6, 7]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">glom</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">glom</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">10</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.saveAsParquetFile">
<tt class="descname">saveAsParquetFile</tt><big>(</big><em>path</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.saveAsParquetFile"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.saveAsParquetFile" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the contents as a Parquet file, preserving the schema.</p>
<p>Files that are written out using this method can be read back in as
a SchemaRDD using the <a class="reference internal" href="#pyspark.sql.SQLContext.parquetFile" title="pyspark.sql.SQLContext.parquetFile"><tt class="xref py py-class docutils literal"><span class="pre">SQLContext.parquetFile</span></tt></a> method.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tempfile</span><span class="o">,</span> <span class="nn">shutil</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parquetFile</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">parquetFile</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">saveAsParquetFile</span><span class="p">(</span><span class="n">parquetFile</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd2</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">parquetFile</span><span class="p">(</span><span class="n">parquetFile</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">srdd2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">srdd</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.saveAsTable">
<tt class="descname">saveAsTable</tt><big>(</big><em>tableName</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.saveAsTable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.saveAsTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new table with the contents of this SchemaRDD.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.schema">
<tt class="descname">schema</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.schema"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.schema" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the schema of this SchemaRDD (represented by
a <a class="reference internal" href="#pyspark.sql.StructType" title="pyspark.sql.StructType"><tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt></a>).</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.schemaString">
<tt class="descname">schemaString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.schemaString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.schemaString" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the output schema in the tree format.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.subtract">
<tt class="descname">subtract</tt><big>(</big><em>other</em>, <em>numPartitions=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.subtract"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.subtract" title="Permalink to this definition">¶</a></dt>
<dd><p>Return each value in <tt class="xref py py-class docutils literal"><span class="pre">self</span></tt> that is not contained in <tt class="xref py py-class docutils literal"><span class="pre">other</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([(</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s">&quot;b&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s">&quot;b&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([(</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s">&quot;c&quot;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">[(&#39;a&#39;, 1), (&#39;b&#39;, 4), (&#39;b&#39;, 5)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.take">
<tt class="descname">take</tt><big>(</big><em>num</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.take"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.take" title="Permalink to this definition">¶</a></dt>
<dd><p>Take the first num rows of the RDD.</p>
<p>Each object in the list is a Row, the fields can be accessed as
attributes.</p>
<p>Unlike the base RDD implementation of take, this implementation
leverages the query optimizer to perform a collect on a SchemaRDD,
which supports features such as filter pushdown.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">[Row(field1=1, field2=u&#39;row1&#39;), Row(field1=2, field2=u&#39;row2&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.toJSON">
<tt class="descname">toJSON</tt><big>(</big><em>use_unicode=False</em><big>)</big><a class="headerlink" href="#pyspark.sql.SchemaRDD.toJSON" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a SchemaRDD into a MappedRDD of JSON documents; one document per row.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">srdd1</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">jsonRDD</span><span class="p">(</span><span class="n">json</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlCtx</span><span class="o">.</span><span class="n">registerRDDAsTable</span><span class="p">(</span><span class="n">srdd1</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd2</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span> <span class="s">&quot;SELECT * from table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd2</span><span class="o">.</span><span class="n">toJSON</span><span class="p">()</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s">&#39;{&quot;field1&quot;:1,&quot;field2&quot;:&quot;row1&quot;,&quot;field3&quot;:{&quot;field4&quot;:11}}&#39;</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd3</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span> <span class="s">&quot;SELECT field3.field4 from table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">srdd3</span><span class="o">.</span><span class="n">toJSON</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span> <span class="o">==</span> <span class="p">[</span><span class="s">&#39;{&quot;field4&quot;:11}&#39;</span><span class="p">,</span> <span class="s">&#39;{&quot;field4&quot;:22}&#39;</span><span class="p">,</span> <span class="s">&#39;{&quot;field4&quot;:33}&#39;</span><span class="p">]</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SchemaRDD.unpersist">
<tt class="descname">unpersist</tt><big>(</big><em>blocking=True</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#SchemaRDD.unpersist"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.SchemaRDD.unpersist" title="Permalink to this definition">¶</a></dt>
<dd><p>Mark the RDD as non-persistent, and remove all blocks for it from
memory and disk.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.Row">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">Row</tt><a class="reference internal" href="_modules/pyspark/sql.html#Row"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Row" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">tuple</span></tt></p>
<p>A row in <a class="reference internal" href="#pyspark.sql.SchemaRDD" title="pyspark.sql.SchemaRDD"><tt class="xref py py-class docutils literal"><span class="pre">SchemaRDD</span></tt></a>. The fields in it can be accessed like attributes.</p>
<p>Row can be used to create a row object by using named arguments,
the fields will be sorted by names.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">row</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&quot;Alice&quot;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span>
<span class="go">Row(age=11, name=&#39;Alice&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">row</span><span class="o">.</span><span class="n">age</span>
<span class="go">(&#39;Alice&#39;, 11)</span>
</pre></div>
</div>
<p>Row also can be used to create another Row like class, then it
could be used to create Row objects, such as</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="s">&quot;age&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span>
<span class="go">&lt;Row(name, age)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span><span class="p">(</span><span class="s">&quot;Alice&quot;</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="go">Row(name=&#39;Alice&#39;, age=11)</span>
</pre></div>
</div>
<dl class="method">
<dt id="pyspark.sql.Row.asDict">
<tt class="descname">asDict</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql.html#Row.asDict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.Row.asDict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return as an dict</p>
</dd></dl>

</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/spark-logo-hd.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">pyspark.sql module</a><ul>
<li><a class="reference internal" href="#module-pyspark.sql">Module contents</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="pyspark.mllib.html"
                        title="previous chapter">pyspark.mllib package</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="pyspark.streaming.html"
                        title="next chapter">pyspark.streaming module</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/pyspark.sql.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="pyspark.streaming.html" title="pyspark.streaming module"
             >next</a></li>
        <li class="right" >
          <a href="pyspark.mllib.html" title="pyspark.mllib package"
             >previous</a> |</li>
        <li><a href="index.html">PySpark 1.2-SNAPSHOT documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2014, Author.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>