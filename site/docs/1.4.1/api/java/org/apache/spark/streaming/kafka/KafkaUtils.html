<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!--NewPage-->
<HTML>
<HEAD>
<!-- Generated by javadoc (build 1.6.0_45) on Wed Jul 08 16:13:41 PDT 2015 -->
<TITLE>
KafkaUtils (Spark 1.4.1 JavaDoc)
</TITLE>

<META NAME="date" CONTENT="2015-07-08">

<LINK REL ="stylesheet" TYPE="text/css" HREF="../../../../../stylesheet.css" TITLE="Style">

<SCRIPT type="text/javascript">
function windowTitle()
{
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="KafkaUtils (Spark 1.4.1 JavaDoc)";
    }
}
</SCRIPT>
<NOSCRIPT>
</NOSCRIPT>

</HEAD>

<BODY BGCOLOR="white" onload="windowTitle();">
<HR>


<!-- ========= START OF TOP NAVBAR ======= -->
<A NAME="navbar_top"><!-- --></A>
<A HREF="#skip-navbar_top" title="Skip navigation links"></A>
<TABLE BORDER="0" WIDTH="100%" CELLPADDING="1" CELLSPACING="0" SUMMARY="">
<TR>
<TD COLSPAN=2 BGCOLOR="#EEEEFF" CLASS="NavBarCell1">
<A NAME="navbar_top_firstrow"><!-- --></A>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="3" SUMMARY="">
  <TR ALIGN="center" VALIGN="top">
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../../overview-summary.html"><FONT CLASS="NavBarFont1"><B>Overview</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-summary.html"><FONT CLASS="NavBarFont1"><B>Package</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#FFFFFF" CLASS="NavBarCell1Rev"> &nbsp;<FONT CLASS="NavBarFont1Rev"><B>Class</B></FONT>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-tree.html"><FONT CLASS="NavBarFont1"><B>Tree</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../../deprecated-list.html"><FONT CLASS="NavBarFont1"><B>Deprecated</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../../index-all.html"><FONT CLASS="NavBarFont1"><B>Index</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../../help-doc.html"><FONT CLASS="NavBarFont1"><B>Help</B></FONT></A>&nbsp;</TD>
  </TR>
</TABLE>
</TD>
<TD ALIGN="right" VALIGN="top" ROWSPAN=3><EM>
</EM>
</TD>
</TR>

<TR>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
&nbsp;<A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaTestUtils.html" title="class in org.apache.spark.streaming.kafka"><B>PREV CLASS</B></A>&nbsp;
&nbsp;<A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtilsPythonHelper.html" title="class in org.apache.spark.streaming.kafka"><B>NEXT CLASS</B></A></FONT></TD>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
  <A HREF="../../../../../index.html?org/apache/spark/streaming/kafka/KafkaUtils.html" target="_top"><B>FRAMES</B></A>  &nbsp;
&nbsp;<A HREF="KafkaUtils.html" target="_top"><B>NO FRAMES</B></A>  &nbsp;
&nbsp;<SCRIPT type="text/javascript">
  <!--
  if(window==top) {
    document.writeln('<A HREF="../../../../../allclasses-noframe.html"><B>All Classes</B></A>');
  }
  //-->
</SCRIPT>
<NOSCRIPT>
  <A HREF="../../../../../allclasses-noframe.html"><B>All Classes</B></A>
</NOSCRIPT>


</FONT></TD>
</TR>
<TR>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_summary">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_summary">METHOD</A></FONT></TD>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
DETAIL:&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_detail">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_detail">METHOD</A></FONT></TD>
</TR>
</TABLE>
<A NAME="skip-navbar_top"></A>
<!-- ========= END OF TOP NAVBAR ========= -->

<HR>
<!-- ======== START OF CLASS DATA ======== -->
<H2>
<FONT SIZE="-1">
org.apache.spark.streaming.kafka</FONT>
<BR>
Class KafkaUtils</H2>
<PRE>
Object
  <IMG SRC="../../../../../resources/inherit.gif" ALT="extended by "><B>org.apache.spark.streaming.kafka.KafkaUtils</B>
</PRE>
<HR>
<DL>
<DT><PRE>public class <B>KafkaUtils</B><DT>extends Object</DL>
</PRE>

<P>
<HR>

<P>

<!-- ======== CONSTRUCTOR SUMMARY ======== -->

<A NAME="constructor_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Constructor Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#KafkaUtils()">KafkaUtils</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
</TABLE>
&nbsp;
<!-- ========== METHOD SUMMARY =========== -->

<A NAME="method_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Method Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;,R&gt; 
<BR>
<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaInputDStream.html" title="class in org.apache.spark.streaming.api.java">JavaInputDStream</A>&lt;R&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createDirectStream(org.apache.spark.streaming.api.java.JavaStreamingContext, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.util.Map, java.util.Map, org.apache.spark.api.java.function.Function)">createDirectStream</A></B>(<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="class in org.apache.spark.streaming.api.java">JavaStreamingContext</A>&nbsp;jssc,
                   Class&lt;K&gt;&nbsp;keyClass,
                   Class&lt;V&gt;&nbsp;valueClass,
                   Class&lt;KD&gt;&nbsp;keyDecoderClass,
                   Class&lt;VD&gt;&nbsp;valueDecoderClass,
                   Class&lt;R&gt;&nbsp;recordClass,
                   java.util.Map&lt;String,String&gt;&nbsp;kafkaParams,
                   java.util.Map&lt;kafka.common.TopicAndPartition,Long&gt;&nbsp;fromOffsets,
                   <A HREF="../../../../../org/apache/spark/api/java/function/Function.html" title="interface in org.apache.spark.api.java.function">Function</A>&lt;kafka.message.MessageAndMetadata&lt;K,V&gt;,R&gt;&nbsp;messageHandler)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:: Experimental ::
 Create an input stream that directly pulls messages from Kafka Brokers
 without using any receiver.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;&gt; 
<BR>
<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaPairInputDStream.html" title="class in org.apache.spark.streaming.api.java">JavaPairInputDStream</A>&lt;K,V&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createDirectStream(org.apache.spark.streaming.api.java.JavaStreamingContext, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.util.Map, java.util.Set)">createDirectStream</A></B>(<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="class in org.apache.spark.streaming.api.java">JavaStreamingContext</A>&nbsp;jssc,
                   Class&lt;K&gt;&nbsp;keyClass,
                   Class&lt;V&gt;&nbsp;valueClass,
                   Class&lt;KD&gt;&nbsp;keyDecoderClass,
                   Class&lt;VD&gt;&nbsp;valueDecoderClass,
                   java.util.Map&lt;String,String&gt;&nbsp;kafkaParams,
                   java.util.Set&lt;String&gt;&nbsp;topics)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:: Experimental ::
 Create an input stream that directly pulls messages from Kafka Brokers
 without using any receiver.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;,R&gt; 
<BR>
<A HREF="../../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</A>&lt;R&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createDirectStream(org.apache.spark.streaming.StreamingContext, scala.collection.immutable.Map, scala.collection.immutable.Map, scala.Function1, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">createDirectStream</A></B>(<A HREF="../../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</A>&nbsp;ssc,
                   scala.collection.immutable.Map&lt;String,String&gt;&nbsp;kafkaParams,
                   scala.collection.immutable.Map&lt;kafka.common.TopicAndPartition,Object&gt;&nbsp;fromOffsets,
                   scala.Function1&lt;kafka.message.MessageAndMetadata&lt;K,V&gt;,R&gt;&nbsp;messageHandler,
                   scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$14,
                   scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$15,
                   scala.reflect.ClassTag&lt;KD&gt;&nbsp;evidence$16,
                   scala.reflect.ClassTag&lt;VD&gt;&nbsp;evidence$17,
                   scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$18)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:: Experimental ::
 Create an input stream that directly pulls messages from Kafka Brokers
 without using any receiver.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;&gt; 
<BR>
<A HREF="../../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</A>&lt;scala.Tuple2&lt;K,V&gt;&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createDirectStream(org.apache.spark.streaming.StreamingContext, scala.collection.immutable.Map, scala.collection.immutable.Set, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">createDirectStream</A></B>(<A HREF="../../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</A>&nbsp;ssc,
                   scala.collection.immutable.Map&lt;String,String&gt;&nbsp;kafkaParams,
                   scala.collection.immutable.Set&lt;String&gt;&nbsp;topics,
                   scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$19,
                   scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$20,
                   scala.reflect.ClassTag&lt;KD&gt;&nbsp;evidence$21,
                   scala.reflect.ClassTag&lt;VD&gt;&nbsp;evidence$22)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:: Experimental ::
 Create an input stream that directly pulls messages from Kafka Brokers
 without using any receiver.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;,R&gt; 
<BR>
<A HREF="../../../../../org/apache/spark/api/java/JavaRDD.html" title="class in org.apache.spark.api.java">JavaRDD</A>&lt;R&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createRDD(org.apache.spark.api.java.JavaSparkContext, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.util.Map, org.apache.spark.streaming.kafka.OffsetRange[], java.util.Map, org.apache.spark.api.java.function.Function)">createRDD</A></B>(<A HREF="../../../../../org/apache/spark/api/java/JavaSparkContext.html" title="class in org.apache.spark.api.java">JavaSparkContext</A>&nbsp;jsc,
          Class&lt;K&gt;&nbsp;keyClass,
          Class&lt;V&gt;&nbsp;valueClass,
          Class&lt;KD&gt;&nbsp;keyDecoderClass,
          Class&lt;VD&gt;&nbsp;valueDecoderClass,
          Class&lt;R&gt;&nbsp;recordClass,
          java.util.Map&lt;String,String&gt;&nbsp;kafkaParams,
          <A HREF="../../../../../org/apache/spark/streaming/kafka/OffsetRange.html" title="class in org.apache.spark.streaming.kafka">OffsetRange</A>[]&nbsp;offsetRanges,
          java.util.Map&lt;kafka.common.TopicAndPartition,<A HREF="../../../../../org/apache/spark/streaming/kafka/Broker.html" title="class in org.apache.spark.streaming.kafka">Broker</A>&gt;&nbsp;leaders,
          <A HREF="../../../../../org/apache/spark/api/java/function/Function.html" title="interface in org.apache.spark.api.java.function">Function</A>&lt;kafka.message.MessageAndMetadata&lt;K,V&gt;,R&gt;&nbsp;messageHandler)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:: Experimental ::
 Create a RDD from Kafka using offset ranges for each topic and partition.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;&gt; 
<BR>
<A HREF="../../../../../org/apache/spark/api/java/JavaPairRDD.html" title="class in org.apache.spark.api.java">JavaPairRDD</A>&lt;K,V&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createRDD(org.apache.spark.api.java.JavaSparkContext, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.util.Map, org.apache.spark.streaming.kafka.OffsetRange[])">createRDD</A></B>(<A HREF="../../../../../org/apache/spark/api/java/JavaSparkContext.html" title="class in org.apache.spark.api.java">JavaSparkContext</A>&nbsp;jsc,
          Class&lt;K&gt;&nbsp;keyClass,
          Class&lt;V&gt;&nbsp;valueClass,
          Class&lt;KD&gt;&nbsp;keyDecoderClass,
          Class&lt;VD&gt;&nbsp;valueDecoderClass,
          java.util.Map&lt;String,String&gt;&nbsp;kafkaParams,
          <A HREF="../../../../../org/apache/spark/streaming/kafka/OffsetRange.html" title="class in org.apache.spark.streaming.kafka">OffsetRange</A>[]&nbsp;offsetRanges)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create a RDD from Kafka using offset ranges for each topic and partition.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;&gt; 
<BR>
<A HREF="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,V&gt;&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createRDD(org.apache.spark.SparkContext, scala.collection.immutable.Map, org.apache.spark.streaming.kafka.OffsetRange[], scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">createRDD</A></B>(<A HREF="../../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</A>&nbsp;sc,
          scala.collection.immutable.Map&lt;String,String&gt;&nbsp;kafkaParams,
          <A HREF="../../../../../org/apache/spark/streaming/kafka/OffsetRange.html" title="class in org.apache.spark.streaming.kafka">OffsetRange</A>[]&nbsp;offsetRanges,
          scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$5,
          scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$6,
          scala.reflect.ClassTag&lt;KD&gt;&nbsp;evidence$7,
          scala.reflect.ClassTag&lt;VD&gt;&nbsp;evidence$8)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create a RDD from Kafka using offset ranges for each topic and partition.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;,R&gt; 
<BR>
<A HREF="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;R&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createRDD(org.apache.spark.SparkContext, scala.collection.immutable.Map, org.apache.spark.streaming.kafka.OffsetRange[], scala.collection.immutable.Map, scala.Function1, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">createRDD</A></B>(<A HREF="../../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</A>&nbsp;sc,
          scala.collection.immutable.Map&lt;String,String&gt;&nbsp;kafkaParams,
          <A HREF="../../../../../org/apache/spark/streaming/kafka/OffsetRange.html" title="class in org.apache.spark.streaming.kafka">OffsetRange</A>[]&nbsp;offsetRanges,
          scala.collection.immutable.Map&lt;kafka.common.TopicAndPartition,<A HREF="../../../../../org/apache/spark/streaming/kafka/Broker.html" title="class in org.apache.spark.streaming.kafka">Broker</A>&gt;&nbsp;leaders,
          scala.Function1&lt;kafka.message.MessageAndMetadata&lt;K,V&gt;,R&gt;&nbsp;messageHandler,
          scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$9,
          scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$10,
          scala.reflect.ClassTag&lt;KD&gt;&nbsp;evidence$11,
          scala.reflect.ClassTag&lt;VD&gt;&nbsp;evidence$12,
          scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$13)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:: Experimental ::
 Create a RDD from Kafka using offset ranges for each topic and partition.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V,U extends kafka.serializer.Decoder&lt;?&gt;,T extends kafka.serializer.Decoder&lt;?&gt;&gt; 
<BR>
<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.html" title="class in org.apache.spark.streaming.api.java">JavaPairReceiverInputDStream</A>&lt;K,V&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createStream(org.apache.spark.streaming.api.java.JavaStreamingContext, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.util.Map, java.util.Map, org.apache.spark.storage.StorageLevel)">createStream</A></B>(<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="class in org.apache.spark.streaming.api.java">JavaStreamingContext</A>&nbsp;jssc,
             Class&lt;K&gt;&nbsp;keyTypeClass,
             Class&lt;V&gt;&nbsp;valueTypeClass,
             Class&lt;U&gt;&nbsp;keyDecoderClass,
             Class&lt;T&gt;&nbsp;valueDecoderClass,
             java.util.Map&lt;String,String&gt;&nbsp;kafkaParams,
             java.util.Map&lt;String,Integer&gt;&nbsp;topics,
             <A HREF="../../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A>&nbsp;storageLevel)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create an input stream that pulls messages from Kafka Brokers.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static&nbsp;<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.html" title="class in org.apache.spark.streaming.api.java">JavaPairReceiverInputDStream</A>&lt;String,String&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createStream(org.apache.spark.streaming.api.java.JavaStreamingContext, java.lang.String, java.lang.String, java.util.Map)">createStream</A></B>(<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="class in org.apache.spark.streaming.api.java">JavaStreamingContext</A>&nbsp;jssc,
             String&nbsp;zkQuorum,
             String&nbsp;groupId,
             java.util.Map&lt;String,Integer&gt;&nbsp;topics)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create an input stream that pulls messages from Kafka Brokers.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static&nbsp;<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.html" title="class in org.apache.spark.streaming.api.java">JavaPairReceiverInputDStream</A>&lt;String,String&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createStream(org.apache.spark.streaming.api.java.JavaStreamingContext, java.lang.String, java.lang.String, java.util.Map, org.apache.spark.storage.StorageLevel)">createStream</A></B>(<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="class in org.apache.spark.streaming.api.java">JavaStreamingContext</A>&nbsp;jssc,
             String&nbsp;zkQuorum,
             String&nbsp;groupId,
             java.util.Map&lt;String,Integer&gt;&nbsp;topics,
             <A HREF="../../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A>&nbsp;storageLevel)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create an input stream that pulls messages from Kafka Brokers.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V,U extends kafka.serializer.Decoder&lt;?&gt;,T extends kafka.serializer.Decoder&lt;?&gt;&gt; 
<BR>
<A HREF="../../../../../org/apache/spark/streaming/dstream/ReceiverInputDStream.html" title="class in org.apache.spark.streaming.dstream">ReceiverInputDStream</A>&lt;scala.Tuple2&lt;K,V&gt;&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createStream(org.apache.spark.streaming.StreamingContext, scala.collection.immutable.Map, scala.collection.immutable.Map, org.apache.spark.storage.StorageLevel, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">createStream</A></B>(<A HREF="../../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</A>&nbsp;ssc,
             scala.collection.immutable.Map&lt;String,String&gt;&nbsp;kafkaParams,
             scala.collection.immutable.Map&lt;String,Object&gt;&nbsp;topics,
             <A HREF="../../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A>&nbsp;storageLevel,
             scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$1,
             scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$2,
             scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$3,
             scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$4)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create an input stream that pulls messages from Kafka Brokers.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static&nbsp;<A HREF="../../../../../org/apache/spark/streaming/dstream/ReceiverInputDStream.html" title="class in org.apache.spark.streaming.dstream">ReceiverInputDStream</A>&lt;scala.Tuple2&lt;String,String&gt;&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtils.html#createStream(org.apache.spark.streaming.StreamingContext, java.lang.String, java.lang.String, scala.collection.immutable.Map, org.apache.spark.storage.StorageLevel)">createStream</A></B>(<A HREF="../../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</A>&nbsp;ssc,
             String&nbsp;zkQuorum,
             String&nbsp;groupId,
             scala.collection.immutable.Map&lt;String,Object&gt;&nbsp;topics,
             <A HREF="../../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A>&nbsp;storageLevel)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create an input stream that pulls messages from Kafka Brokers.</TD>
</TR>
</TABLE>
&nbsp;<A NAME="methods_inherited_from_class_Object"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#EEEEFF" CLASS="TableSubHeadingColor">
<TH ALIGN="left"><B>Methods inherited from class Object</B></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE>equals, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait</CODE></TD>
</TR>
</TABLE>
&nbsp;
<P>

<!-- ========= CONSTRUCTOR DETAIL ======== -->

<A NAME="constructor_detail"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="1"><FONT SIZE="+2">
<B>Constructor Detail</B></FONT></TH>
</TR>
</TABLE>

<A NAME="KafkaUtils()"><!-- --></A><H3>
KafkaUtils</H3>
<PRE>
public <B>KafkaUtils</B>()</PRE>
<DL>
</DL>

<!-- ============ METHOD DETAIL ========== -->

<A NAME="method_detail"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="1"><FONT SIZE="+2">
<B>Method Detail</B></FONT></TH>
</TR>
</TABLE>

<A NAME="createStream(org.apache.spark.streaming.StreamingContext, java.lang.String, java.lang.String, scala.collection.immutable.Map, org.apache.spark.storage.StorageLevel)"><!-- --></A><H3>
createStream</H3>
<PRE>
public static <A HREF="../../../../../org/apache/spark/streaming/dstream/ReceiverInputDStream.html" title="class in org.apache.spark.streaming.dstream">ReceiverInputDStream</A>&lt;scala.Tuple2&lt;String,String&gt;&gt; <B>createStream</B>(<A HREF="../../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</A>&nbsp;ssc,
                                                                             String&nbsp;zkQuorum,
                                                                             String&nbsp;groupId,
                                                                             scala.collection.immutable.Map&lt;String,Object&gt;&nbsp;topics,
                                                                             <A HREF="../../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A>&nbsp;storageLevel)</PRE>
<DL>
<DD>Create an input stream that pulls messages from Kafka Brokers.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>ssc</CODE> - StreamingContext object<DD><CODE>zkQuorum</CODE> - Zookeeper quorum (hostname:port,hostname:port,..)<DD><CODE>groupId</CODE> - The group id for this consumer<DD><CODE>topics</CODE> - Map of (topic_name -> numPartitions) to consume. Each partition is consumed
                  in its own thread<DD><CODE>storageLevel</CODE> - Storage level to use for storing the received objects
                      (default: StorageLevel.MEMORY_AND_DISK_SER_2)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createStream(org.apache.spark.streaming.StreamingContext, scala.collection.immutable.Map, scala.collection.immutable.Map, org.apache.spark.storage.StorageLevel, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)"><!-- --></A><H3>
createStream</H3>
<PRE>
public static &lt;K,V,U extends kafka.serializer.Decoder&lt;?&gt;,T extends kafka.serializer.Decoder&lt;?&gt;&gt; <A HREF="../../../../../org/apache/spark/streaming/dstream/ReceiverInputDStream.html" title="class in org.apache.spark.streaming.dstream">ReceiverInputDStream</A>&lt;scala.Tuple2&lt;K,V&gt;&gt; <B>createStream</B>(<A HREF="../../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</A>&nbsp;ssc,
                                                                                                                                                     scala.collection.immutable.Map&lt;String,String&gt;&nbsp;kafkaParams,
                                                                                                                                                     scala.collection.immutable.Map&lt;String,Object&gt;&nbsp;topics,
                                                                                                                                                     <A HREF="../../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A>&nbsp;storageLevel,
                                                                                                                                                     scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$1,
                                                                                                                                                     scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$2,
                                                                                                                                                     scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$3,
                                                                                                                                                     scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$4)</PRE>
<DL>
<DD>Create an input stream that pulls messages from Kafka Brokers.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>ssc</CODE> - StreamingContext object<DD><CODE>kafkaParams</CODE> - Map of kafka configuration parameters,
                    see http://kafka.apache.org/08/configuration.html<DD><CODE>topics</CODE> - Map of (topic_name -> numPartitions) to consume. Each partition is consumed
                    in its own thread.<DD><CODE>storageLevel</CODE> - Storage level to use for storing the received objects<DD><CODE>evidence$1</CODE> - (undocumented)<DD><CODE>evidence$2</CODE> - (undocumented)<DD><CODE>evidence$3</CODE> - (undocumented)<DD><CODE>evidence$4</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createStream(org.apache.spark.streaming.api.java.JavaStreamingContext, java.lang.String, java.lang.String, java.util.Map)"><!-- --></A><H3>
createStream</H3>
<PRE>
public static <A HREF="../../../../../org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.html" title="class in org.apache.spark.streaming.api.java">JavaPairReceiverInputDStream</A>&lt;String,String&gt; <B>createStream</B>(<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="class in org.apache.spark.streaming.api.java">JavaStreamingContext</A>&nbsp;jssc,
                                                                       String&nbsp;zkQuorum,
                                                                       String&nbsp;groupId,
                                                                       java.util.Map&lt;String,Integer&gt;&nbsp;topics)</PRE>
<DL>
<DD>Create an input stream that pulls messages from Kafka Brokers.
 Storage level of the data will be the default StorageLevel.MEMORY_AND_DISK_SER_2.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>jssc</CODE> - JavaStreamingContext object<DD><CODE>zkQuorum</CODE> - Zookeeper quorum (hostname:port,hostname:port,..)<DD><CODE>groupId</CODE> - The group id for this consumer<DD><CODE>topics</CODE> - Map of (topic_name -> numPartitions) to consume. Each partition is consumed
                  in its own thread
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createStream(org.apache.spark.streaming.api.java.JavaStreamingContext, java.lang.String, java.lang.String, java.util.Map, org.apache.spark.storage.StorageLevel)"><!-- --></A><H3>
createStream</H3>
<PRE>
public static <A HREF="../../../../../org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.html" title="class in org.apache.spark.streaming.api.java">JavaPairReceiverInputDStream</A>&lt;String,String&gt; <B>createStream</B>(<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="class in org.apache.spark.streaming.api.java">JavaStreamingContext</A>&nbsp;jssc,
                                                                       String&nbsp;zkQuorum,
                                                                       String&nbsp;groupId,
                                                                       java.util.Map&lt;String,Integer&gt;&nbsp;topics,
                                                                       <A HREF="../../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A>&nbsp;storageLevel)</PRE>
<DL>
<DD>Create an input stream that pulls messages from Kafka Brokers.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>jssc</CODE> - JavaStreamingContext object<DD><CODE>zkQuorum</CODE> - Zookeeper quorum (hostname:port,hostname:port,..).<DD><CODE>groupId</CODE> - The group id for this consumer.<DD><CODE>topics</CODE> - Map of (topic_name -> numPartitions) to consume. Each partition is consumed
                  in its own thread.<DD><CODE>storageLevel</CODE> - RDD storage level.
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createStream(org.apache.spark.streaming.api.java.JavaStreamingContext, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.util.Map, java.util.Map, org.apache.spark.storage.StorageLevel)"><!-- --></A><H3>
createStream</H3>
<PRE>
public static &lt;K,V,U extends kafka.serializer.Decoder&lt;?&gt;,T extends kafka.serializer.Decoder&lt;?&gt;&gt; <A HREF="../../../../../org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.html" title="class in org.apache.spark.streaming.api.java">JavaPairReceiverInputDStream</A>&lt;K,V&gt; <B>createStream</B>(<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="class in org.apache.spark.streaming.api.java">JavaStreamingContext</A>&nbsp;jssc,
                                                                                                                                               Class&lt;K&gt;&nbsp;keyTypeClass,
                                                                                                                                               Class&lt;V&gt;&nbsp;valueTypeClass,
                                                                                                                                               Class&lt;U&gt;&nbsp;keyDecoderClass,
                                                                                                                                               Class&lt;T&gt;&nbsp;valueDecoderClass,
                                                                                                                                               java.util.Map&lt;String,String&gt;&nbsp;kafkaParams,
                                                                                                                                               java.util.Map&lt;String,Integer&gt;&nbsp;topics,
                                                                                                                                               <A HREF="../../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A>&nbsp;storageLevel)</PRE>
<DL>
<DD>Create an input stream that pulls messages from Kafka Brokers.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>jssc</CODE> - JavaStreamingContext object<DD><CODE>keyTypeClass</CODE> - Key type of DStream<DD><CODE>valueTypeClass</CODE> - value type of Dstream<DD><CODE>keyDecoderClass</CODE> - Type of kafka key decoder<DD><CODE>valueDecoderClass</CODE> - Type of kafka value decoder<DD><CODE>kafkaParams</CODE> - Map of kafka configuration parameters,
                    see http://kafka.apache.org/08/configuration.html<DD><CODE>topics</CODE> - Map of (topic_name -> numPartitions) to consume. Each partition is consumed
                in its own thread<DD><CODE>storageLevel</CODE> - RDD storage level.
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createRDD(org.apache.spark.SparkContext, scala.collection.immutable.Map, org.apache.spark.streaming.kafka.OffsetRange[], scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)"><!-- --></A><H3>
createRDD</H3>
<PRE>
public static &lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;&gt; <A HREF="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,V&gt;&gt; <B>createRDD</B>(<A HREF="../../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</A>&nbsp;sc,
                                                                                                                                   scala.collection.immutable.Map&lt;String,String&gt;&nbsp;kafkaParams,
                                                                                                                                   <A HREF="../../../../../org/apache/spark/streaming/kafka/OffsetRange.html" title="class in org.apache.spark.streaming.kafka">OffsetRange</A>[]&nbsp;offsetRanges,
                                                                                                                                   scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$5,
                                                                                                                                   scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$6,
                                                                                                                                   scala.reflect.ClassTag&lt;KD&gt;&nbsp;evidence$7,
                                                                                                                                   scala.reflect.ClassTag&lt;VD&gt;&nbsp;evidence$8)</PRE>
<DL>
<DD>Create a RDD from Kafka using offset ranges for each topic and partition.
 <p>
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sc</CODE> - SparkContext object<DD><CODE>kafkaParams</CODE> - Kafka <a href="http://kafka.apache.org/documentation.html#configuration">
    configuration parameters</a>. Requires "metadata.broker.list" or "bootstrap.servers"
    to be set with Kafka broker(s) (NOT zookeeper servers) specified in
    host1:port1,host2:port2 form.<DD><CODE>offsetRanges</CODE> - Each OffsetRange in the batch corresponds to a
   range of offsets for a given Kafka topic/partition<DD><CODE>evidence$5</CODE> - (undocumented)<DD><CODE>evidence$6</CODE> - (undocumented)<DD><CODE>evidence$7</CODE> - (undocumented)<DD><CODE>evidence$8</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createRDD(org.apache.spark.SparkContext, scala.collection.immutable.Map, org.apache.spark.streaming.kafka.OffsetRange[], scala.collection.immutable.Map, scala.Function1, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)"><!-- --></A><H3>
createRDD</H3>
<PRE>
public static &lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;,R&gt; <A HREF="../../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;R&gt; <B>createRDD</B>(<A HREF="../../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</A>&nbsp;sc,
                                                                                                                     scala.collection.immutable.Map&lt;String,String&gt;&nbsp;kafkaParams,
                                                                                                                     <A HREF="../../../../../org/apache/spark/streaming/kafka/OffsetRange.html" title="class in org.apache.spark.streaming.kafka">OffsetRange</A>[]&nbsp;offsetRanges,
                                                                                                                     scala.collection.immutable.Map&lt;kafka.common.TopicAndPartition,<A HREF="../../../../../org/apache/spark/streaming/kafka/Broker.html" title="class in org.apache.spark.streaming.kafka">Broker</A>&gt;&nbsp;leaders,
                                                                                                                     scala.Function1&lt;kafka.message.MessageAndMetadata&lt;K,V&gt;,R&gt;&nbsp;messageHandler,
                                                                                                                     scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$9,
                                                                                                                     scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$10,
                                                                                                                     scala.reflect.ClassTag&lt;KD&gt;&nbsp;evidence$11,
                                                                                                                     scala.reflect.ClassTag&lt;VD&gt;&nbsp;evidence$12,
                                                                                                                     scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$13)</PRE>
<DL>
<DD>:: Experimental ::
 Create a RDD from Kafka using offset ranges for each topic and partition. This allows you
 specify the Kafka leader to connect to (to optimize fetching) and access the message as well
 as the metadata.
 <p>
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sc</CODE> - SparkContext object<DD><CODE>kafkaParams</CODE> - Kafka <a href="http://kafka.apache.org/documentation.html#configuration">
    configuration parameters</a>. Requires "metadata.broker.list" or "bootstrap.servers"
    to be set with Kafka broker(s) (NOT zookeeper servers) specified in
    host1:port1,host2:port2 form.<DD><CODE>offsetRanges</CODE> - Each OffsetRange in the batch corresponds to a
   range of offsets for a given Kafka topic/partition<DD><CODE>leaders</CODE> - Kafka brokers for each TopicAndPartition in offsetRanges.  May be an empty map,
   in which case leaders will be looked up on the driver.<DD><CODE>messageHandler</CODE> - Function for translating each message and metadata into the desired type<DD><CODE>evidence$9</CODE> - (undocumented)<DD><CODE>evidence$10</CODE> - (undocumented)<DD><CODE>evidence$11</CODE> - (undocumented)<DD><CODE>evidence$12</CODE> - (undocumented)<DD><CODE>evidence$13</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createRDD(org.apache.spark.api.java.JavaSparkContext, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.util.Map, org.apache.spark.streaming.kafka.OffsetRange[])"><!-- --></A><H3>
createRDD</H3>
<PRE>
public static &lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;&gt; <A HREF="../../../../../org/apache/spark/api/java/JavaPairRDD.html" title="class in org.apache.spark.api.java">JavaPairRDD</A>&lt;K,V&gt; <B>createRDD</B>(<A HREF="../../../../../org/apache/spark/api/java/JavaSparkContext.html" title="class in org.apache.spark.api.java">JavaSparkContext</A>&nbsp;jsc,
                                                                                                                             Class&lt;K&gt;&nbsp;keyClass,
                                                                                                                             Class&lt;V&gt;&nbsp;valueClass,
                                                                                                                             Class&lt;KD&gt;&nbsp;keyDecoderClass,
                                                                                                                             Class&lt;VD&gt;&nbsp;valueDecoderClass,
                                                                                                                             java.util.Map&lt;String,String&gt;&nbsp;kafkaParams,
                                                                                                                             <A HREF="../../../../../org/apache/spark/streaming/kafka/OffsetRange.html" title="class in org.apache.spark.streaming.kafka">OffsetRange</A>[]&nbsp;offsetRanges)</PRE>
<DL>
<DD>Create a RDD from Kafka using offset ranges for each topic and partition.
 <p>
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>jsc</CODE> - JavaSparkContext object<DD><CODE>kafkaParams</CODE> - Kafka <a href="http://kafka.apache.org/documentation.html#configuration">
    configuration parameters</a>. Requires "metadata.broker.list" or "bootstrap.servers"
    to be set with Kafka broker(s) (NOT zookeeper servers) specified in
    host1:port1,host2:port2 form.<DD><CODE>offsetRanges</CODE> - Each OffsetRange in the batch corresponds to a
   range of offsets for a given Kafka topic/partition<DD><CODE>keyClass</CODE> - (undocumented)<DD><CODE>valueClass</CODE> - (undocumented)<DD><CODE>keyDecoderClass</CODE> - (undocumented)<DD><CODE>valueDecoderClass</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createRDD(org.apache.spark.api.java.JavaSparkContext, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.util.Map, org.apache.spark.streaming.kafka.OffsetRange[], java.util.Map, org.apache.spark.api.java.function.Function)"><!-- --></A><H3>
createRDD</H3>
<PRE>
public static &lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;,R&gt; <A HREF="../../../../../org/apache/spark/api/java/JavaRDD.html" title="class in org.apache.spark.api.java">JavaRDD</A>&lt;R&gt; <B>createRDD</B>(<A HREF="../../../../../org/apache/spark/api/java/JavaSparkContext.html" title="class in org.apache.spark.api.java">JavaSparkContext</A>&nbsp;jsc,
                                                                                                                         Class&lt;K&gt;&nbsp;keyClass,
                                                                                                                         Class&lt;V&gt;&nbsp;valueClass,
                                                                                                                         Class&lt;KD&gt;&nbsp;keyDecoderClass,
                                                                                                                         Class&lt;VD&gt;&nbsp;valueDecoderClass,
                                                                                                                         Class&lt;R&gt;&nbsp;recordClass,
                                                                                                                         java.util.Map&lt;String,String&gt;&nbsp;kafkaParams,
                                                                                                                         <A HREF="../../../../../org/apache/spark/streaming/kafka/OffsetRange.html" title="class in org.apache.spark.streaming.kafka">OffsetRange</A>[]&nbsp;offsetRanges,
                                                                                                                         java.util.Map&lt;kafka.common.TopicAndPartition,<A HREF="../../../../../org/apache/spark/streaming/kafka/Broker.html" title="class in org.apache.spark.streaming.kafka">Broker</A>&gt;&nbsp;leaders,
                                                                                                                         <A HREF="../../../../../org/apache/spark/api/java/function/Function.html" title="interface in org.apache.spark.api.java.function">Function</A>&lt;kafka.message.MessageAndMetadata&lt;K,V&gt;,R&gt;&nbsp;messageHandler)</PRE>
<DL>
<DD>:: Experimental ::
 Create a RDD from Kafka using offset ranges for each topic and partition. This allows you
 specify the Kafka leader to connect to (to optimize fetching) and access the message as well
 as the metadata.
 <p>
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>jsc</CODE> - JavaSparkContext object<DD><CODE>kafkaParams</CODE> - Kafka <a href="http://kafka.apache.org/documentation.html#configuration">
    configuration parameters</a>. Requires "metadata.broker.list" or "bootstrap.servers"
    to be set with Kafka broker(s) (NOT zookeeper servers) specified in
    host1:port1,host2:port2 form.<DD><CODE>offsetRanges</CODE> - Each OffsetRange in the batch corresponds to a
   range of offsets for a given Kafka topic/partition<DD><CODE>leaders</CODE> - Kafka brokers for each TopicAndPartition in offsetRanges.  May be an empty map,
   in which case leaders will be looked up on the driver.<DD><CODE>messageHandler</CODE> - Function for translating each message and metadata into the desired type<DD><CODE>keyClass</CODE> - (undocumented)<DD><CODE>valueClass</CODE> - (undocumented)<DD><CODE>keyDecoderClass</CODE> - (undocumented)<DD><CODE>valueDecoderClass</CODE> - (undocumented)<DD><CODE>recordClass</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createDirectStream(org.apache.spark.streaming.StreamingContext, scala.collection.immutable.Map, scala.collection.immutable.Map, scala.Function1, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)"><!-- --></A><H3>
createDirectStream</H3>
<PRE>
public static &lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;,R&gt; <A HREF="../../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</A>&lt;R&gt; <B>createDirectStream</B>(<A HREF="../../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</A>&nbsp;ssc,
                                                                                                                                       scala.collection.immutable.Map&lt;String,String&gt;&nbsp;kafkaParams,
                                                                                                                                       scala.collection.immutable.Map&lt;kafka.common.TopicAndPartition,Object&gt;&nbsp;fromOffsets,
                                                                                                                                       scala.Function1&lt;kafka.message.MessageAndMetadata&lt;K,V&gt;,R&gt;&nbsp;messageHandler,
                                                                                                                                       scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$14,
                                                                                                                                       scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$15,
                                                                                                                                       scala.reflect.ClassTag&lt;KD&gt;&nbsp;evidence$16,
                                                                                                                                       scala.reflect.ClassTag&lt;VD&gt;&nbsp;evidence$17,
                                                                                                                                       scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$18)</PRE>
<DL>
<DD>:: Experimental ::
 Create an input stream that directly pulls messages from Kafka Brokers
 without using any receiver. This stream can guarantee that each message
 from Kafka is included in transformations exactly once (see points below).
 <p>
 Points to note:
  - No receivers: This stream does not use any receiver. It directly queries Kafka
  - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked
    by the stream itself. For interoperability with Kafka monitoring tools that depend on
    Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.
    You can access the offsets used in each batch from the generated RDDs (see
    <A HREF="../../../../../org/apache/spark/streaming/kafka/HasOffsetRanges.html" title="interface in org.apache.spark.streaming.kafka"><CODE>HasOffsetRanges</CODE></A>).
  - Failure Recovery: To recover from driver failures, you have to enable checkpointing
    in the <CODE>StreamingContext</CODE>. The information on consumed offset can be
    recovered from the checkpoint. See the programming guide for details (constraints, etc.).
  - End-to-end semantics: This stream ensures that every records is effectively received and
    transformed exactly once, but gives no guarantees on whether the transformed data are
    outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure
    that the output operation is idempotent, or use transactions to output records atomically.
    See the programming guide for more details.
 <p>
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>ssc</CODE> - StreamingContext object<DD><CODE>kafkaParams</CODE> - Kafka <a href="http://kafka.apache.org/documentation.html#configuration">
    configuration parameters</a>. Requires "metadata.broker.list" or "bootstrap.servers"
    to be set with Kafka broker(s) (NOT zookeeper servers) specified in
    host1:port1,host2:port2 form.<DD><CODE>fromOffsets</CODE> - Per-topic/partition Kafka offsets defining the (inclusive)
    starting point of the stream<DD><CODE>messageHandler</CODE> - Function for translating each message and metadata into the desired type<DD><CODE>evidence$14</CODE> - (undocumented)<DD><CODE>evidence$15</CODE> - (undocumented)<DD><CODE>evidence$16</CODE> - (undocumented)<DD><CODE>evidence$17</CODE> - (undocumented)<DD><CODE>evidence$18</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createDirectStream(org.apache.spark.streaming.StreamingContext, scala.collection.immutable.Map, scala.collection.immutable.Set, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)"><!-- --></A><H3>
createDirectStream</H3>
<PRE>
public static &lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;&gt; <A HREF="../../../../../org/apache/spark/streaming/dstream/InputDStream.html" title="class in org.apache.spark.streaming.dstream">InputDStream</A>&lt;scala.Tuple2&lt;K,V&gt;&gt; <B>createDirectStream</B>(<A HREF="../../../../../org/apache/spark/streaming/StreamingContext.html" title="class in org.apache.spark.streaming">StreamingContext</A>&nbsp;ssc,
                                                                                                                                                     scala.collection.immutable.Map&lt;String,String&gt;&nbsp;kafkaParams,
                                                                                                                                                     scala.collection.immutable.Set&lt;String&gt;&nbsp;topics,
                                                                                                                                                     scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$19,
                                                                                                                                                     scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$20,
                                                                                                                                                     scala.reflect.ClassTag&lt;KD&gt;&nbsp;evidence$21,
                                                                                                                                                     scala.reflect.ClassTag&lt;VD&gt;&nbsp;evidence$22)</PRE>
<DL>
<DD>:: Experimental ::
 Create an input stream that directly pulls messages from Kafka Brokers
 without using any receiver. This stream can guarantee that each message
 from Kafka is included in transformations exactly once (see points below).
 <p>
 Points to note:
  - No receivers: This stream does not use any receiver. It directly queries Kafka
  - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked
    by the stream itself. For interoperability with Kafka monitoring tools that depend on
    Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.
    You can access the offsets used in each batch from the generated RDDs (see
    <A HREF="../../../../../org/apache/spark/streaming/kafka/HasOffsetRanges.html" title="interface in org.apache.spark.streaming.kafka"><CODE>HasOffsetRanges</CODE></A>).
  - Failure Recovery: To recover from driver failures, you have to enable checkpointing
    in the <CODE>StreamingContext</CODE>. The information on consumed offset can be
    recovered from the checkpoint. See the programming guide for details (constraints, etc.).
  - End-to-end semantics: This stream ensures that every records is effectively received and
    transformed exactly once, but gives no guarantees on whether the transformed data are
    outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure
    that the output operation is idempotent, or use transactions to output records atomically.
    See the programming guide for more details.
 <p>
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>ssc</CODE> - StreamingContext object<DD><CODE>kafkaParams</CODE> - Kafka <a href="http://kafka.apache.org/documentation.html#configuration">
   configuration parameters</a>. Requires "metadata.broker.list" or "bootstrap.servers"
   to be set with Kafka broker(s) (NOT zookeeper servers), specified in
   host1:port1,host2:port2 form.
   If not starting from a checkpoint, "auto.offset.reset" may be set to "largest" or "smallest"
   to determine where the stream starts (defaults to "largest")<DD><CODE>topics</CODE> - Names of the topics to consume<DD><CODE>evidence$19</CODE> - (undocumented)<DD><CODE>evidence$20</CODE> - (undocumented)<DD><CODE>evidence$21</CODE> - (undocumented)<DD><CODE>evidence$22</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createDirectStream(org.apache.spark.streaming.api.java.JavaStreamingContext, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.util.Map, java.util.Map, org.apache.spark.api.java.function.Function)"><!-- --></A><H3>
createDirectStream</H3>
<PRE>
public static &lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;,R&gt; <A HREF="../../../../../org/apache/spark/streaming/api/java/JavaInputDStream.html" title="class in org.apache.spark.streaming.api.java">JavaInputDStream</A>&lt;R&gt; <B>createDirectStream</B>(<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="class in org.apache.spark.streaming.api.java">JavaStreamingContext</A>&nbsp;jssc,
                                                                                                                                           Class&lt;K&gt;&nbsp;keyClass,
                                                                                                                                           Class&lt;V&gt;&nbsp;valueClass,
                                                                                                                                           Class&lt;KD&gt;&nbsp;keyDecoderClass,
                                                                                                                                           Class&lt;VD&gt;&nbsp;valueDecoderClass,
                                                                                                                                           Class&lt;R&gt;&nbsp;recordClass,
                                                                                                                                           java.util.Map&lt;String,String&gt;&nbsp;kafkaParams,
                                                                                                                                           java.util.Map&lt;kafka.common.TopicAndPartition,Long&gt;&nbsp;fromOffsets,
                                                                                                                                           <A HREF="../../../../../org/apache/spark/api/java/function/Function.html" title="interface in org.apache.spark.api.java.function">Function</A>&lt;kafka.message.MessageAndMetadata&lt;K,V&gt;,R&gt;&nbsp;messageHandler)</PRE>
<DL>
<DD>:: Experimental ::
 Create an input stream that directly pulls messages from Kafka Brokers
 without using any receiver. This stream can guarantee that each message
 from Kafka is included in transformations exactly once (see points below).
 <p>
 Points to note:
  - No receivers: This stream does not use any receiver. It directly queries Kafka
  - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked
    by the stream itself. For interoperability with Kafka monitoring tools that depend on
    Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.
    You can access the offsets used in each batch from the generated RDDs (see
    <A HREF="../../../../../org/apache/spark/streaming/kafka/HasOffsetRanges.html" title="interface in org.apache.spark.streaming.kafka"><CODE>HasOffsetRanges</CODE></A>).
  - Failure Recovery: To recover from driver failures, you have to enable checkpointing
    in the <CODE>StreamingContext</CODE>. The information on consumed offset can be
    recovered from the checkpoint. See the programming guide for details (constraints, etc.).
  - End-to-end semantics: This stream ensures that every records is effectively received and
    transformed exactly once, but gives no guarantees on whether the transformed data are
    outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure
    that the output operation is idempotent, or use transactions to output records atomically.
    See the programming guide for more details.
 <p>
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>jssc</CODE> - JavaStreamingContext object<DD><CODE>keyClass</CODE> - Class of the keys in the Kafka records<DD><CODE>valueClass</CODE> - Class of the values in the Kafka records<DD><CODE>keyDecoderClass</CODE> - Class of the key decoder<DD><CODE>valueDecoderClass</CODE> - Class of the value decoder<DD><CODE>recordClass</CODE> - Class of the records in DStream<DD><CODE>kafkaParams</CODE> - Kafka <a href="http://kafka.apache.org/documentation.html#configuration">
   configuration parameters</a>. Requires "metadata.broker.list" or "bootstrap.servers"
   to be set with Kafka broker(s) (NOT zookeeper servers), specified in
   host1:port1,host2:port2 form.<DD><CODE>fromOffsets</CODE> - Per-topic/partition Kafka offsets defining the (inclusive)
    starting point of the stream<DD><CODE>messageHandler</CODE> - Function for translating each message and metadata into the desired type
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createDirectStream(org.apache.spark.streaming.api.java.JavaStreamingContext, java.lang.Class, java.lang.Class, java.lang.Class, java.lang.Class, java.util.Map, java.util.Set)"><!-- --></A><H3>
createDirectStream</H3>
<PRE>
public static &lt;K,V,KD extends kafka.serializer.Decoder&lt;K&gt;,VD extends kafka.serializer.Decoder&lt;V&gt;&gt; <A HREF="../../../../../org/apache/spark/streaming/api/java/JavaPairInputDStream.html" title="class in org.apache.spark.streaming.api.java">JavaPairInputDStream</A>&lt;K,V&gt; <B>createDirectStream</B>(<A HREF="../../../../../org/apache/spark/streaming/api/java/JavaStreamingContext.html" title="class in org.apache.spark.streaming.api.java">JavaStreamingContext</A>&nbsp;jssc,
                                                                                                                                               Class&lt;K&gt;&nbsp;keyClass,
                                                                                                                                               Class&lt;V&gt;&nbsp;valueClass,
                                                                                                                                               Class&lt;KD&gt;&nbsp;keyDecoderClass,
                                                                                                                                               Class&lt;VD&gt;&nbsp;valueDecoderClass,
                                                                                                                                               java.util.Map&lt;String,String&gt;&nbsp;kafkaParams,
                                                                                                                                               java.util.Set&lt;String&gt;&nbsp;topics)</PRE>
<DL>
<DD>:: Experimental ::
 Create an input stream that directly pulls messages from Kafka Brokers
 without using any receiver. This stream can guarantee that each message
 from Kafka is included in transformations exactly once (see points below).
 <p>
 Points to note:
  - No receivers: This stream does not use any receiver. It directly queries Kafka
  - Offsets: This does not use Zookeeper to store offsets. The consumed offsets are tracked
    by the stream itself. For interoperability with Kafka monitoring tools that depend on
    Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.
    You can access the offsets used in each batch from the generated RDDs (see
    <A HREF="../../../../../org/apache/spark/streaming/kafka/HasOffsetRanges.html" title="interface in org.apache.spark.streaming.kafka"><CODE>HasOffsetRanges</CODE></A>).
  - Failure Recovery: To recover from driver failures, you have to enable checkpointing
    in the <CODE>StreamingContext</CODE>. The information on consumed offset can be
    recovered from the checkpoint. See the programming guide for details (constraints, etc.).
  - End-to-end semantics: This stream ensures that every records is effectively received and
    transformed exactly once, but gives no guarantees on whether the transformed data are
    outputted exactly once. For end-to-end exactly-once semantics, you have to either ensure
    that the output operation is idempotent, or use transactions to output records atomically.
    See the programming guide for more details.
 <p>
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>jssc</CODE> - JavaStreamingContext object<DD><CODE>keyClass</CODE> - Class of the keys in the Kafka records<DD><CODE>valueClass</CODE> - Class of the values in the Kafka records<DD><CODE>keyDecoderClass</CODE> - Class of the key decoder<DD><CODE>valueDecoderClass</CODE> - Class type of the value decoder<DD><CODE>kafkaParams</CODE> - Kafka <a href="http://kafka.apache.org/documentation.html#configuration">
   configuration parameters</a>. Requires "metadata.broker.list" or "bootstrap.servers"
   to be set with Kafka broker(s) (NOT zookeeper servers), specified in
   host1:port1,host2:port2 form.
   If not starting from a checkpoint, "auto.offset.reset" may be set to "largest" or "smallest"
   to determine where the stream starts (defaults to "largest")<DD><CODE>topics</CODE> - Names of the topics to consume
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<!-- ========= END OF CLASS DATA ========= -->
<HR>


<!-- ======= START OF BOTTOM NAVBAR ====== -->
<A NAME="navbar_bottom"><!-- --></A>
<A HREF="#skip-navbar_bottom" title="Skip navigation links"></A>
<TABLE BORDER="0" WIDTH="100%" CELLPADDING="1" CELLSPACING="0" SUMMARY="">
<TR>
<TD COLSPAN=2 BGCOLOR="#EEEEFF" CLASS="NavBarCell1">
<A NAME="navbar_bottom_firstrow"><!-- --></A>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="3" SUMMARY="">
  <TR ALIGN="center" VALIGN="top">
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../../overview-summary.html"><FONT CLASS="NavBarFont1"><B>Overview</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-summary.html"><FONT CLASS="NavBarFont1"><B>Package</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#FFFFFF" CLASS="NavBarCell1Rev"> &nbsp;<FONT CLASS="NavBarFont1Rev"><B>Class</B></FONT>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-tree.html"><FONT CLASS="NavBarFont1"><B>Tree</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../../deprecated-list.html"><FONT CLASS="NavBarFont1"><B>Deprecated</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../../index-all.html"><FONT CLASS="NavBarFont1"><B>Index</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../../help-doc.html"><FONT CLASS="NavBarFont1"><B>Help</B></FONT></A>&nbsp;</TD>
  </TR>
</TABLE>
</TD>
<TD ALIGN="right" VALIGN="top" ROWSPAN=3><EM>
</EM>
</TD>
</TR>

<TR>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
&nbsp;<A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaTestUtils.html" title="class in org.apache.spark.streaming.kafka"><B>PREV CLASS</B></A>&nbsp;
&nbsp;<A HREF="../../../../../org/apache/spark/streaming/kafka/KafkaUtilsPythonHelper.html" title="class in org.apache.spark.streaming.kafka"><B>NEXT CLASS</B></A></FONT></TD>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
  <A HREF="../../../../../index.html?org/apache/spark/streaming/kafka/KafkaUtils.html" target="_top"><B>FRAMES</B></A>  &nbsp;
&nbsp;<A HREF="KafkaUtils.html" target="_top"><B>NO FRAMES</B></A>  &nbsp;
&nbsp;<SCRIPT type="text/javascript">
  <!--
  if(window==top) {
    document.writeln('<A HREF="../../../../../allclasses-noframe.html"><B>All Classes</B></A>');
  }
  //-->
</SCRIPT>
<NOSCRIPT>
  <A HREF="../../../../../allclasses-noframe.html"><B>All Classes</B></A>
</NOSCRIPT>


</FONT></TD>
</TR>
<TR>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_summary">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_summary">METHOD</A></FONT></TD>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
DETAIL:&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_detail">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_detail">METHOD</A></FONT></TD>
</TR>
</TABLE>
<A NAME="skip-navbar_bottom"></A>
<!-- ======== END OF BOTTOM NAVBAR ======= -->

<HR>

</BODY>
</HTML>
