<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!--NewPage-->
<HTML>
<HEAD>
<!-- Generated by javadoc (build 1.6.0_45) on Wed Jul 08 16:13:38 PDT 2015 -->
<TITLE>
DataFrame (Spark 1.4.1 JavaDoc)
</TITLE>

<META NAME="date" CONTENT="2015-07-08">

<LINK REL ="stylesheet" TYPE="text/css" HREF="../../../../stylesheet.css" TITLE="Style">

<SCRIPT type="text/javascript">
function windowTitle()
{
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="DataFrame (Spark 1.4.1 JavaDoc)";
    }
}
</SCRIPT>
<NOSCRIPT>
</NOSCRIPT>

</HEAD>

<BODY BGCOLOR="white" onload="windowTitle();">
<HR>


<!-- ========= START OF TOP NAVBAR ======= -->
<A NAME="navbar_top"><!-- --></A>
<A HREF="#skip-navbar_top" title="Skip navigation links"></A>
<TABLE BORDER="0" WIDTH="100%" CELLPADDING="1" CELLSPACING="0" SUMMARY="">
<TR>
<TD COLSPAN=2 BGCOLOR="#EEEEFF" CLASS="NavBarCell1">
<A NAME="navbar_top_firstrow"><!-- --></A>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="3" SUMMARY="">
  <TR ALIGN="center" VALIGN="top">
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../overview-summary.html"><FONT CLASS="NavBarFont1"><B>Overview</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-summary.html"><FONT CLASS="NavBarFont1"><B>Package</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#FFFFFF" CLASS="NavBarCell1Rev"> &nbsp;<FONT CLASS="NavBarFont1Rev"><B>Class</B></FONT>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-tree.html"><FONT CLASS="NavBarFont1"><B>Tree</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../deprecated-list.html"><FONT CLASS="NavBarFont1"><B>Deprecated</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../index-all.html"><FONT CLASS="NavBarFont1"><B>Index</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../help-doc.html"><FONT CLASS="NavBarFont1"><B>Help</B></FONT></A>&nbsp;</TD>
  </TR>
</TABLE>
</TD>
<TD ALIGN="right" VALIGN="top" ROWSPAN=3><EM>
</EM>
</TD>
</TR>

<TR>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
&nbsp;<A HREF="../../../../org/apache/spark/sql/ColumnName.html" title="class in org.apache.spark.sql"><B>PREV CLASS</B></A>&nbsp;
&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrameNaFunctions.html" title="class in org.apache.spark.sql"><B>NEXT CLASS</B></A></FONT></TD>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
  <A HREF="../../../../index.html?org/apache/spark/sql/DataFrame.html" target="_top"><B>FRAMES</B></A>  &nbsp;
&nbsp;<A HREF="DataFrame.html" target="_top"><B>NO FRAMES</B></A>  &nbsp;
&nbsp;<SCRIPT type="text/javascript">
  <!--
  if(window==top) {
    document.writeln('<A HREF="../../../../allclasses-noframe.html"><B>All Classes</B></A>');
  }
  //-->
</SCRIPT>
<NOSCRIPT>
  <A HREF="../../../../allclasses-noframe.html"><B>All Classes</B></A>
</NOSCRIPT>


</FONT></TD>
</TR>
<TR>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_summary">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_summary">METHOD</A></FONT></TD>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
DETAIL:&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_detail">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_detail">METHOD</A></FONT></TD>
</TR>
</TABLE>
<A NAME="skip-navbar_top"></A>
<!-- ========= END OF TOP NAVBAR ========= -->

<HR>
<!-- ======== START OF CLASS DATA ======== -->
<H2>
<FONT SIZE="-1">
org.apache.spark.sql</FONT>
<BR>
Class DataFrame</H2>
<PRE>
Object
  <IMG SRC="../../../../resources/inherit.gif" ALT="extended by "><B>org.apache.spark.sql.DataFrame</B>
</PRE>
<DL>
<DT><B>All Implemented Interfaces:</B> <DD>java.io.Serializable</DD>
</DL>
<HR>
<DL>
<DT><PRE>public class <B>DataFrame</B><DT>extends Object<DT>implements scala.Serializable</DL>
</PRE>

<P>
:: Experimental ::
 A distributed collection of data organized into named columns.
 <p>
 A <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> is equivalent to a relational table in Spark SQL. The following example creates
 a <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> by pointing Spark SQL to a Parquet data set.
 <pre><code>
   val people = sqlContext.read.parquet("...")  // in Scala
   DataFrame people = sqlContext.read().parquet("...")  // in Java
 </code></pre>
 <p>
 Once created, it can be manipulated using the various domain-specific-language (DSL) functions
 defined in: <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> (this class), <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql"><CODE>Column</CODE></A>, and <A HREF="../../../../org/apache/spark/sql/functions.html" title="class in org.apache.spark.sql"><CODE>functions</CODE></A>.
 <p>
 To select a column from the data frame, use <code>apply</code> method in Scala and <code>col</code> in Java.
 <pre><code>
   val ageCol = people("age")  // in Scala
   Column ageCol = people.col("age")  // in Java
 </code></pre>
 <p>
 Note that the <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql"><CODE>Column</CODE></A> type can also be manipulated through its various functions.
 <pre><code>
   // The following creates a new column that increases everybody's age by 10.
   people("age") + 10  // in Scala
   people.col("age").plus(10);  // in Java
 </code></pre>
 <p>
 A more concrete example in Scala:
 <pre><code>
   // To create DataFrame using SQLContext
   val people = sqlContext.read.parquet("...")
   val department = sqlContext.read.parquet("...")

   people.filter("age &gt; 30")
     .join(department, people("deptId") === department("id"))
     .groupBy(department("name"), "gender")
     .agg(avg(people("salary")), max(people("age")))
 </code></pre>
 <p>
 and in Java:
 <pre><code>
   // To create DataFrame using SQLContext
   DataFrame people = sqlContext.read().parquet("...");
   DataFrame department = sqlContext.read().parquet("...");

   people.filter("age".gt(30))
     .join(department, people.col("deptId").equalTo(department("id")))
     .groupBy(department.col("name"), "gender")
     .agg(avg(people.col("salary")), max(people.col("age")));
 </code></pre>
 <p>
<P>

<P>
<DL>
<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
<DT><B>See Also:</B><DD><A HREF="../../../../serialized-form.html#org.apache.spark.sql.DataFrame">Serialized Form</A></DL>
<HR>

<P>

<!-- ======== CONSTRUCTOR SUMMARY ======== -->

<A NAME="constructor_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Constructor Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#DataFrame(org.apache.spark.sql.SQLContext, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)">DataFrame</A></B>(<A HREF="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</A>&nbsp;sqlContext,
          org.apache.spark.sql.catalyst.plans.logical.LogicalPlan&nbsp;logicalPlan)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A constructor that automatically analyzes the logical plan.</TD>
</TR>
</TABLE>
&nbsp;
<!-- ========== METHOD SUMMARY =========== -->

<A NAME="method_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Method Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#agg(org.apache.spark.sql.Column, org.apache.spark.sql.Column...)">agg</A></B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;expr,
    <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;exprs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Aggregates on the entire <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> without groups.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#agg(org.apache.spark.sql.Column, scala.collection.Seq)">agg</A></B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;expr,
    scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;exprs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Aggregates on the entire <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> without groups.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#agg(scala.collection.immutable.Map)">agg</A></B>(scala.collection.immutable.Map&lt;String,String&gt;&nbsp;exprs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Scala-specific) Aggregates on the entire <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> without groups.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#agg(java.util.Map)">agg</A></B>(java.util.Map&lt;String,String&gt;&nbsp;exprs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Java-specific) Aggregates on the entire <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> without groups.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#agg(scala.Tuple2, scala.collection.Seq)">agg</A></B>(scala.Tuple2&lt;String,String&gt;&nbsp;aggExpr,
    scala.collection.Seq&lt;scala.Tuple2&lt;String,String&gt;&gt;&nbsp;aggExprs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Scala-specific) Aggregates on the entire <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> without groups.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#apply(java.lang.String)">apply</A></B>(String&nbsp;colName)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Selects column based on the column name and return it as a <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql"><CODE>Column</CODE></A>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#as(java.lang.String)">as</A></B>(String&nbsp;alias)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with an alias set.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#as(scala.Symbol)">as</A></B>(scala.Symbol&nbsp;alias)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Scala-specific) Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with an alias set.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#cache()">cache</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#coalesce(int)">coalesce</A></B>(int&nbsp;numPartitions)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> that has exactly <code>numPartitions</code> partitions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#col(java.lang.String)">col</A></B>(String&nbsp;colName)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Selects column based on the column name and return it as a <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql"><CODE>Column</CODE></A>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#collect()">collect</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns an array that contains all of <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql"><CODE>Row</CODE></A>s in this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;java.util.List&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#collectAsList()">collectAsList</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a Java list that contains all of <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql"><CODE>Row</CODE></A>s in this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;String[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#columns()">columns</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns all column names as an array.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;long</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#count()">count</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the number of rows in the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#createJDBCTable(java.lang.String, java.lang.String, boolean)">createJDBCTable</A></B>(String&nbsp;url,
                String&nbsp;table,
                boolean&nbsp;allowExisting)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.340, replaced by <code>write().jdbc()</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#cube(org.apache.spark.sql.Column...)">cube</A></B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create a multi-dimensional cube for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#cube(scala.collection.Seq)">cube</A></B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create a multi-dimensional cube for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#cube(java.lang.String, scala.collection.Seq)">cube</A></B>(String&nbsp;col1,
     scala.collection.Seq&lt;String&gt;&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create a multi-dimensional cube for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#cube(java.lang.String, java.lang.String...)">cube</A></B>(String&nbsp;col1,
     String...&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create a multi-dimensional cube for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#describe(scala.collection.Seq)">describe</A></B>(scala.collection.Seq&lt;String&gt;&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Computes statistics for numeric columns, including count, mean, stddev, min, and max.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#describe(java.lang.String...)">describe</A></B>(String...&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Computes statistics for numeric columns, including count, mean, stddev, min, and max.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#distinct()">distinct</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> that contains only the unique rows from this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#drop(org.apache.spark.sql.Column)">drop</A></B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;col)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with a column dropped.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#drop(java.lang.String)">drop</A></B>(String&nbsp;colName)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with a column dropped.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#dropDuplicates()">dropDuplicates</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> that contains only the unique rows from this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#dropDuplicates(scala.collection.Seq)">dropDuplicates</A></B>(scala.collection.Seq&lt;String&gt;&nbsp;colNames)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Scala-specific) Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with duplicate rows removed, considering only
 the subset of columns.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#dropDuplicates(java.lang.String[])">dropDuplicates</A></B>(String[]&nbsp;colNames)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with duplicate rows removed, considering only
 the subset of columns.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;scala.Tuple2&lt;String,String&gt;[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#dtypes()">dtypes</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns all column names and their data types as an array.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#except(org.apache.spark.sql.DataFrame)">except</A></B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;other)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> containing rows in this frame but not in another frame.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#explain()">explain</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Only prints the physical plan to the console for debugging purposes.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#explain(boolean)">explain</A></B>(boolean&nbsp;extended)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Prints the plans (logical and physical) to the console for debugging purposes.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;A extends scala.Product&gt; 
<BR>
<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#explode(scala.collection.Seq, scala.Function1, scala.reflect.api.TypeTags.TypeTag)">explode</A></B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;input,
        scala.Function1&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>,scala.collection.TraversableOnce&lt;A&gt;&gt;&nbsp;f,
        scala.reflect.api.TypeTags.TypeTag&lt;A&gt;&nbsp;evidence$1)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Scala-specific) Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> where each row has been expanded to zero or more
 rows by the provided function.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;A,B&gt; <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#explode(java.lang.String, java.lang.String, scala.Function1, scala.reflect.api.TypeTags.TypeTag)">explode</A></B>(String&nbsp;inputColumn,
        String&nbsp;outputColumn,
        scala.Function1&lt;A,scala.collection.TraversableOnce&lt;B&gt;&gt;&nbsp;f,
        scala.reflect.api.TypeTags.TypeTag&lt;B&gt;&nbsp;evidence$2)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Scala-specific) Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> where a single column has been expanded to zero
 or more rows by the provided function.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#filter(org.apache.spark.sql.Column)">filter</A></B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;condition)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Filters rows using the given condition.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#filter(java.lang.String)">filter</A></B>(String&nbsp;conditionExpr)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Filters rows using the given SQL expression.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#first()">first</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the first row.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;R&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;R&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#flatMap(scala.Function1, scala.reflect.ClassTag)">flatMap</A></B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>,scala.collection.TraversableOnce&lt;R&gt;&gt;&nbsp;f,
        scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$4)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new RDD by first applying a function to all rows of this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>,
 and then flattening the results.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#foreach(scala.Function1)">foreach</A></B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>,scala.runtime.BoxedUnit&gt;&nbsp;f)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Applies a function <code>f</code> to all rows.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#foreachPartition(scala.Function1)">foreachPartition</A></B>(scala.Function1&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>&gt;,scala.runtime.BoxedUnit&gt;&nbsp;f)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Applies a function f to each partition of this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#groupBy(org.apache.spark.sql.Column...)">groupBy</A></B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Groups the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns, so we can run aggregation on them.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#groupBy(scala.collection.Seq)">groupBy</A></B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Groups the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns, so we can run aggregation on them.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#groupBy(java.lang.String, scala.collection.Seq)">groupBy</A></B>(String&nbsp;col1,
        scala.collection.Seq&lt;String&gt;&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Groups the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns, so we can run aggregation on them.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#groupBy(java.lang.String, java.lang.String...)">groupBy</A></B>(String&nbsp;col1,
        String...&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Groups the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns, so we can run aggregation on them.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#head()">head</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the first row.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#head(int)">head</A></B>(int&nbsp;n)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the first <code>n</code> rows.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#insertInto(java.lang.String)">insertInto</A></B>(String&nbsp;tableName)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by
            <code>write().mode(SaveMode.Append).saveAsTable(tableName)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#insertInto(java.lang.String, boolean)">insertInto</A></B>(String&nbsp;tableName,
           boolean&nbsp;overwrite)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by
            <code>write().mode(SaveMode.Append|SaveMode.Overwrite).saveAsTable(tableName)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#insertIntoJDBC(java.lang.String, java.lang.String, boolean)">insertIntoJDBC</A></B>(String&nbsp;url,
               String&nbsp;table,
               boolean&nbsp;overwrite)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().jdbc()</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#intersect(org.apache.spark.sql.DataFrame)">intersect</A></B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;other)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> containing rows only in both this frame and another frame.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;boolean</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#isLocal()">isLocal</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns true if the <code>collect</code> and <code>take</code> methods can be run locally
 (without any Spark executors).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/api/java/JavaRDD.html" title="class in org.apache.spark.api.java">JavaRDD</A>&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#javaRDD()">javaRDD</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the content of the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> as a <CODE>JavaRDD</CODE> of <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql"><CODE>Row</CODE></A>s.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#join(org.apache.spark.sql.DataFrame)">join</A></B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;right)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cartesian join with another <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#join(org.apache.spark.sql.DataFrame, org.apache.spark.sql.Column)">join</A></B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;right,
     <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;joinExprs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Inner join with another <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>, using the given join expression.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#join(org.apache.spark.sql.DataFrame, org.apache.spark.sql.Column, java.lang.String)">join</A></B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;right,
     <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;joinExprs,
     String&nbsp;joinType)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Join with another <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>, using the given join expression.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#join(org.apache.spark.sql.DataFrame, java.lang.String)">join</A></B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;right,
     String&nbsp;usingColumn)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Inner equi-join with another <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the given column.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#limit(int)">limit</A></B>(int&nbsp;n)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> by taking the first <code>n</code> rows.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;R&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;R&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#map(scala.Function1, scala.reflect.ClassTag)">map</A></B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>,R&gt;&nbsp;f,
    scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$3)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new RDD by applying a function to all rows of this DataFrame.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;R&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;R&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#mapPartitions(scala.Function1, scala.reflect.ClassTag)">mapPartitions</A></B>(scala.Function1&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>&gt;,scala.collection.Iterator&lt;R&gt;&gt;&nbsp;f,
              scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$5)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new RDD by applying a function to each partition of this DataFrame.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrameNaFunctions.html" title="class in org.apache.spark.sql">DataFrameNaFunctions</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#na()">na</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a <A HREF="../../../../org/apache/spark/sql/DataFrameNaFunctions.html" title="class in org.apache.spark.sql"><CODE>DataFrameNaFunctions</CODE></A> for working with missing data.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#orderBy(org.apache.spark.sql.Column...)">orderBy</A></B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;sortExprs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the given expressions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#orderBy(scala.collection.Seq)">orderBy</A></B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;sortExprs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the given expressions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#orderBy(java.lang.String, scala.collection.Seq)">orderBy</A></B>(String&nbsp;sortCol,
        scala.collection.Seq&lt;String&gt;&nbsp;sortCols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the given expressions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#orderBy(java.lang.String, java.lang.String...)">orderBy</A></B>(String&nbsp;sortCol,
        String...&nbsp;sortCols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the given expressions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#persist()">persist</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#persist(org.apache.spark.storage.StorageLevel)">persist</A></B>(<A HREF="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A>&nbsp;newLevel)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#printSchema()">printSchema</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Prints the schema to the console in a nice tree format.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;org.apache.spark.sql.SQLContext.QueryExecution</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#queryExecution()">queryExecution</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#randomSplit(double[])">randomSplit</A></B>(double[]&nbsp;weights)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Randomly splits this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with the provided weights.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#randomSplit(double[], long)">randomSplit</A></B>(double[]&nbsp;weights,
            long&nbsp;seed)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Randomly splits this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with the provided weights.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#rdd()">rdd</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Represents the content of the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> as an <CODE>RDD</CODE> of <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql"><CODE>Row</CODE></A>s.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#registerTempTable(java.lang.String)">registerTempTable</A></B>(String&nbsp;tableName)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Registers this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> as a temporary table using the given name.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#repartition(int)">repartition</A></B>(int&nbsp;numPartitions)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> that has exactly <code>numPartitions</code> partitions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#rollup(org.apache.spark.sql.Column...)">rollup</A></B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create a multi-dimensional rollup for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#rollup(scala.collection.Seq)">rollup</A></B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create a multi-dimensional rollup for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#rollup(java.lang.String, scala.collection.Seq)">rollup</A></B>(String&nbsp;col1,
       scala.collection.Seq&lt;String&gt;&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create a multi-dimensional rollup for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#rollup(java.lang.String, java.lang.String...)">rollup</A></B>(String&nbsp;col1,
       String...&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Create a multi-dimensional rollup for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#sample(boolean, double)">sample</A></B>(boolean&nbsp;withReplacement,
       double&nbsp;fraction)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> by sampling a fraction of rows, using a random seed.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#sample(boolean, double, long)">sample</A></B>(boolean&nbsp;withReplacement,
       double&nbsp;fraction,
       long&nbsp;seed)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> by sampling a fraction of rows.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#save(java.lang.String)">save</A></B>(String&nbsp;path)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().save(path)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#save(java.lang.String, org.apache.spark.sql.SaveMode)">save</A></B>(String&nbsp;path,
     <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().mode(mode).save(path)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#save(java.lang.String, org.apache.spark.sql.SaveMode, java.util.Map)">save</A></B>(String&nbsp;source,
     <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode,
     java.util.Map&lt;String,String&gt;&nbsp;options)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by
            <code>write().format(source).mode(mode).options(options).save(path)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#save(java.lang.String, org.apache.spark.sql.SaveMode, scala.collection.immutable.Map)">save</A></B>(String&nbsp;source,
     <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode,
     scala.collection.immutable.Map&lt;String,String&gt;&nbsp;options)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by
            <code>write().format(source).mode(mode).options(options).save(path)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#save(java.lang.String, java.lang.String)">save</A></B>(String&nbsp;path,
     String&nbsp;source)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().format(source).save(path)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#save(java.lang.String, java.lang.String, org.apache.spark.sql.SaveMode)">save</A></B>(String&nbsp;path,
     String&nbsp;source,
     <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().format(source).mode(mode).save(path)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#saveAsParquetFile(java.lang.String)">saveAsParquetFile</A></B>(String&nbsp;path)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().parquet()</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#saveAsTable(java.lang.String)">saveAsTable</A></B>(String&nbsp;tableName)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().saveAsTable(tableName)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#saveAsTable(java.lang.String, org.apache.spark.sql.SaveMode)">saveAsTable</A></B>(String&nbsp;tableName,
            <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().mode(mode).saveAsTable(tableName)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#saveAsTable(java.lang.String, java.lang.String)">saveAsTable</A></B>(String&nbsp;tableName,
            String&nbsp;source)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().format(source).saveAsTable(tableName)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#saveAsTable(java.lang.String, java.lang.String, org.apache.spark.sql.SaveMode)">saveAsTable</A></B>(String&nbsp;tableName,
            String&nbsp;source,
            <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().mode(mode).saveAsTable(tableName)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#saveAsTable(java.lang.String, java.lang.String, org.apache.spark.sql.SaveMode, java.util.Map)">saveAsTable</A></B>(String&nbsp;tableName,
            String&nbsp;source,
            <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode,
            java.util.Map&lt;String,String&gt;&nbsp;options)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by
            <code>write().format(source).mode(mode).options(options).saveAsTable(tableName)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#saveAsTable(java.lang.String, java.lang.String, org.apache.spark.sql.SaveMode, scala.collection.immutable.Map)">saveAsTable</A></B>(String&nbsp;tableName,
            String&nbsp;source,
            <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode,
            scala.collection.immutable.Map&lt;String,String&gt;&nbsp;options)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by
            <code>write().format(source).mode(mode).options(options).saveAsTable(tableName)</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/types/StructType.html" title="class in org.apache.spark.sql.types">StructType</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#schema()">schema</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the schema of this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#select(org.apache.spark.sql.Column...)">select</A></B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Selects a set of column based expressions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#select(scala.collection.Seq)">select</A></B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Selects a set of column based expressions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#select(java.lang.String, scala.collection.Seq)">select</A></B>(String&nbsp;col,
       scala.collection.Seq&lt;String&gt;&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Selects a set of columns.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#select(java.lang.String, java.lang.String...)">select</A></B>(String&nbsp;col,
       String...&nbsp;cols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Selects a set of columns.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#selectExpr(scala.collection.Seq)">selectExpr</A></B>(scala.collection.Seq&lt;String&gt;&nbsp;exprs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Selects a set of SQL expressions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#selectExpr(java.lang.String...)">selectExpr</A></B>(String...&nbsp;exprs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Selects a set of SQL expressions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#show()">show</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Displays the top 20 rows of <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> in a tabular form.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#show(int)">show</A></B>(int&nbsp;numRows)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Displays the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> in a tabular form.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#sort(org.apache.spark.sql.Column...)">sort</A></B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;sortExprs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the given expressions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#sort(scala.collection.Seq)">sort</A></B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;sortExprs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the given expressions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#sort(java.lang.String, scala.collection.Seq)">sort</A></B>(String&nbsp;sortCol,
     scala.collection.Seq&lt;String&gt;&nbsp;sortCols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the specified column, all in ascending order.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#sort(java.lang.String, java.lang.String...)">sort</A></B>(String&nbsp;sortCol,
     String...&nbsp;sortCols)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the specified column, all in ascending order.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#sqlContext()">sqlContext</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrameStatFunctions.html" title="class in org.apache.spark.sql">DataFrameStatFunctions</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#stat()">stat</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a <A HREF="../../../../org/apache/spark/sql/DataFrameStatFunctions.html" title="class in org.apache.spark.sql"><CODE>DataFrameStatFunctions</CODE></A> for working statistic functions support.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#take(int)">take</A></B>(int&nbsp;n)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the first <code>n</code> rows in the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#toDF()">toDF</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the object itself.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#toDF(scala.collection.Seq)">toDF</A></B>(scala.collection.Seq&lt;String&gt;&nbsp;colNames)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with columns renamed.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#toDF(java.lang.String...)">toDF</A></B>(String...&nbsp;colNames)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with columns renamed.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/api/java/JavaRDD.html" title="class in org.apache.spark.api.java">JavaRDD</A>&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#toJavaRDD()">toJavaRDD</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the content of the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> as a <CODE>JavaRDD</CODE> of <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql"><CODE>Row</CODE></A>s.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;String&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#toJSON()">toJSON</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the content of the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> as a RDD of JSON strings.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#toSchemaRDD()">toSchemaRDD</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<B>Deprecated.</B>&nbsp;<I>As of 1.3.0, replaced by <code>toDF()</code>.</I></TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;String</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#toString()">toString</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#unionAll(org.apache.spark.sql.DataFrame)">unionAll</A></B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;other)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> containing union of rows in this frame and another frame.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#unpersist()">unpersist</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#unpersist(boolean)">unpersist</A></B>(boolean&nbsp;blocking)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#where(org.apache.spark.sql.Column)">where</A></B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;condition)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Filters rows using the given condition.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#withColumn(java.lang.String, org.apache.spark.sql.Column)">withColumn</A></B>(String&nbsp;colName,
           <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;col)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> by adding a column.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#withColumnRenamed(java.lang.String, java.lang.String)">withColumnRenamed</A></B>(String&nbsp;existingName,
                  String&nbsp;newName)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with a column renamed.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/sql/DataFrame.html#write()">write</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:: Experimental ::
 Interface for saving the content of the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> out into external storage.</TD>
</TR>
</TABLE>
&nbsp;<A NAME="methods_inherited_from_class_Object"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#EEEEFF" CLASS="TableSubHeadingColor">
<TH ALIGN="left"><B>Methods inherited from class Object</B></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE>equals, getClass, hashCode, notify, notifyAll, wait, wait, wait</CODE></TD>
</TR>
</TABLE>
&nbsp;
<P>

<!-- ========= CONSTRUCTOR DETAIL ======== -->

<A NAME="constructor_detail"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="1"><FONT SIZE="+2">
<B>Constructor Detail</B></FONT></TH>
</TR>
</TABLE>

<A NAME="DataFrame(org.apache.spark.sql.SQLContext, org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)"><!-- --></A><H3>
DataFrame</H3>
<PRE>
public <B>DataFrame</B>(<A HREF="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</A>&nbsp;sqlContext,
                 org.apache.spark.sql.catalyst.plans.logical.LogicalPlan&nbsp;logicalPlan)</PRE>
<DL>
<DD>A constructor that automatically analyzes the logical plan.
 <p>
 This reports error eagerly as the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> is constructed, unless
 <CODE>SQLConf.dataFrameEagerAnalysis</CODE> is turned off.
<P>
<DL>
<DT><B>Parameters:</B><DD><CODE>sqlContext</CODE> - (undocumented)<DD><CODE>logicalPlan</CODE> - (undocumented)</DL>
</DL>

<!-- ============ METHOD DETAIL ========== -->

<A NAME="method_detail"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="1"><FONT SIZE="+2">
<B>Method Detail</B></FONT></TH>
</TR>
</TABLE>

<A NAME="toDF(java.lang.String...)"><!-- --></A><H3>
toDF</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>toDF</B>(String...&nbsp;colNames)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with columns renamed. This can be quite convenient in conversion
 from a RDD of tuples into a <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with meaningful names. For example:
 <pre><code>
   val rdd: RDD[(Int, String)] = ...
   rdd.toDF()  // this implicit conversion creates a DataFrame with column name _1 and _2
   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>colNames</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="sort(java.lang.String, java.lang.String...)"><!-- --></A><H3>
sort</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>sort</B>(String&nbsp;sortCol,
                      String...&nbsp;sortCols)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the specified column, all in ascending order.
 <pre><code>
   // The following 3 are equivalent
   df.sort("sortcol")
   df.sort($"sortcol")
   df.sort($"sortcol".asc)
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sortCol</CODE> - (undocumented)<DD><CODE>sortCols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="sort(org.apache.spark.sql.Column...)"><!-- --></A><H3>
sort</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>sort</B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;sortExprs)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the given expressions. For example:
 <pre><code>
   df.sort($"col1", $"col2".desc)
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sortExprs</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="orderBy(java.lang.String, java.lang.String...)"><!-- --></A><H3>
orderBy</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>orderBy</B>(String&nbsp;sortCol,
                         String...&nbsp;sortCols)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the given expressions.
 This is an alias of the <code>sort</code> function.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sortCol</CODE> - (undocumented)<DD><CODE>sortCols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="orderBy(org.apache.spark.sql.Column...)"><!-- --></A><H3>
orderBy</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>orderBy</B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;sortExprs)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the given expressions.
 This is an alias of the <code>sort</code> function.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sortExprs</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="select(org.apache.spark.sql.Column...)"><!-- --></A><H3>
select</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>select</B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;cols)</PRE>
<DL>
<DD>Selects a set of column based expressions.
 <pre><code>
   df.select($"colA", $"colB" + 1)
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="select(java.lang.String, java.lang.String...)"><!-- --></A><H3>
select</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>select</B>(String&nbsp;col,
                        String...&nbsp;cols)</PRE>
<DL>
<DD>Selects a set of columns. This is a variant of <code>select</code> that can only select
 existing columns using column names (i.e. cannot construct expressions).
 <p>
 <pre><code>
   // The following two are equivalent:
   df.select("colA", "colB")
   df.select($"colA", $"colB")
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>col</CODE> - (undocumented)<DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="selectExpr(java.lang.String...)"><!-- --></A><H3>
selectExpr</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>selectExpr</B>(String...&nbsp;exprs)</PRE>
<DL>
<DD>Selects a set of SQL expressions. This is a variant of <code>select</code> that accepts
 SQL expressions.
 <p>
 <pre><code>
   df.selectExpr("colA", "colB as newName", "abs(colC)")
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>exprs</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="groupBy(org.apache.spark.sql.Column...)"><!-- --></A><H3>
groupBy</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A> <B>groupBy</B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;cols)</PRE>
<DL>
<DD>Groups the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns, so we can run aggregation on them.
 See <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><CODE>GroupedData</CODE></A> for all the available aggregate functions.
 <p>
 <pre><code>
   // Compute the average for all numeric columns grouped by department.
   df.groupBy($"department").avg()

   // Compute the max age and average salary, grouped by department and gender.
   df.groupBy($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="rollup(org.apache.spark.sql.Column...)"><!-- --></A><H3>
rollup</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A> <B>rollup</B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;cols)</PRE>
<DL>
<DD>Create a multi-dimensional rollup for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.
 See <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><CODE>GroupedData</CODE></A> for all the available aggregate functions.
 <p>
 <pre><code>
   // Compute the average for all numeric columns rolluped by department and group.
   df.rollup($"department", $"group").avg()

   // Compute the max age and average salary, rolluped by department and gender.
   df.rollup($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="cube(org.apache.spark.sql.Column...)"><!-- --></A><H3>
cube</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A> <B>cube</B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;cols)</PRE>
<DL>
<DD>Create a multi-dimensional cube for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.
 See <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><CODE>GroupedData</CODE></A> for all the available aggregate functions.
 <p>
 <pre><code>
   // Compute the average for all numeric columns cubed by department and group.
   df.cube($"department", $"group").avg()

   // Compute the max age and average salary, cubed by department and gender.
   df.cube($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="groupBy(java.lang.String, java.lang.String...)"><!-- --></A><H3>
groupBy</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A> <B>groupBy</B>(String&nbsp;col1,
                           String...&nbsp;cols)</PRE>
<DL>
<DD>Groups the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns, so we can run aggregation on them.
 See <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><CODE>GroupedData</CODE></A> for all the available aggregate functions.
 <p>
 This is a variant of groupBy that can only group by existing columns using column names
 (i.e. cannot construct expressions).
 <p>
 <pre><code>
   // Compute the average for all numeric columns grouped by department.
   df.groupBy("department").avg()

   // Compute the max age and average salary, grouped by department and gender.
   df.groupBy($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>col1</CODE> - (undocumented)<DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="rollup(java.lang.String, java.lang.String...)"><!-- --></A><H3>
rollup</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A> <B>rollup</B>(String&nbsp;col1,
                          String...&nbsp;cols)</PRE>
<DL>
<DD>Create a multi-dimensional rollup for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.
 See <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><CODE>GroupedData</CODE></A> for all the available aggregate functions.
 <p>
 This is a variant of rollup that can only group by existing columns using column names
 (i.e. cannot construct expressions).
 <p>
 <pre><code>
   // Compute the average for all numeric columns rolluped by department and group.
   df.rollup("department", "group").avg()

   // Compute the max age and average salary, rolluped by department and gender.
   df.rollup($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>col1</CODE> - (undocumented)<DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="cube(java.lang.String, java.lang.String...)"><!-- --></A><H3>
cube</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A> <B>cube</B>(String&nbsp;col1,
                        String...&nbsp;cols)</PRE>
<DL>
<DD>Create a multi-dimensional cube for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.
 See <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><CODE>GroupedData</CODE></A> for all the available aggregate functions.
 <p>
 This is a variant of cube that can only group by existing columns using column names
 (i.e. cannot construct expressions).
 <p>
 <pre><code>
   // Compute the average for all numeric columns cubed by department and group.
   df.cube("department", "group").avg()

   // Compute the max age and average salary, cubed by department and gender.
   df.cube($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>col1</CODE> - (undocumented)<DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="agg(org.apache.spark.sql.Column, org.apache.spark.sql.Column...)"><!-- --></A><H3>
agg</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>agg</B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;expr,
                     <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>...&nbsp;exprs)</PRE>
<DL>
<DD>Aggregates on the entire <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> without groups.
 <pre><code>
   // df.agg(...) is a shorthand for df.groupBy().agg(...)
   df.agg(max($"age"), avg($"salary"))
   df.groupBy().agg(max($"age"), avg($"salary"))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>expr</CODE> - (undocumented)<DD><CODE>exprs</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="describe(java.lang.String...)"><!-- --></A><H3>
describe</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>describe</B>(String...&nbsp;cols)</PRE>
<DL>
<DD>Computes statistics for numeric columns, including count, mean, stddev, min, and max.
 If no columns are given, this function computes statistics for all numerical columns.
 <p>
 This function is meant for exploratory data analysis, as we make no guarantee about the
 backward compatibility of the schema of the resulting <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>. If you want to
 programmatically compute summary statistics, use the <code>agg</code> function instead.
 <p>
 <pre><code>
   df.describe("age", "height").show()

   // output:
   // summary age   height
   // count   10.0  10.0
   // mean    53.3  178.05
   // stddev  11.6  15.7
   // min     18.0  163.0
   // max     92.0  192.0
 </code></pre>
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.1</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="sqlContext()"><!-- --></A><H3>
sqlContext</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql">SQLContext</A> <B>sqlContext</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="queryExecution()"><!-- --></A><H3>
queryExecution</H3>
<PRE>
public org.apache.spark.sql.SQLContext.QueryExecution <B>queryExecution</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="toString()"><!-- --></A><H3>
toString</H3>
<PRE>
public String <B>toString</B>()</PRE>
<DL>
<DD><DL>
<DT><B>Overrides:</B><DD><CODE>toString</CODE> in class <CODE>Object</CODE></DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="toDF()"><!-- --></A><H3>
toDF</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>toDF</B>()</PRE>
<DL>
<DD>Returns the object itself.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="toDF(scala.collection.Seq)"><!-- --></A><H3>
toDF</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>toDF</B>(scala.collection.Seq&lt;String&gt;&nbsp;colNames)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with columns renamed. This can be quite convenient in conversion
 from a RDD of tuples into a <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with meaningful names. For example:
 <pre><code>
   val rdd: RDD[(Int, String)] = ...
   rdd.toDF()  // this implicit conversion creates a DataFrame with column name _1 and _2
   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>colNames</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="schema()"><!-- --></A><H3>
schema</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/types/StructType.html" title="class in org.apache.spark.sql.types">StructType</A> <B>schema</B>()</PRE>
<DL>
<DD>Returns the schema of this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="dtypes()"><!-- --></A><H3>
dtypes</H3>
<PRE>
public scala.Tuple2&lt;String,String&gt;[] <B>dtypes</B>()</PRE>
<DL>
<DD>Returns all column names and their data types as an array.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="columns()"><!-- --></A><H3>
columns</H3>
<PRE>
public String[] <B>columns</B>()</PRE>
<DL>
<DD>Returns all column names as an array.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="printSchema()"><!-- --></A><H3>
printSchema</H3>
<PRE>
public void <B>printSchema</B>()</PRE>
<DL>
<DD>Prints the schema to the console in a nice tree format.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="explain(boolean)"><!-- --></A><H3>
explain</H3>
<PRE>
public void <B>explain</B>(boolean&nbsp;extended)</PRE>
<DL>
<DD>Prints the plans (logical and physical) to the console for debugging purposes.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>extended</CODE> - (undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="explain()"><!-- --></A><H3>
explain</H3>
<PRE>
public void <B>explain</B>()</PRE>
<DL>
<DD>Only prints the physical plan to the console for debugging purposes.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="isLocal()"><!-- --></A><H3>
isLocal</H3>
<PRE>
public boolean <B>isLocal</B>()</PRE>
<DL>
<DD>Returns true if the <code>collect</code> and <code>take</code> methods can be run locally
 (without any Spark executors).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="show(int)"><!-- --></A><H3>
show</H3>
<PRE>
public void <B>show</B>(int&nbsp;numRows)</PRE>
<DL>
<DD>Displays the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> in a tabular form. For example:
 <pre><code>
   year  month AVG('Adj Close) MAX('Adj Close)
   1980  12    0.503218        0.595103
   1981  01    0.523289        0.570307
   1982  02    0.436504        0.475256
   1983  03    0.410516        0.442194
   1984  04    0.450090        0.483521
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>numRows</CODE> - Number of rows to show
 <p><DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="show()"><!-- --></A><H3>
show</H3>
<PRE>
public void <B>show</B>()</PRE>
<DL>
<DD>Displays the top 20 rows of <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> in a tabular form.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="na()"><!-- --></A><H3>
na</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrameNaFunctions.html" title="class in org.apache.spark.sql">DataFrameNaFunctions</A> <B>na</B>()</PRE>
<DL>
<DD>Returns a <A HREF="../../../../org/apache/spark/sql/DataFrameNaFunctions.html" title="class in org.apache.spark.sql"><CODE>DataFrameNaFunctions</CODE></A> for working with missing data.
 <pre><code>
   // Dropping rows containing any null values.
   df.na.drop()
 </code></pre>
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.1</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="stat()"><!-- --></A><H3>
stat</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrameStatFunctions.html" title="class in org.apache.spark.sql">DataFrameStatFunctions</A> <B>stat</B>()</PRE>
<DL>
<DD>Returns a <A HREF="../../../../org/apache/spark/sql/DataFrameStatFunctions.html" title="class in org.apache.spark.sql"><CODE>DataFrameStatFunctions</CODE></A> for working statistic functions support.
 <pre><code>
   // Finding frequent items in column with name 'a'.
   df.stat.freqItems(Seq("a"))
 </code></pre>
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="join(org.apache.spark.sql.DataFrame)"><!-- --></A><H3>
join</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>join</B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;right)</PRE>
<DL>
<DD>Cartesian join with another <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.
 <p>
 Note that cartesian joins are very expensive without an extra filter that can be pushed down.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>right</CODE> - Right side of the join operation.
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="join(org.apache.spark.sql.DataFrame, java.lang.String)"><!-- --></A><H3>
join</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>join</B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;right,
                      String&nbsp;usingColumn)</PRE>
<DL>
<DD>Inner equi-join with another <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the given column.
 <p>
 Different from other join functions, the join column will only appear once in the output,
 i.e. similar to SQL's <code>JOIN USING</code> syntax.
 <p>
 <pre><code>
   // Joining df1 and df2 using the column "user_id"
   df1.join(df2, "user_id")
 </code></pre>
 <p>
 Note that if you perform a self-join using this function without aliasing the input
 <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>s, you will NOT be able to reference any columns after the join, since
 there is no way to disambiguate which side of the join you would like to reference.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>right</CODE> - Right side of the join operation.<DD><CODE>usingColumn</CODE> - Name of the column to join on. This column must exist on both sides.
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="join(org.apache.spark.sql.DataFrame, org.apache.spark.sql.Column)"><!-- --></A><H3>
join</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>join</B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;right,
                      <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;joinExprs)</PRE>
<DL>
<DD>Inner join with another <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>, using the given join expression.
 <p>
 <pre><code>
   // The following two are equivalent:
   df1.join(df2, $"df1Key" === $"df2Key")
   df1.join(df2).where($"df1Key" === $"df2Key")
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>right</CODE> - (undocumented)<DD><CODE>joinExprs</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="join(org.apache.spark.sql.DataFrame, org.apache.spark.sql.Column, java.lang.String)"><!-- --></A><H3>
join</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>join</B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;right,
                      <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;joinExprs,
                      String&nbsp;joinType)</PRE>
<DL>
<DD>Join with another <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>, using the given join expression. The following performs
 a full outer join between <code>df1</code> and <code>df2</code>.
 <p>
 <pre><code>
   // Scala:
   import org.apache.spark.sql.functions._
   df1.join(df2, $"df1Key" === $"df2Key", "outer")

   // Java:
   import static org.apache.spark.sql.functions.*;
   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
 </code></pre>
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>right</CODE> - Right side of the join.<DD><CODE>joinExprs</CODE> - Join expression.<DD><CODE>joinType</CODE> - One of: <code>inner</code>, <code>outer</code>, <code>left_outer</code>, <code>right_outer</code>, <code>leftsemi</code>.
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="sort(java.lang.String, scala.collection.Seq)"><!-- --></A><H3>
sort</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>sort</B>(String&nbsp;sortCol,
                      scala.collection.Seq&lt;String&gt;&nbsp;sortCols)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the specified column, all in ascending order.
 <pre><code>
   // The following 3 are equivalent
   df.sort("sortcol")
   df.sort($"sortcol")
   df.sort($"sortcol".asc)
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sortCol</CODE> - (undocumented)<DD><CODE>sortCols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="sort(scala.collection.Seq)"><!-- --></A><H3>
sort</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>sort</B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;sortExprs)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the given expressions. For example:
 <pre><code>
   df.sort($"col1", $"col2".desc)
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sortExprs</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="orderBy(java.lang.String, scala.collection.Seq)"><!-- --></A><H3>
orderBy</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>orderBy</B>(String&nbsp;sortCol,
                         scala.collection.Seq&lt;String&gt;&nbsp;sortCols)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the given expressions.
 This is an alias of the <code>sort</code> function.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sortCol</CODE> - (undocumented)<DD><CODE>sortCols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="orderBy(scala.collection.Seq)"><!-- --></A><H3>
orderBy</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>orderBy</B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;sortExprs)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> sorted by the given expressions.
 This is an alias of the <code>sort</code> function.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sortExprs</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="apply(java.lang.String)"><!-- --></A><H3>
apply</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A> <B>apply</B>(String&nbsp;colName)</PRE>
<DL>
<DD>Selects column based on the column name and return it as a <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql"><CODE>Column</CODE></A>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>colName</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="col(java.lang.String)"><!-- --></A><H3>
col</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A> <B>col</B>(String&nbsp;colName)</PRE>
<DL>
<DD>Selects column based on the column name and return it as a <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql"><CODE>Column</CODE></A>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>colName</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="as(java.lang.String)"><!-- --></A><H3>
as</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>as</B>(String&nbsp;alias)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with an alias set.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>alias</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="as(scala.Symbol)"><!-- --></A><H3>
as</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>as</B>(scala.Symbol&nbsp;alias)</PRE>
<DL>
<DD>(Scala-specific) Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with an alias set.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>alias</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="select(scala.collection.Seq)"><!-- --></A><H3>
select</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>select</B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;cols)</PRE>
<DL>
<DD>Selects a set of column based expressions.
 <pre><code>
   df.select($"colA", $"colB" + 1)
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="select(java.lang.String, scala.collection.Seq)"><!-- --></A><H3>
select</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>select</B>(String&nbsp;col,
                        scala.collection.Seq&lt;String&gt;&nbsp;cols)</PRE>
<DL>
<DD>Selects a set of columns. This is a variant of <code>select</code> that can only select
 existing columns using column names (i.e. cannot construct expressions).
 <p>
 <pre><code>
   // The following two are equivalent:
   df.select("colA", "colB")
   df.select($"colA", $"colB")
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>col</CODE> - (undocumented)<DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="selectExpr(scala.collection.Seq)"><!-- --></A><H3>
selectExpr</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>selectExpr</B>(scala.collection.Seq&lt;String&gt;&nbsp;exprs)</PRE>
<DL>
<DD>Selects a set of SQL expressions. This is a variant of <code>select</code> that accepts
 SQL expressions.
 <p>
 <pre><code>
   df.selectExpr("colA", "colB as newName", "abs(colC)")
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>exprs</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="filter(org.apache.spark.sql.Column)"><!-- --></A><H3>
filter</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>filter</B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;condition)</PRE>
<DL>
<DD>Filters rows using the given condition.
 <pre><code>
   // The following are equivalent:
   peopleDf.filter($"age" &gt; 15)
   peopleDf.where($"age" &gt; 15)
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>condition</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="filter(java.lang.String)"><!-- --></A><H3>
filter</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>filter</B>(String&nbsp;conditionExpr)</PRE>
<DL>
<DD>Filters rows using the given SQL expression.
 <pre><code>
   peopleDf.filter("age &gt; 15")
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>conditionExpr</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="where(org.apache.spark.sql.Column)"><!-- --></A><H3>
where</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>where</B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;condition)</PRE>
<DL>
<DD>Filters rows using the given condition. This is an alias for <code>filter</code>.
 <pre><code>
   // The following are equivalent:
   peopleDf.filter($"age" &gt; 15)
   peopleDf.where($"age" &gt; 15)
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>condition</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="groupBy(scala.collection.Seq)"><!-- --></A><H3>
groupBy</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A> <B>groupBy</B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;cols)</PRE>
<DL>
<DD>Groups the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns, so we can run aggregation on them.
 See <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><CODE>GroupedData</CODE></A> for all the available aggregate functions.
 <p>
 <pre><code>
   // Compute the average for all numeric columns grouped by department.
   df.groupBy($"department").avg()

   // Compute the max age and average salary, grouped by department and gender.
   df.groupBy($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="rollup(scala.collection.Seq)"><!-- --></A><H3>
rollup</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A> <B>rollup</B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;cols)</PRE>
<DL>
<DD>Create a multi-dimensional rollup for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.
 See <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><CODE>GroupedData</CODE></A> for all the available aggregate functions.
 <p>
 <pre><code>
   // Compute the average for all numeric columns rolluped by department and group.
   df.rollup($"department", $"group").avg()

   // Compute the max age and average salary, rolluped by department and gender.
   df.rollup($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="cube(scala.collection.Seq)"><!-- --></A><H3>
cube</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A> <B>cube</B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;cols)</PRE>
<DL>
<DD>Create a multi-dimensional cube for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.
 See <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><CODE>GroupedData</CODE></A> for all the available aggregate functions.
 <p>
 <pre><code>
   // Compute the average for all numeric columns cubed by department and group.
   df.cube($"department", $"group").avg()

   // Compute the max age and average salary, cubed by department and gender.
   df.cube($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="groupBy(java.lang.String, scala.collection.Seq)"><!-- --></A><H3>
groupBy</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A> <B>groupBy</B>(String&nbsp;col1,
                           scala.collection.Seq&lt;String&gt;&nbsp;cols)</PRE>
<DL>
<DD>Groups the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns, so we can run aggregation on them.
 See <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><CODE>GroupedData</CODE></A> for all the available aggregate functions.
 <p>
 This is a variant of groupBy that can only group by existing columns using column names
 (i.e. cannot construct expressions).
 <p>
 <pre><code>
   // Compute the average for all numeric columns grouped by department.
   df.groupBy("department").avg()

   // Compute the max age and average salary, grouped by department and gender.
   df.groupBy($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>col1</CODE> - (undocumented)<DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="rollup(java.lang.String, scala.collection.Seq)"><!-- --></A><H3>
rollup</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A> <B>rollup</B>(String&nbsp;col1,
                          scala.collection.Seq&lt;String&gt;&nbsp;cols)</PRE>
<DL>
<DD>Create a multi-dimensional rollup for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.
 See <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><CODE>GroupedData</CODE></A> for all the available aggregate functions.
 <p>
 This is a variant of rollup that can only group by existing columns using column names
 (i.e. cannot construct expressions).
 <p>
 <pre><code>
   // Compute the average for all numeric columns rolluped by department and group.
   df.rollup("department", "group").avg()

   // Compute the max age and average salary, rolluped by department and gender.
   df.rollup($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>col1</CODE> - (undocumented)<DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="cube(java.lang.String, scala.collection.Seq)"><!-- --></A><H3>
cube</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql">GroupedData</A> <B>cube</B>(String&nbsp;col1,
                        scala.collection.Seq&lt;String&gt;&nbsp;cols)</PRE>
<DL>
<DD>Create a multi-dimensional cube for the current <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> using the specified columns,
 so we can run aggregation on them.
 See <A HREF="../../../../org/apache/spark/sql/GroupedData.html" title="class in org.apache.spark.sql"><CODE>GroupedData</CODE></A> for all the available aggregate functions.
 <p>
 This is a variant of cube that can only group by existing columns using column names
 (i.e. cannot construct expressions).
 <p>
 <pre><code>
   // Compute the average for all numeric columns cubed by department and group.
   df.cube("department", "group").avg()

   // Compute the max age and average salary, cubed by department and gender.
   df.cube($"department", $"gender").agg(Map(
     "salary" -&gt; "avg",
     "age" -&gt; "max"
   ))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>col1</CODE> - (undocumented)<DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="agg(scala.Tuple2, scala.collection.Seq)"><!-- --></A><H3>
agg</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>agg</B>(scala.Tuple2&lt;String,String&gt;&nbsp;aggExpr,
                     scala.collection.Seq&lt;scala.Tuple2&lt;String,String&gt;&gt;&nbsp;aggExprs)</PRE>
<DL>
<DD>(Scala-specific) Aggregates on the entire <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> without groups.
 <pre><code>
   // df.agg(...) is a shorthand for df.groupBy().agg(...)
   df.agg("age" -&gt; "max", "salary" -&gt; "avg")
   df.groupBy().agg("age" -&gt; "max", "salary" -&gt; "avg")
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>aggExpr</CODE> - (undocumented)<DD><CODE>aggExprs</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="agg(scala.collection.immutable.Map)"><!-- --></A><H3>
agg</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>agg</B>(scala.collection.immutable.Map&lt;String,String&gt;&nbsp;exprs)</PRE>
<DL>
<DD>(Scala-specific) Aggregates on the entire <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> without groups.
 <pre><code>
   // df.agg(...) is a shorthand for df.groupBy().agg(...)
   df.agg(Map("age" -&gt; "max", "salary" -&gt; "avg"))
   df.groupBy().agg(Map("age" -&gt; "max", "salary" -&gt; "avg"))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>exprs</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="agg(java.util.Map)"><!-- --></A><H3>
agg</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>agg</B>(java.util.Map&lt;String,String&gt;&nbsp;exprs)</PRE>
<DL>
<DD>(Java-specific) Aggregates on the entire <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> without groups.
 <pre><code>
   // df.agg(...) is a shorthand for df.groupBy().agg(...)
   df.agg(Map("age" -&gt; "max", "salary" -&gt; "avg"))
   df.groupBy().agg(Map("age" -&gt; "max", "salary" -&gt; "avg"))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>exprs</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="agg(org.apache.spark.sql.Column, scala.collection.Seq)"><!-- --></A><H3>
agg</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>agg</B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;expr,
                     scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;exprs)</PRE>
<DL>
<DD>Aggregates on the entire <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> without groups.
 <pre><code>
   // df.agg(...) is a shorthand for df.groupBy().agg(...)
   df.agg(max($"age"), avg($"salary"))
   df.groupBy().agg(max($"age"), avg($"salary"))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>expr</CODE> - (undocumented)<DD><CODE>exprs</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="limit(int)"><!-- --></A><H3>
limit</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>limit</B>(int&nbsp;n)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> by taking the first <code>n</code> rows. The difference between this function
 and <code>head</code> is that <code>head</code> returns an array while <code>limit</code> returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>n</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="unionAll(org.apache.spark.sql.DataFrame)"><!-- --></A><H3>
unionAll</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>unionAll</B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;other)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> containing union of rows in this frame and another frame.
 This is equivalent to <code>UNION ALL</code> in SQL.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>other</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="intersect(org.apache.spark.sql.DataFrame)"><!-- --></A><H3>
intersect</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>intersect</B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;other)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> containing rows only in both this frame and another frame.
 This is equivalent to <code>INTERSECT</code> in SQL.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>other</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="except(org.apache.spark.sql.DataFrame)"><!-- --></A><H3>
except</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>except</B>(<A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>&nbsp;other)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> containing rows in this frame but not in another frame.
 This is equivalent to <code>EXCEPT</code> in SQL.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>other</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="sample(boolean, double, long)"><!-- --></A><H3>
sample</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>sample</B>(boolean&nbsp;withReplacement,
                        double&nbsp;fraction,
                        long&nbsp;seed)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> by sampling a fraction of rows.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>withReplacement</CODE> - Sample with replacement or not.<DD><CODE>fraction</CODE> - Fraction of rows to generate.<DD><CODE>seed</CODE> - Seed for sampling.
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="sample(boolean, double)"><!-- --></A><H3>
sample</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>sample</B>(boolean&nbsp;withReplacement,
                        double&nbsp;fraction)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> by sampling a fraction of rows, using a random seed.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>withReplacement</CODE> - Sample with replacement or not.<DD><CODE>fraction</CODE> - Fraction of rows to generate.
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="randomSplit(double[], long)"><!-- --></A><H3>
randomSplit</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>[] <B>randomSplit</B>(double[]&nbsp;weights,
                               long&nbsp;seed)</PRE>
<DL>
<DD>Randomly splits this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with the provided weights.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>weights</CODE> - weights for splits, will be normalized if they don't sum to 1.<DD><CODE>seed</CODE> - Seed for sampling.
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="randomSplit(double[])"><!-- --></A><H3>
randomSplit</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A>[] <B>randomSplit</B>(double[]&nbsp;weights)</PRE>
<DL>
<DD>Randomly splits this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with the provided weights.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>weights</CODE> - weights for splits, will be normalized if they don't sum to 1.
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="explode(scala.collection.Seq, scala.Function1, scala.reflect.api.TypeTags.TypeTag)"><!-- --></A><H3>
explode</H3>
<PRE>
public &lt;A extends scala.Product&gt; <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>explode</B>(scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&gt;&nbsp;input,
                                                   scala.Function1&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>,scala.collection.TraversableOnce&lt;A&gt;&gt;&nbsp;f,
                                                   scala.reflect.api.TypeTags.TypeTag&lt;A&gt;&nbsp;evidence$1)</PRE>
<DL>
<DD>(Scala-specific) Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> where each row has been expanded to zero or more
 rows by the provided function.  This is similar to a <code>LATERAL VIEW</code> in HiveQL. The columns of
 the input row are implicitly joined with each row that is output by the function.
 <p>
 The following example uses this function to count the number of books which contain
 a given word:
 <p>
 <pre><code>
   case class Book(title: String, words: String)
   val df: RDD[Book]

   case class Word(word: String)
   val allWords = df.explode('words) {
     case Row(words: String) =&gt; words.split(" ").map(Word(_))
   }

   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>input</CODE> - (undocumented)<DD><CODE>f</CODE> - (undocumented)<DD><CODE>evidence$1</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="explode(java.lang.String, java.lang.String, scala.Function1, scala.reflect.api.TypeTags.TypeTag)"><!-- --></A><H3>
explode</H3>
<PRE>
public &lt;A,B&gt; <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>explode</B>(String&nbsp;inputColumn,
                               String&nbsp;outputColumn,
                               scala.Function1&lt;A,scala.collection.TraversableOnce&lt;B&gt;&gt;&nbsp;f,
                               scala.reflect.api.TypeTags.TypeTag&lt;B&gt;&nbsp;evidence$2)</PRE>
<DL>
<DD>(Scala-specific) Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> where a single column has been expanded to zero
 or more rows by the provided function.  This is similar to a <code>LATERAL VIEW</code> in HiveQL. All
 columns of the input row are implicitly joined with each value that is output by the function.
 <p>
 <pre><code>
   df.explode("words", "word"){words: String =&gt; words.split(" ")}
 </code></pre>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>inputColumn</CODE> - (undocumented)<DD><CODE>outputColumn</CODE> - (undocumented)<DD><CODE>f</CODE> - (undocumented)<DD><CODE>evidence$2</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="withColumn(java.lang.String, org.apache.spark.sql.Column)"><!-- --></A><H3>
withColumn</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>withColumn</B>(String&nbsp;colName,
                            <A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;col)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> by adding a column.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>colName</CODE> - (undocumented)<DD><CODE>col</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="withColumnRenamed(java.lang.String, java.lang.String)"><!-- --></A><H3>
withColumnRenamed</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>withColumnRenamed</B>(String&nbsp;existingName,
                                   String&nbsp;newName)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with a column renamed.
 This is a no-op if schema doesn't contain existingName.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>existingName</CODE> - (undocumented)<DD><CODE>newName</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="drop(java.lang.String)"><!-- --></A><H3>
drop</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>drop</B>(String&nbsp;colName)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with a column dropped.
 This is a no-op if schema doesn't contain column name.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>colName</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="drop(org.apache.spark.sql.Column)"><!-- --></A><H3>
drop</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>drop</B>(<A HREF="../../../../org/apache/spark/sql/Column.html" title="class in org.apache.spark.sql">Column</A>&nbsp;col)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with a column dropped.
 This version of drop accepts a Column rather than a name.
 This is a no-op if the DataFrame doesn't have a column
 with an equivalent expression.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>col</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.1</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="dropDuplicates()"><!-- --></A><H3>
dropDuplicates</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>dropDuplicates</B>()</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> that contains only the unique rows from this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.
 This is an alias for <code>distinct</code>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="dropDuplicates(scala.collection.Seq)"><!-- --></A><H3>
dropDuplicates</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>dropDuplicates</B>(scala.collection.Seq&lt;String&gt;&nbsp;colNames)</PRE>
<DL>
<DD>(Scala-specific) Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with duplicate rows removed, considering only
 the subset of columns.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>colNames</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="dropDuplicates(java.lang.String[])"><!-- --></A><H3>
dropDuplicates</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>dropDuplicates</B>(String[]&nbsp;colNames)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> with duplicate rows removed, considering only
 the subset of columns.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>colNames</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="describe(scala.collection.Seq)"><!-- --></A><H3>
describe</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>describe</B>(scala.collection.Seq&lt;String&gt;&nbsp;cols)</PRE>
<DL>
<DD>Computes statistics for numeric columns, including count, mean, stddev, min, and max.
 If no columns are given, this function computes statistics for all numerical columns.
 <p>
 This function is meant for exploratory data analysis, as we make no guarantee about the
 backward compatibility of the schema of the resulting <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>. If you want to
 programmatically compute summary statistics, use the <code>agg</code> function instead.
 <p>
 <pre><code>
   df.describe("age", "height").show()

   // output:
   // summary age   height
   // count   10.0  10.0
   // mean    53.3  178.05
   // stddev  11.6  15.7
   // min     18.0  163.0
   // max     92.0  192.0
 </code></pre>
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>cols</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.1</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="head(int)"><!-- --></A><H3>
head</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>[] <B>head</B>(int&nbsp;n)</PRE>
<DL>
<DD>Returns the first <code>n</code> rows.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>n</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="head()"><!-- --></A><H3>
head</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A> <B>head</B>()</PRE>
<DL>
<DD>Returns the first row.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="first()"><!-- --></A><H3>
first</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A> <B>first</B>()</PRE>
<DL>
<DD>Returns the first row. Alias for head().
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="map(scala.Function1, scala.reflect.ClassTag)"><!-- --></A><H3>
map</H3>
<PRE>
public &lt;R&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;R&gt; <B>map</B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>,R&gt;&nbsp;f,
                      scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$3)</PRE>
<DL>
<DD>Returns a new RDD by applying a function to all rows of this DataFrame.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>evidence$3</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="flatMap(scala.Function1, scala.reflect.ClassTag)"><!-- --></A><H3>
flatMap</H3>
<PRE>
public &lt;R&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;R&gt; <B>flatMap</B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>,scala.collection.TraversableOnce&lt;R&gt;&gt;&nbsp;f,
                          scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$4)</PRE>
<DL>
<DD>Returns a new RDD by first applying a function to all rows of this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>,
 and then flattening the results.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>evidence$4</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="mapPartitions(scala.Function1, scala.reflect.ClassTag)"><!-- --></A><H3>
mapPartitions</H3>
<PRE>
public &lt;R&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;R&gt; <B>mapPartitions</B>(scala.Function1&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>&gt;,scala.collection.Iterator&lt;R&gt;&gt;&nbsp;f,
                                scala.reflect.ClassTag&lt;R&gt;&nbsp;evidence$5)</PRE>
<DL>
<DD>Returns a new RDD by applying a function to each partition of this DataFrame.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>evidence$5</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="foreach(scala.Function1)"><!-- --></A><H3>
foreach</H3>
<PRE>
public void <B>foreach</B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>,scala.runtime.BoxedUnit&gt;&nbsp;f)</PRE>
<DL>
<DD>Applies a function <code>f</code> to all rows.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="foreachPartition(scala.Function1)"><!-- --></A><H3>
foreachPartition</H3>
<PRE>
public void <B>foreachPartition</B>(scala.Function1&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>&gt;,scala.runtime.BoxedUnit&gt;&nbsp;f)</PRE>
<DL>
<DD>Applies a function f to each partition of this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="take(int)"><!-- --></A><H3>
take</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>[] <B>take</B>(int&nbsp;n)</PRE>
<DL>
<DD>Returns the first <code>n</code> rows in the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>n</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="collect()"><!-- --></A><H3>
collect</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>[] <B>collect</B>()</PRE>
<DL>
<DD>Returns an array that contains all of <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql"><CODE>Row</CODE></A>s in this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="collectAsList()"><!-- --></A><H3>
collectAsList</H3>
<PRE>
public java.util.List&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>&gt; <B>collectAsList</B>()</PRE>
<DL>
<DD>Returns a Java list that contains all of <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql"><CODE>Row</CODE></A>s in this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="count()"><!-- --></A><H3>
count</H3>
<PRE>
public long <B>count</B>()</PRE>
<DL>
<DD>Returns the number of rows in the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="repartition(int)"><!-- --></A><H3>
repartition</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>repartition</B>(int&nbsp;numPartitions)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> that has exactly <code>numPartitions</code> partitions.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>numPartitions</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="coalesce(int)"><!-- --></A><H3>
coalesce</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>coalesce</B>(int&nbsp;numPartitions)</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> that has exactly <code>numPartitions</code> partitions.
 Similar to coalesce defined on an <CODE>RDD</CODE>, this operation results in a narrow dependency, e.g.
 if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of
 the 100 new partitions will claim 10 of the current partitions.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>numPartitions</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="distinct()"><!-- --></A><H3>
distinct</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>distinct</B>()</PRE>
<DL>
<DD>Returns a new <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> that contains only the unique rows from this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>.
 This is an alias for <code>dropDuplicates</code>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="persist()"><!-- --></A><H3>
persist</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>persist</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="cache()"><!-- --></A><H3>
cache</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>cache</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="persist(org.apache.spark.storage.StorageLevel)"><!-- --></A><H3>
persist</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>persist</B>(<A HREF="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A>&nbsp;newLevel)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>newLevel</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="unpersist(boolean)"><!-- --></A><H3>
unpersist</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>unpersist</B>(boolean&nbsp;blocking)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>blocking</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="unpersist()"><!-- --></A><H3>
unpersist</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>unpersist</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="rdd()"><!-- --></A><H3>
rdd</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>&gt; <B>rdd</B>()</PRE>
<DL>
<DD>Represents the content of the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> as an <CODE>RDD</CODE> of <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql"><CODE>Row</CODE></A>s. Note that the RDD is
 memoized. Once called, it won't change even if you change any query planning related Spark SQL
 configurations (e.g. <code>spark.sql.shuffle.partitions</code>).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="toJavaRDD()"><!-- --></A><H3>
toJavaRDD</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/api/java/JavaRDD.html" title="class in org.apache.spark.api.java">JavaRDD</A>&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>&gt; <B>toJavaRDD</B>()</PRE>
<DL>
<DD>Returns the content of the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> as a <CODE>JavaRDD</CODE> of <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql"><CODE>Row</CODE></A>s.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="javaRDD()"><!-- --></A><H3>
javaRDD</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/api/java/JavaRDD.html" title="class in org.apache.spark.api.java">JavaRDD</A>&lt;<A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql">Row</A>&gt; <B>javaRDD</B>()</PRE>
<DL>
<DD>Returns the content of the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> as a <CODE>JavaRDD</CODE> of <A HREF="../../../../org/apache/spark/sql/Row.html" title="interface in org.apache.spark.sql"><CODE>Row</CODE></A>s.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="registerTempTable(java.lang.String)"><!-- --></A><H3>
registerTempTable</H3>
<PRE>
public void <B>registerTempTable</B>(String&nbsp;tableName)</PRE>
<DL>
<DD>Registers this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> as a temporary table using the given name.  The lifetime of this
 temporary table is tied to the <A HREF="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql"><CODE>SQLContext</CODE></A> that was used to create this DataFrame.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>tableName</CODE> - (undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="write()"><!-- --></A><H3>
write</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrameWriter.html" title="class in org.apache.spark.sql">DataFrameWriter</A> <B>write</B>()</PRE>
<DL>
<DD>:: Experimental ::
 Interface for saving the content of the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> out into external storage.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.4.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="toJSON()"><!-- --></A><H3>
toJSON</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;String&gt; <B>toJSON</B>()</PRE>
<DL>
<DD>Returns the content of the <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> as a RDD of JSON strings.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)<DT><B>Since:</B></DT>
  <DD>1.3.0</DD>
</DL>
</DD>
</DL>
<HR>

<A NAME="toSchemaRDD()"><!-- --></A><H3>
toSchemaRDD</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql">DataFrame</A> <B>toSchemaRDD</B>()</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.3.0, replaced by <code>toDF()</code>.</I>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="createJDBCTable(java.lang.String, java.lang.String, boolean)"><!-- --></A><H3>
createJDBCTable</H3>
<PRE>
public void <B>createJDBCTable</B>(String&nbsp;url,
                            String&nbsp;table,
                            boolean&nbsp;allowExisting)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.340, replaced by <code>write().jdbc()</code>.</I>
<P>
<DD>Save this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> to a JDBC database at <code>url</code> under the table name <code>table</code>.
 This will run a <code>CREATE TABLE</code> and a bunch of <code>INSERT INTO</code> statements.
 If you pass <code>true</code> for <code>allowExisting</code>, it will drop any table with the
 given name; if you pass <code>false</code>, it will throw if the table already
 exists.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>url</CODE> - (undocumented)<DD><CODE>table</CODE> - (undocumented)<DD><CODE>allowExisting</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="insertIntoJDBC(java.lang.String, java.lang.String, boolean)"><!-- --></A><H3>
insertIntoJDBC</H3>
<PRE>
public void <B>insertIntoJDBC</B>(String&nbsp;url,
                           String&nbsp;table,
                           boolean&nbsp;overwrite)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().jdbc()</code>.</I>
<P>
<DD>Save this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> to a JDBC database at <code>url</code> under the table name <code>table</code>.
 Assumes the table already exists and has a compatible schema.  If you
 pass <code>true</code> for <code>overwrite</code>, it will <code>TRUNCATE</code> the table before
 performing the <code>INSERT</code>s.
 <p>
 The table must already exist on the database.  It must have a schema
 that is compatible with the schema of this RDD; inserting the rows of
 the RDD in order via the simple statement
 <code>INSERT INTO table VALUES (?, ?, ..., ?)</code> should not fail.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>url</CODE> - (undocumented)<DD><CODE>table</CODE> - (undocumented)<DD><CODE>overwrite</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="saveAsParquetFile(java.lang.String)"><!-- --></A><H3>
saveAsParquetFile</H3>
<PRE>
public void <B>saveAsParquetFile</B>(String&nbsp;path)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().parquet()</code>.</I>
<P>
<DD>Saves the contents of this <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A> as a parquet file, preserving the schema.
 Files that are written out using this method can be read back in as a <A HREF="../../../../org/apache/spark/sql/DataFrame.html" title="class in org.apache.spark.sql"><CODE>DataFrame</CODE></A>
 using the <code>parquetFile</code> function in <A HREF="../../../../org/apache/spark/sql/SQLContext.html" title="class in org.apache.spark.sql"><CODE>SQLContext</CODE></A>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>path</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="saveAsTable(java.lang.String)"><!-- --></A><H3>
saveAsTable</H3>
<PRE>
public void <B>saveAsTable</B>(String&nbsp;tableName)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().saveAsTable(tableName)</code>.</I>
<P>
<DD>Creates a table from the the contents of this DataFrame.
 It will use the default data source configured by spark.sql.sources.default.
 This will fail if the table already exists.
 <p>
 Note that this currently only works with DataFrames that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.
 <p>
 Also note that while this function can persist the table metadata into Hive's metastore,
 the table will NOT be accessible from Hive, until SPARK-7550 is resolved.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>tableName</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="saveAsTable(java.lang.String, org.apache.spark.sql.SaveMode)"><!-- --></A><H3>
saveAsTable</H3>
<PRE>
public void <B>saveAsTable</B>(String&nbsp;tableName,
                        <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().mode(mode).saveAsTable(tableName)</code>.</I>
<P>
<DD>Creates a table from the the contents of this DataFrame, using the default data source
 configured by spark.sql.sources.default and <CODE>SaveMode.ErrorIfExists</CODE> as the save mode.
 <p>
 Note that this currently only works with DataFrames that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.
 <p>
 Also note that while this function can persist the table metadata into Hive's metastore,
 the table will NOT be accessible from Hive, until SPARK-7550 is resolved.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>tableName</CODE> - (undocumented)<DD><CODE>mode</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="saveAsTable(java.lang.String, java.lang.String)"><!-- --></A><H3>
saveAsTable</H3>
<PRE>
public void <B>saveAsTable</B>(String&nbsp;tableName,
                        String&nbsp;source)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().format(source).saveAsTable(tableName)</code>.</I>
<P>
<DD>Creates a table at the given path from the the contents of this DataFrame
 based on a given data source and a set of options,
 using <CODE>SaveMode.ErrorIfExists</CODE> as the save mode.
 <p>
 Note that this currently only works with DataFrames that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.
 <p>
 Also note that while this function can persist the table metadata into Hive's metastore,
 the table will NOT be accessible from Hive, until SPARK-7550 is resolved.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>tableName</CODE> - (undocumented)<DD><CODE>source</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="saveAsTable(java.lang.String, java.lang.String, org.apache.spark.sql.SaveMode)"><!-- --></A><H3>
saveAsTable</H3>
<PRE>
public void <B>saveAsTable</B>(String&nbsp;tableName,
                        String&nbsp;source,
                        <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().mode(mode).saveAsTable(tableName)</code>.</I>
<P>
<DD>:: Experimental ::
 Creates a table at the given path from the the contents of this DataFrame
 based on a given data source, <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><CODE>SaveMode</CODE></A> specified by mode, and a set of options.
 <p>
 Note that this currently only works with DataFrames that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.
 <p>
 Also note that while this function can persist the table metadata into Hive's metastore,
 the table will NOT be accessible from Hive, until SPARK-7550 is resolved.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>tableName</CODE> - (undocumented)<DD><CODE>source</CODE> - (undocumented)<DD><CODE>mode</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="saveAsTable(java.lang.String, java.lang.String, org.apache.spark.sql.SaveMode, java.util.Map)"><!-- --></A><H3>
saveAsTable</H3>
<PRE>
public void <B>saveAsTable</B>(String&nbsp;tableName,
                        String&nbsp;source,
                        <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode,
                        java.util.Map&lt;String,String&gt;&nbsp;options)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by
            <code>write().format(source).mode(mode).options(options).saveAsTable(tableName)</code>.</I>
<P>
<DD>Creates a table at the given path from the the contents of this DataFrame
 based on a given data source, <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><CODE>SaveMode</CODE></A> specified by mode, and a set of options.
 <p>
 Note that this currently only works with DataFrames that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.
 <p>
 Also note that while this function can persist the table metadata into Hive's metastore,
 the table will NOT be accessible from Hive, until SPARK-7550 is resolved.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>tableName</CODE> - (undocumented)<DD><CODE>source</CODE> - (undocumented)<DD><CODE>mode</CODE> - (undocumented)<DD><CODE>options</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="saveAsTable(java.lang.String, java.lang.String, org.apache.spark.sql.SaveMode, scala.collection.immutable.Map)"><!-- --></A><H3>
saveAsTable</H3>
<PRE>
public void <B>saveAsTable</B>(String&nbsp;tableName,
                        String&nbsp;source,
                        <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode,
                        scala.collection.immutable.Map&lt;String,String&gt;&nbsp;options)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by
            <code>write().format(source).mode(mode).options(options).saveAsTable(tableName)</code>.</I>
<P>
<DD>(Scala-specific)
 Creates a table from the the contents of this DataFrame based on a given data source,
 <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><CODE>SaveMode</CODE></A> specified by mode, and a set of options.
 <p>
 Note that this currently only works with DataFrames that are created from a HiveContext as
 there is no notion of a persisted catalog in a standard SQL context.  Instead you can write
 an RDD out to a parquet file, and then register that file as a table.  This "table" can then
 be the target of an <code>insertInto</code>.
 <p>
 Also note that while this function can persist the table metadata into Hive's metastore,
 the table will NOT be accessible from Hive, until SPARK-7550 is resolved.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>tableName</CODE> - (undocumented)<DD><CODE>source</CODE> - (undocumented)<DD><CODE>mode</CODE> - (undocumented)<DD><CODE>options</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="save(java.lang.String)"><!-- --></A><H3>
save</H3>
<PRE>
public void <B>save</B>(String&nbsp;path)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().save(path)</code>.</I>
<P>
<DD>Saves the contents of this DataFrame to the given path,
 using the default data source configured by spark.sql.sources.default and
 <CODE>SaveMode.ErrorIfExists</CODE> as the save mode.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>path</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="save(java.lang.String, org.apache.spark.sql.SaveMode)"><!-- --></A><H3>
save</H3>
<PRE>
public void <B>save</B>(String&nbsp;path,
                 <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().mode(mode).save(path)</code>.</I>
<P>
<DD>Saves the contents of this DataFrame to the given path and <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><CODE>SaveMode</CODE></A> specified by mode,
 using the default data source configured by spark.sql.sources.default.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>path</CODE> - (undocumented)<DD><CODE>mode</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="save(java.lang.String, java.lang.String)"><!-- --></A><H3>
save</H3>
<PRE>
public void <B>save</B>(String&nbsp;path,
                 String&nbsp;source)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().format(source).save(path)</code>.</I>
<P>
<DD>Saves the contents of this DataFrame to the given path based on the given data source,
 using <CODE>SaveMode.ErrorIfExists</CODE> as the save mode.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>path</CODE> - (undocumented)<DD><CODE>source</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="save(java.lang.String, java.lang.String, org.apache.spark.sql.SaveMode)"><!-- --></A><H3>
save</H3>
<PRE>
public void <B>save</B>(String&nbsp;path,
                 String&nbsp;source,
                 <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by <code>write().format(source).mode(mode).save(path)</code>.</I>
<P>
<DD>Saves the contents of this DataFrame to the given path based on the given data source and
 <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><CODE>SaveMode</CODE></A> specified by mode.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>path</CODE> - (undocumented)<DD><CODE>source</CODE> - (undocumented)<DD><CODE>mode</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="save(java.lang.String, org.apache.spark.sql.SaveMode, java.util.Map)"><!-- --></A><H3>
save</H3>
<PRE>
public void <B>save</B>(String&nbsp;source,
                 <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode,
                 java.util.Map&lt;String,String&gt;&nbsp;options)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by
            <code>write().format(source).mode(mode).options(options).save(path)</code>.</I>
<P>
<DD>Saves the contents of this DataFrame based on the given data source,
 <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><CODE>SaveMode</CODE></A> specified by mode, and a set of options.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>source</CODE> - (undocumented)<DD><CODE>mode</CODE> - (undocumented)<DD><CODE>options</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="save(java.lang.String, org.apache.spark.sql.SaveMode, scala.collection.immutable.Map)"><!-- --></A><H3>
save</H3>
<PRE>
public void <B>save</B>(String&nbsp;source,
                 <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql">SaveMode</A>&nbsp;mode,
                 scala.collection.immutable.Map&lt;String,String&gt;&nbsp;options)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by
            <code>write().format(source).mode(mode).options(options).save(path)</code>.</I>
<P>
<DD>(Scala-specific)
 Saves the contents of this DataFrame based on the given data source,
 <A HREF="../../../../org/apache/spark/sql/SaveMode.html" title="enum in org.apache.spark.sql"><CODE>SaveMode</CODE></A> specified by mode, and a set of options
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>source</CODE> - (undocumented)<DD><CODE>mode</CODE> - (undocumented)<DD><CODE>options</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="insertInto(java.lang.String, boolean)"><!-- --></A><H3>
insertInto</H3>
<PRE>
public void <B>insertInto</B>(String&nbsp;tableName,
                       boolean&nbsp;overwrite)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by
            <code>write().mode(SaveMode.Append|SaveMode.Overwrite).saveAsTable(tableName)</code>.</I>
<P>
<DD>Adds the rows from this RDD to the specified table, optionally overwriting the existing data.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>tableName</CODE> - (undocumented)<DD><CODE>overwrite</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="insertInto(java.lang.String)"><!-- --></A><H3>
insertInto</H3>
<PRE>
public void <B>insertInto</B>(String&nbsp;tableName)</PRE>
<DL>
<DD><B>Deprecated.</B>&nbsp;<I>As of 1.4.0, replaced by
            <code>write().mode(SaveMode.Append).saveAsTable(tableName)</code>.</I>
<P>
<DD>Adds the rows from this RDD to the specified table.
 Throws an exception if the table already exists.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>tableName</CODE> - (undocumented)</DL>
</DD>
</DL>
<!-- ========= END OF CLASS DATA ========= -->
<HR>


<!-- ======= START OF BOTTOM NAVBAR ====== -->
<A NAME="navbar_bottom"><!-- --></A>
<A HREF="#skip-navbar_bottom" title="Skip navigation links"></A>
<TABLE BORDER="0" WIDTH="100%" CELLPADDING="1" CELLSPACING="0" SUMMARY="">
<TR>
<TD COLSPAN=2 BGCOLOR="#EEEEFF" CLASS="NavBarCell1">
<A NAME="navbar_bottom_firstrow"><!-- --></A>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="3" SUMMARY="">
  <TR ALIGN="center" VALIGN="top">
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../overview-summary.html"><FONT CLASS="NavBarFont1"><B>Overview</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-summary.html"><FONT CLASS="NavBarFont1"><B>Package</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#FFFFFF" CLASS="NavBarCell1Rev"> &nbsp;<FONT CLASS="NavBarFont1Rev"><B>Class</B></FONT>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-tree.html"><FONT CLASS="NavBarFont1"><B>Tree</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../deprecated-list.html"><FONT CLASS="NavBarFont1"><B>Deprecated</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../index-all.html"><FONT CLASS="NavBarFont1"><B>Index</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../help-doc.html"><FONT CLASS="NavBarFont1"><B>Help</B></FONT></A>&nbsp;</TD>
  </TR>
</TABLE>
</TD>
<TD ALIGN="right" VALIGN="top" ROWSPAN=3><EM>
</EM>
</TD>
</TR>

<TR>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
&nbsp;<A HREF="../../../../org/apache/spark/sql/ColumnName.html" title="class in org.apache.spark.sql"><B>PREV CLASS</B></A>&nbsp;
&nbsp;<A HREF="../../../../org/apache/spark/sql/DataFrameNaFunctions.html" title="class in org.apache.spark.sql"><B>NEXT CLASS</B></A></FONT></TD>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
  <A HREF="../../../../index.html?org/apache/spark/sql/DataFrame.html" target="_top"><B>FRAMES</B></A>  &nbsp;
&nbsp;<A HREF="DataFrame.html" target="_top"><B>NO FRAMES</B></A>  &nbsp;
&nbsp;<SCRIPT type="text/javascript">
  <!--
  if(window==top) {
    document.writeln('<A HREF="../../../../allclasses-noframe.html"><B>All Classes</B></A>');
  }
  //-->
</SCRIPT>
<NOSCRIPT>
  <A HREF="../../../../allclasses-noframe.html"><B>All Classes</B></A>
</NOSCRIPT>


</FONT></TD>
</TR>
<TR>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_summary">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_summary">METHOD</A></FONT></TD>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
DETAIL:&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_detail">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_detail">METHOD</A></FONT></TD>
</TR>
</TABLE>
<A NAME="skip-navbar_bottom"></A>
<!-- ======== END OF BOTTOM NAVBAR ======= -->

<HR>

</BODY>
</HTML>
