<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!--NewPage-->
<HTML>
<HEAD>
<!-- Generated by javadoc (build 1.6.0_45) on Wed Jul 08 16:13:40 PDT 2015 -->
<TITLE>
RDD (Spark 1.4.1 JavaDoc)
</TITLE>

<META NAME="date" CONTENT="2015-07-08">

<LINK REL ="stylesheet" TYPE="text/css" HREF="../../../../stylesheet.css" TITLE="Style">

<SCRIPT type="text/javascript">
function windowTitle()
{
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="RDD (Spark 1.4.1 JavaDoc)";
    }
}
</SCRIPT>
<NOSCRIPT>
</NOSCRIPT>

</HEAD>

<BODY BGCOLOR="white" onload="windowTitle();">
<HR>


<!-- ========= START OF TOP NAVBAR ======= -->
<A NAME="navbar_top"><!-- --></A>
<A HREF="#skip-navbar_top" title="Skip navigation links"></A>
<TABLE BORDER="0" WIDTH="100%" CELLPADDING="1" CELLSPACING="0" SUMMARY="">
<TR>
<TD COLSPAN=2 BGCOLOR="#EEEEFF" CLASS="NavBarCell1">
<A NAME="navbar_top_firstrow"><!-- --></A>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="3" SUMMARY="">
  <TR ALIGN="center" VALIGN="top">
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../overview-summary.html"><FONT CLASS="NavBarFont1"><B>Overview</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-summary.html"><FONT CLASS="NavBarFont1"><B>Package</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#FFFFFF" CLASS="NavBarCell1Rev"> &nbsp;<FONT CLASS="NavBarFont1Rev"><B>Class</B></FONT>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-tree.html"><FONT CLASS="NavBarFont1"><B>Tree</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../deprecated-list.html"><FONT CLASS="NavBarFont1"><B>Deprecated</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../index-all.html"><FONT CLASS="NavBarFont1"><B>Index</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../help-doc.html"><FONT CLASS="NavBarFont1"><B>Help</B></FONT></A>&nbsp;</TD>
  </TR>
</TABLE>
</TD>
<TD ALIGN="right" VALIGN="top" ROWSPAN=3><EM>
</EM>
</TD>
</TR>

<TR>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
&nbsp;<A HREF="../../../../org/apache/spark/rdd/PartitionPruningRDD.html" title="class in org.apache.spark.rdd"><B>PREV CLASS</B></A>&nbsp;
&nbsp;<A HREF="../../../../org/apache/spark/rdd/SequenceFileRDDFunctions.html" title="class in org.apache.spark.rdd"><B>NEXT CLASS</B></A></FONT></TD>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
  <A HREF="../../../../index.html?org/apache/spark/rdd/RDD.html" target="_top"><B>FRAMES</B></A>  &nbsp;
&nbsp;<A HREF="RDD.html" target="_top"><B>NO FRAMES</B></A>  &nbsp;
&nbsp;<SCRIPT type="text/javascript">
  <!--
  if(window==top) {
    document.writeln('<A HREF="../../../../allclasses-noframe.html"><B>All Classes</B></A>');
  }
  //-->
</SCRIPT>
<NOSCRIPT>
  <A HREF="../../../../allclasses-noframe.html"><B>All Classes</B></A>
</NOSCRIPT>


</FONT></TD>
</TR>
<TR>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_summary">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_summary">METHOD</A></FONT></TD>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
DETAIL:&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_detail">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_detail">METHOD</A></FONT></TD>
</TR>
</TABLE>
<A NAME="skip-navbar_top"></A>
<!-- ========= END OF TOP NAVBAR ========= -->

<HR>
<!-- ======== START OF CLASS DATA ======== -->
<H2>
<FONT SIZE="-1">
org.apache.spark.rdd</FONT>
<BR>
Class RDD&lt;T&gt;</H2>
<PRE>
Object
  <IMG SRC="../../../../resources/inherit.gif" ALT="extended by "><B>org.apache.spark.rdd.RDD&lt;T&gt;</B>
</PRE>
<DL>
<DT><B>All Implemented Interfaces:</B> <DD>java.io.Serializable, <A HREF="../../../../org/apache/spark/Logging.html" title="interface in org.apache.spark">Logging</A></DD>
</DL>
<DL>
<DT><B>Direct Known Subclasses:</B> <DD><A HREF="../../../../org/apache/spark/api/r/BaseRRDD.html" title="class in org.apache.spark.api.r">BaseRRDD</A>, <A HREF="../../../../org/apache/spark/rdd/CoGroupedRDD.html" title="class in org.apache.spark.rdd">CoGroupedRDD</A>, <A HREF="../../../../org/apache/spark/graphx/EdgeRDD.html" title="class in org.apache.spark.graphx">EdgeRDD</A>, <A HREF="../../../../org/apache/spark/rdd/HadoopRDD.html" title="class in org.apache.spark.rdd">HadoopRDD</A>, <A HREF="../../../../org/apache/spark/rdd/JdbcRDD.html" title="class in org.apache.spark.rdd">JdbcRDD</A>, <A HREF="../../../../org/apache/spark/rdd/NewHadoopRDD.html" title="class in org.apache.spark.rdd">NewHadoopRDD</A>, <A HREF="../../../../org/apache/spark/rdd/PartitionPruningRDD.html" title="class in org.apache.spark.rdd">PartitionPruningRDD</A>, <A HREF="../../../../org/apache/spark/rdd/ShuffledRDD.html" title="class in org.apache.spark.rdd">ShuffledRDD</A>, <A HREF="../../../../org/apache/spark/rdd/UnionRDD.html" title="class in org.apache.spark.rdd">UnionRDD</A>, <A HREF="../../../../org/apache/spark/graphx/VertexRDD.html" title="class in org.apache.spark.graphx">VertexRDD</A></DD>
</DL>
<HR>
<DL>
<DT><PRE>public abstract class <B>RDD&lt;T&gt;</B><DT>extends Object<DT>implements scala.Serializable, <A HREF="../../../../org/apache/spark/Logging.html" title="interface in org.apache.spark">Logging</A></DL>
</PRE>

<P>
A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable,
 partitioned collection of elements that can be operated on in parallel. This class contains the
 basic operations available on all RDDs, such as <code>map</code>, <code>filter</code>, and <code>persist</code>. In addition,
 <A HREF="../../../../org/apache/spark/rdd/PairRDDFunctions.html" title="class in org.apache.spark.rdd"><CODE>PairRDDFunctions</CODE></A> contains operations available only on RDDs of key-value
 pairs, such as <code>groupByKey</code> and <code>join</code>;
 <A HREF="../../../../org/apache/spark/rdd/DoubleRDDFunctions.html" title="class in org.apache.spark.rdd"><CODE>DoubleRDDFunctions</CODE></A> contains operations available only on RDDs of
 Doubles; and
 <A HREF="../../../../org/apache/spark/rdd/SequenceFileRDDFunctions.html" title="class in org.apache.spark.rdd"><CODE>SequenceFileRDDFunctions</CODE></A> contains operations available on RDDs that
 can be saved as SequenceFiles.
 All operations are automatically available on any RDD of the right type (e.g. RDD[(Int, Int)]
 through implicit.
 <p>
 Internally, each RDD is characterized by five main properties:
 <p>
  - A list of partitions
  - A function for computing each split
  - A list of dependencies on other RDDs
  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
    an HDFS file)
 <p>
 All of the scheduling and execution in Spark is done based on these methods, allowing each RDD
 to implement its own way of computing itself. Indeed, users can implement custom RDDs (e.g. for
 reading data from a new storage system) by overriding these functions. Please refer to the
 <CODE>Spark paper</CODE> for more details
 on RDD internals.
<P>

<P>
<DL>
<DT><B>See Also:</B><DD><A HREF="../../../../serialized-form.html#org.apache.spark.rdd.RDD">Serialized Form</A></DL>
<HR>

<P>

<!-- ======== CONSTRUCTOR SUMMARY ======== -->

<A NAME="constructor_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Constructor Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#RDD(org.apache.spark.rdd.RDD, scala.reflect.ClassTag)">RDD</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;?&gt;&nbsp;oneParent,
    scala.reflect.ClassTag&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;evidence$2)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Construct an RDD with just a one-to-one dependency on one parent</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#RDD(org.apache.spark.SparkContext, scala.collection.Seq, scala.reflect.ClassTag)">RDD</A></B>(<A HREF="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</A>&nbsp;_sc,
    scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/Dependency.html" title="class in org.apache.spark">Dependency</A>&lt;?&gt;&gt;&nbsp;deps,
    scala.reflect.ClassTag&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;evidence$1)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
</TABLE>
&nbsp;
<!-- ========== METHOD SUMMARY =========== -->

<A NAME="method_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Method Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;U&gt; U</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#aggregate(U, scala.Function2, scala.Function2, scala.reflect.ClassTag)">aggregate</A></B>(U&nbsp;zeroValue,
          scala.Function2&lt;U,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,U&gt;&nbsp;seqOp,
          scala.Function2&lt;U,U,U&gt;&nbsp;combOp,
          scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$32)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Aggregate the elements of each partition, and then the results for all the partitions, using
 given combine functions and a neutral "zero value".</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#cache()">cache</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Persist this RDD with the default storage level (`MEMORY_ONLY`).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,U&gt;&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#cartesian(org.apache.spark.rdd.RDD, scala.reflect.ClassTag)">cartesian</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;&nbsp;other,
          scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$5)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of
 elements (a, b) where a is in <code>this</code> and b is in <code>other</code>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#checkpoint()">checkpoint</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mark this RDD for checkpointing.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;scala.Option&lt;org.apache.spark.rdd.RDDCheckpointData&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#checkpointData()">checkpointData</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#coalesce(int, boolean, scala.math.Ordering)">coalesce</A></B>(int&nbsp;numPartitions,
         boolean&nbsp;shuffle,
         scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return a new RDD that is reduced into <code>numPartitions</code> partitions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;Object</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#collect()">collect</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an array that contains all of the elements in this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#collect(scala.PartialFunction, scala.reflect.ClassTag)">collect</A></B>(scala.PartialFunction&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,U&gt;&nbsp;f,
        scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$31)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an RDD that contains all matching values by applying <code>f</code>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>abstract &nbsp;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#compute(org.apache.spark.Partition, org.apache.spark.TaskContext)">compute</A></B>(<A HREF="../../../../org/apache/spark/Partition.html" title="interface in org.apache.spark">Partition</A>&nbsp;split,
        <A HREF="../../../../org/apache/spark/TaskContext.html" title="class in org.apache.spark">TaskContext</A>&nbsp;context)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:: DeveloperApi ::
 Implemented by subclasses to compute a given partition.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#context()">context</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The <A HREF="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark"><CODE>SparkContext</CODE></A> that this RDD was created on.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;long</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#count()">count</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the number of elements in the RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/partial/PartialResult.html" title="class in org.apache.spark.partial">PartialResult</A>&lt;<A HREF="../../../../org/apache/spark/partial/BoundedDouble.html" title="class in org.apache.spark.partial">BoundedDouble</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#countApprox(long, double)">countApprox</A></B>(long&nbsp;timeout,
            double&nbsp;confidence)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:: Experimental ::
 Approximate version of count() that returns a potentially incomplete result
 within a timeout, even if not all tasks have finished.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;long</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#countApproxDistinct(double)">countApproxDistinct</A></B>(double&nbsp;relativeSD)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return approximate number of distinct elements in the RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;long</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#countApproxDistinct(int, int)">countApproxDistinct</A></B>(int&nbsp;p,
                    int&nbsp;sp)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:: Experimental ::
 Return approximate number of distinct elements in the RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;scala.collection.Map&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,Object&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#countByValue(scala.math.Ordering)">countByValue</A></B>(scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the count of each unique value in this RDD as a local map of (value, count) pairs.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/partial/PartialResult.html" title="class in org.apache.spark.partial">PartialResult</A>&lt;scala.collection.Map&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/partial/BoundedDouble.html" title="class in org.apache.spark.partial">BoundedDouble</A>&gt;&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#countByValueApprox(long, double, scala.math.Ordering)">countByValueApprox</A></B>(long&nbsp;timeout,
                   double&nbsp;confidence,
                   scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:: Experimental ::
 Approximate version of countByValue().</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;org.apache.spark.util.CallSite</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#creationSite()">creationSite</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;User code that created this RDD (e.g.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/Dependency.html" title="class in org.apache.spark">Dependency</A>&lt;?&gt;&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#dependencies()">dependencies</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get the list of dependencies of this RDD, taking into account whether the
 RDD is checkpointed or not.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#distinct()">distinct</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return a new RDD containing the distinct elements in this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#distinct(int, scala.math.Ordering)">distinct</A></B>(int&nbsp;numPartitions,
         scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return a new RDD containing the distinct elements in this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static&nbsp;<A HREF="../../../../org/apache/spark/rdd/DoubleRDDFunctions.html" title="class in org.apache.spark.rdd">DoubleRDDFunctions</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#doubleRDDToDoubleRDDFunctions(org.apache.spark.rdd.RDD)">doubleRDDToDoubleRDDFunctions</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;Object&gt;&nbsp;rdd)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#filter(scala.Function1)">filter</A></B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,Object&gt;&nbsp;f)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return a new RDD containing only the elements that satisfy a predicate.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;A&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#filterWith(scala.Function1, scala.Function2)">filterWith</A></B>(scala.Function1&lt;Object,A&gt;&nbsp;constructA,
           scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,A,Object&gt;&nbsp;p)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Filters this RDD with p, where p takes an additional parameter of type A.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#first()">first</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the first element in this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#flatMap(scala.Function1, scala.reflect.ClassTag)">flatMap</A></B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,scala.collection.TraversableOnce&lt;U&gt;&gt;&nbsp;f,
        scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$4)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return a new RDD by first applying a function to all elements of this
  RDD, and then flattening the results.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;A,U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#flatMapWith(scala.Function1, boolean, scala.Function2, scala.reflect.ClassTag)">flatMapWith</A></B>(scala.Function1&lt;Object,A&gt;&nbsp;constructA,
            boolean&nbsp;preservesPartitioning,
            scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,A,scala.collection.Seq&lt;U&gt;&gt;&nbsp;f,
            scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$11)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FlatMaps f over this RDD, where f takes an additional parameter of type A.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#fold(T, scala.Function2)">fold</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&nbsp;zeroValue,
     scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;op)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Aggregate the elements of each partition, and then the results for all the partitions, using a
 given associative and commutative function and a neutral "zero value".</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#foreach(scala.Function1)">foreach</A></B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,scala.runtime.BoxedUnit&gt;&nbsp;f)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Applies a function f to all elements of this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#foreachPartition(scala.Function1)">foreachPartition</A></B>(scala.Function1&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.runtime.BoxedUnit&gt;&nbsp;f)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Applies a function f to each partition of this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;A&gt; void</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#foreachWith(scala.Function1, scala.Function2)">foreachWith</A></B>(scala.Function1&lt;Object,A&gt;&nbsp;constructA,
            scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,A,scala.runtime.BoxedUnit&gt;&nbsp;f)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Applies f to each element of this RDD, where f takes an additional parameter of type A.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;scala.Option&lt;String&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#getCheckpointFile()">getCheckpointFile</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gets the name of the file to which this RDD was checkpointed</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#getStorageLevel()">getStorageLevel</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get the RDD's current storage level, or StorageLevel.NONE if none is set.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;Object&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#glom()">glom</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an RDD created by coalescing all elements within each partition into an array.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,scala.collection.Iterable&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&gt;&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#groupBy(scala.Function1, scala.reflect.ClassTag)">groupBy</A></B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,K&gt;&nbsp;f,
        scala.reflect.ClassTag&lt;K&gt;&nbsp;kt)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an RDD of grouped items.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,scala.collection.Iterable&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&gt;&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#groupBy(scala.Function1, int, scala.reflect.ClassTag)">groupBy</A></B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,K&gt;&nbsp;f,
        int&nbsp;numPartitions,
        scala.reflect.ClassTag&lt;K&gt;&nbsp;kt)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an RDD of grouped elements.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,scala.collection.Iterable&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&gt;&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#groupBy(scala.Function1, org.apache.spark.Partitioner, scala.reflect.ClassTag, scala.math.Ordering)">groupBy</A></B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,K&gt;&nbsp;f,
        <A HREF="../../../../org/apache/spark/Partitioner.html" title="class in org.apache.spark">Partitioner</A>&nbsp;p,
        scala.reflect.ClassTag&lt;K&gt;&nbsp;kt,
        scala.math.Ordering&lt;K&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an RDD of grouped items.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#id()">id</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A unique ID for this RDD (within its SparkContext).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#intersection(org.apache.spark.rdd.RDD)">intersection</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the intersection of this RDD and another one.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#intersection(org.apache.spark.rdd.RDD, int)">intersection</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other,
             int&nbsp;numPartitions)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the intersection of this RDD and another one.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#intersection(org.apache.spark.rdd.RDD, org.apache.spark.Partitioner, scala.math.Ordering)">intersection</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other,
             <A HREF="../../../../org/apache/spark/Partitioner.html" title="class in org.apache.spark">Partitioner</A>&nbsp;partitioner,
             scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the intersection of this RDD and another one.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;boolean</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#isCheckpointed()">isCheckpointed</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return whether this RDD has been checkpointed or not</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;boolean</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#isEmpty()">isEmpty</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#iterator(org.apache.spark.Partition, org.apache.spark.TaskContext)">iterator</A></B>(<A HREF="../../../../org/apache/spark/Partition.html" title="interface in org.apache.spark">Partition</A>&nbsp;split,
         <A HREF="../../../../org/apache/spark/TaskContext.html" title="class in org.apache.spark">TaskContext</A>&nbsp;context)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Internal method to this RDD; will read from cache if applicable, or otherwise compute it.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#keyBy(scala.Function1)">keyBy</A></B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,K&gt;&nbsp;f)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Creates tuples of the elements in this RDD by applying <code>f</code>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#map(scala.Function1, scala.reflect.ClassTag)">map</A></B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,U&gt;&nbsp;f,
    scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$3)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return a new RDD by applying a function to all elements of this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#mapPartitions(scala.Function1, boolean, scala.reflect.ClassTag)">mapPartitions</A></B>(scala.Function1&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;U&gt;&gt;&nbsp;f,
              boolean&nbsp;preservesPartitioning,
              scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$6)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return a new RDD by applying a function to each partition of this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#mapPartitionsWithContext(scala.Function2, boolean, scala.reflect.ClassTag)">mapPartitionsWithContext</A></B>(scala.Function2&lt;<A HREF="../../../../org/apache/spark/TaskContext.html" title="class in org.apache.spark">TaskContext</A>,scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;U&gt;&gt;&nbsp;f,
                         boolean&nbsp;preservesPartitioning,
                         scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$8)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:: DeveloperApi ::
 Return a new RDD by applying a function to each partition of this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#mapPartitionsWithIndex(scala.Function2, boolean, scala.reflect.ClassTag)">mapPartitionsWithIndex</A></B>(scala.Function2&lt;Object,scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;U&gt;&gt;&nbsp;f,
                       boolean&nbsp;preservesPartitioning,
                       scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$7)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return a new RDD by applying a function to each partition of this RDD, while tracking the index
 of the original partition.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#mapPartitionsWithSplit(scala.Function2, boolean, scala.reflect.ClassTag)">mapPartitionsWithSplit</A></B>(scala.Function2&lt;Object,scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;U&gt;&gt;&nbsp;f,
                       boolean&nbsp;preservesPartitioning,
                       scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$9)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return a new RDD by applying a function to each partition of this RDD, while tracking the index
 of the original partition.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;A,U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#mapWith(scala.Function1, boolean, scala.Function2, scala.reflect.ClassTag)">mapWith</A></B>(scala.Function1&lt;Object,A&gt;&nbsp;constructA,
        boolean&nbsp;preservesPartitioning,
        scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,A,U&gt;&nbsp;f,
        scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$10)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Maps f over this RDD, where f takes an additional parameter of type A.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#max(scala.math.Ordering)">max</A></B>(scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the max of this RDD as defined by the implicit Ordering[T].</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#min(scala.math.Ordering)">min</A></B>(scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the min of this RDD as defined by the implicit Ordering[T].</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;String</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#name()">name</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A friendly name for this RDD</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;T&gt; <A HREF="../../../../org/apache/spark/rdd/DoubleRDDFunctions.html" title="class in org.apache.spark.rdd">DoubleRDDFunctions</A></CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#numericRDDToDoubleRDDFunctions(org.apache.spark.rdd.RDD, scala.math.Numeric)">numericRDDToDoubleRDDFunctions</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;T&gt;&nbsp;rdd,
                               scala.math.Numeric&lt;T&gt;&nbsp;num)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;scala.Option&lt;<A HREF="../../../../org/apache/spark/Partitioner.html" title="class in org.apache.spark">Partitioner</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#partitioner()">partitioner</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Optionally overridden by subclasses to specify how they are partitioned.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/Partition.html" title="interface in org.apache.spark">Partition</A>[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#partitions()">partitions</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get the array of partitions of this RDD, taking into account whether the
 RDD is checkpointed or not.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#persist()">persist</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Persist this RDD with the default storage level (`MEMORY_ONLY`).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#persist(org.apache.spark.storage.StorageLevel)">persist</A></B>(<A HREF="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A>&nbsp;newLevel)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set this RDD's storage level to persist its values across operations after the first time
 it is computed.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;String&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#pipe(scala.collection.Seq, scala.collection.Map, scala.Function1, scala.Function2, boolean)">pipe</A></B>(scala.collection.Seq&lt;String&gt;&nbsp;command,
     scala.collection.Map&lt;String,String&gt;&nbsp;env,
     scala.Function1&lt;scala.Function1&lt;String,scala.runtime.BoxedUnit&gt;,scala.runtime.BoxedUnit&gt;&nbsp;printPipeContext,
     scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,scala.Function1&lt;String,scala.runtime.BoxedUnit&gt;,scala.runtime.BoxedUnit&gt;&nbsp;printRDDElement,
     boolean&nbsp;separateWorkingDir)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an RDD created by piping elements to a forked external process.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;String&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#pipe(java.lang.String)">pipe</A></B>(String&nbsp;command)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an RDD created by piping elements to a forked external process.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;String&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#pipe(java.lang.String, scala.collection.Map)">pipe</A></B>(String&nbsp;command,
     scala.collection.Map&lt;String,String&gt;&nbsp;env)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an RDD created by piping elements to a forked external process.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;scala.collection.Seq&lt;String&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#preferredLocations(org.apache.spark.Partition)">preferredLocations</A></B>(<A HREF="../../../../org/apache/spark/Partition.html" title="interface in org.apache.spark">Partition</A>&nbsp;split)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get the preferred locations of a partition, taking into account whether the
 RDD is checkpointed.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#randomSplit(double[], long)">randomSplit</A></B>(double[]&nbsp;weights,
            long&nbsp;seed)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Randomly splits this RDD with the provided weights.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;T&gt; <A HREF="../../../../org/apache/spark/rdd/AsyncRDDActions.html" title="class in org.apache.spark.rdd">AsyncRDDActions</A>&lt;T&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#rddToAsyncRDDActions(org.apache.spark.rdd.RDD, scala.reflect.ClassTag)">rddToAsyncRDDActions</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;T&gt;&nbsp;rdd,
                     scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$36)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V&gt; <A HREF="../../../../org/apache/spark/rdd/OrderedRDDFunctions.html" title="class in org.apache.spark.rdd">OrderedRDDFunctions</A>&lt;K,V,scala.Tuple2&lt;K,V&gt;&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#rddToOrderedRDDFunctions(org.apache.spark.rdd.RDD, scala.math.Ordering, scala.reflect.ClassTag, scala.reflect.ClassTag)">rddToOrderedRDDFunctions</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,V&gt;&gt;&nbsp;rdd,
                         scala.math.Ordering&lt;K&gt;&nbsp;evidence$37,
                         scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$38,
                         scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$39)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V&gt; <A HREF="../../../../org/apache/spark/rdd/PairRDDFunctions.html" title="class in org.apache.spark.rdd">PairRDDFunctions</A>&lt;K,V&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#rddToPairRDDFunctions(org.apache.spark.rdd.RDD, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.math.Ordering)">rddToPairRDDFunctions</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,V&gt;&gt;&nbsp;rdd,
                      scala.reflect.ClassTag&lt;K&gt;&nbsp;kt,
                      scala.reflect.ClassTag&lt;V&gt;&nbsp;vt,
                      scala.math.Ordering&lt;K&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K,V&gt; <A HREF="../../../../org/apache/spark/rdd/SequenceFileRDDFunctions.html" title="class in org.apache.spark.rdd">SequenceFileRDDFunctions</A>&lt;K,V&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#rddToSequenceFileRDDFunctions(org.apache.spark.rdd.RDD, scala.reflect.ClassTag, scala.reflect.ClassTag, , )">rddToSequenceFileRDDFunctions</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,V&gt;&gt;&nbsp;rdd,
                              scala.reflect.ClassTag&lt;K&gt;&nbsp;kt,
                              scala.reflect.ClassTag&lt;V&gt;&nbsp;vt,
                              <any>&nbsp;keyWritableFactory,
                              <any>&nbsp;valueWritableFactory)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#reduce(scala.Function2)">reduce</A></B>(scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;f)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reduces the elements of this RDD using the specified commutative and
 associative binary operator.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#repartition(int, scala.math.Ordering)">repartition</A></B>(int&nbsp;numPartitions,
            scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return a new RDD that has exactly numPartitions partitions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#sample(boolean, double, long)">sample</A></B>(boolean&nbsp;withReplacement,
       double&nbsp;fraction,
       long&nbsp;seed)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return a sampled subset of this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#saveAsObjectFile(java.lang.String)">saveAsObjectFile</A></B>(String&nbsp;path)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Save this RDD as a SequenceFile of serialized objects.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#saveAsTextFile(java.lang.String)">saveAsTextFile</A></B>(String&nbsp;path)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Save this RDD as a text file, using string representations of elements.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#saveAsTextFile(java.lang.String, java.lang.Class)">saveAsTextFile</A></B>(String&nbsp;path,
               Class&lt;? extends org.apache.hadoop.io.compress.CompressionCodec&gt;&nbsp;codec)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Save this RDD as a compressed text file, using string representations of elements.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;scala.Option&lt;org.apache.spark.rdd.RDDOperationScope&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#scope()">scope</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The scope associated with the operation that created this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#setName(java.lang.String)">setName</A></B>(String&nbsp;_name)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Assign a name to this RDD</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;K&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#sortBy(scala.Function1, boolean, int, scala.math.Ordering, scala.reflect.ClassTag)">sortBy</A></B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,K&gt;&nbsp;f,
       boolean&nbsp;ascending,
       int&nbsp;numPartitions,
       scala.math.Ordering&lt;K&gt;&nbsp;ord,
       scala.reflect.ClassTag&lt;K&gt;&nbsp;ctag)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return this RDD sorted by the given key function.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#sparkContext()">sparkContext</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The SparkContext that created this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#subtract(org.apache.spark.rdd.RDD)">subtract</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an RDD with the elements from <code>this</code> that are not in <code>other</code>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#subtract(org.apache.spark.rdd.RDD, int)">subtract</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other,
         int&nbsp;numPartitions)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an RDD with the elements from <code>this</code> that are not in <code>other</code>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#subtract(org.apache.spark.rdd.RDD, org.apache.spark.Partitioner, scala.math.Ordering)">subtract</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other,
         <A HREF="../../../../org/apache/spark/Partitioner.html" title="class in org.apache.spark">Partitioner</A>&nbsp;p,
         scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an RDD with the elements from <code>this</code> that are not in <code>other</code>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;Object</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#take(int)">take</A></B>(int&nbsp;num)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Take the first num elements of the RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;Object</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#takeOrdered(int, scala.math.Ordering)">takeOrdered</A></B>(int&nbsp;num,
            scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the first k (smallest) elements from this RDD as defined by the specified
 implicit Ordering[T] and maintains the ordering.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;Object</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#takeSample(boolean, int, long)">takeSample</A></B>(boolean&nbsp;withReplacement,
           int&nbsp;num,
           long&nbsp;seed)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return a fixed-size sampled subset of this RDD in an array</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;Object</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#toArray()">toArray</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an array that contains all of the elements in this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;String</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#toDebugString()">toDebugString</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A description of this RDD and its recursive dependencies for debugging.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/api/java/JavaRDD.html" title="class in org.apache.spark.api.java">JavaRDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#toJavaRDD()">toJavaRDD</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#toLocalIterator()">toLocalIterator</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an iterator that contains all of the elements in this RDD.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;Object</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#top(int, scala.math.Ordering)">top</A></B>(int&nbsp;num,
    scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;String</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#toString()">toString</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;U&gt; U</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#treeAggregate(U, scala.Function2, scala.Function2, int, scala.reflect.ClassTag)">treeAggregate</A></B>(U&nbsp;zeroValue,
              scala.Function2&lt;U,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,U&gt;&nbsp;seqOp,
              scala.Function2&lt;U,U,U&gt;&nbsp;combOp,
              int&nbsp;depth,
              scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$33)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Aggregates the elements of this RDD in a multi-level tree pattern.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#treeReduce(scala.Function2, int)">treeReduce</A></B>(scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;f,
           int&nbsp;depth)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reduces the elements of this RDD in a multi-level tree pattern.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#union(org.apache.spark.rdd.RDD)">union</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the union of this RDD and another one.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#unpersist(boolean)">unpersist</A></B>(boolean&nbsp;blocking)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mark the RDD as non-persistent, and remove all blocks for it from memory and disk.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,U&gt;&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#zip(org.apache.spark.rdd.RDD, scala.reflect.ClassTag)">zip</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;&nbsp;other,
    scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$12)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Zips this RDD with another one, returning key-value pairs with the first element in each RDD,
 second element in each RDD, etc.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;B,V&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;V&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#zipPartitions(org.apache.spark.rdd.RDD, boolean, scala.Function2, scala.reflect.ClassTag, scala.reflect.ClassTag)">zipPartitions</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;B&gt;&nbsp;rdd2,
              boolean&nbsp;preservesPartitioning,
              scala.Function2&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;B&gt;,scala.collection.Iterator&lt;V&gt;&gt;&nbsp;f,
              scala.reflect.ClassTag&lt;B&gt;&nbsp;evidence$13,
              scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$14)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by
 applying a function to the zipped partitions.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;B,V&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;V&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#zipPartitions(org.apache.spark.rdd.RDD, scala.Function2, scala.reflect.ClassTag, scala.reflect.ClassTag)">zipPartitions</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;B&gt;&nbsp;rdd2,
              scala.Function2&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;B&gt;,scala.collection.Iterator&lt;V&gt;&gt;&nbsp;f,
              scala.reflect.ClassTag&lt;B&gt;&nbsp;evidence$15,
              scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$16)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;B,C,V&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;V&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#zipPartitions(org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, boolean, scala.Function3, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">zipPartitions</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;B&gt;&nbsp;rdd2,
              <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;C&gt;&nbsp;rdd3,
              boolean&nbsp;preservesPartitioning,
              scala.Function3&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;B&gt;,scala.collection.Iterator&lt;C&gt;,scala.collection.Iterator&lt;V&gt;&gt;&nbsp;f,
              scala.reflect.ClassTag&lt;B&gt;&nbsp;evidence$17,
              scala.reflect.ClassTag&lt;C&gt;&nbsp;evidence$18,
              scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$19)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;B,C,V&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;V&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#zipPartitions(org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, scala.Function3, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">zipPartitions</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;B&gt;&nbsp;rdd2,
              <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;C&gt;&nbsp;rdd3,
              scala.Function3&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;B&gt;,scala.collection.Iterator&lt;C&gt;,scala.collection.Iterator&lt;V&gt;&gt;&nbsp;f,
              scala.reflect.ClassTag&lt;B&gt;&nbsp;evidence$20,
              scala.reflect.ClassTag&lt;C&gt;&nbsp;evidence$21,
              scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$22)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;B,C,D,V&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;V&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#zipPartitions(org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, boolean, scala.Function4, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">zipPartitions</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;B&gt;&nbsp;rdd2,
              <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;C&gt;&nbsp;rdd3,
              <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;D&gt;&nbsp;rdd4,
              boolean&nbsp;preservesPartitioning,
              scala.Function4&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;B&gt;,scala.collection.Iterator&lt;C&gt;,scala.collection.Iterator&lt;D&gt;,scala.collection.Iterator&lt;V&gt;&gt;&nbsp;f,
              scala.reflect.ClassTag&lt;B&gt;&nbsp;evidence$23,
              scala.reflect.ClassTag&lt;C&gt;&nbsp;evidence$24,
              scala.reflect.ClassTag&lt;D&gt;&nbsp;evidence$25,
              scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$26)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="0" SUMMARY="">
<TR ALIGN="right" VALIGN="">
<TD NOWRAP><FONT SIZE="-1">
<CODE>&lt;B,C,D,V&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;V&gt;</CODE></FONT></TD>
</TR>
</TABLE>
</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#zipPartitions(org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, scala.Function4, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)">zipPartitions</A></B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;B&gt;&nbsp;rdd2,
              <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;C&gt;&nbsp;rdd3,
              <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;D&gt;&nbsp;rdd4,
              scala.Function4&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;B&gt;,scala.collection.Iterator&lt;C&gt;,scala.collection.Iterator&lt;D&gt;,scala.collection.Iterator&lt;V&gt;&gt;&nbsp;f,
              scala.reflect.ClassTag&lt;B&gt;&nbsp;evidence$27,
              scala.reflect.ClassTag&lt;C&gt;&nbsp;evidence$28,
              scala.reflect.ClassTag&lt;D&gt;&nbsp;evidence$29,
              scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$30)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,Object&gt;&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#zipWithIndex()">zipWithIndex</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Zips this RDD with its element indices.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,Object&gt;&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../org/apache/spark/rdd/RDD.html#zipWithUniqueId()">zipWithUniqueId</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Zips this RDD with generated unique Long ids.</TD>
</TR>
</TABLE>
&nbsp;<A NAME="methods_inherited_from_class_Object"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#EEEEFF" CLASS="TableSubHeadingColor">
<TH ALIGN="left"><B>Methods inherited from class Object</B></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE>equals, getClass, hashCode, notify, notifyAll, wait, wait, wait</CODE></TD>
</TR>
</TABLE>
&nbsp;<A NAME="methods_inherited_from_class_org.apache.spark.Logging"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#EEEEFF" CLASS="TableSubHeadingColor">
<TH ALIGN="left"><B>Methods inherited from interface org.apache.spark.<A HREF="../../../../org/apache/spark/Logging.html" title="interface in org.apache.spark">Logging</A></B></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE><A HREF="../../../../org/apache/spark/Logging.html#initializeIfNecessary()">initializeIfNecessary</A>, <A HREF="../../../../org/apache/spark/Logging.html#initializeLogging()">initializeLogging</A>, <A HREF="../../../../org/apache/spark/Logging.html#isTraceEnabled()">isTraceEnabled</A>, <A HREF="../../../../org/apache/spark/Logging.html#log_()">log_</A>, <A HREF="../../../../org/apache/spark/Logging.html#log()">log</A>, <A HREF="../../../../org/apache/spark/Logging.html#logDebug(scala.Function0)">logDebug</A>, <A HREF="../../../../org/apache/spark/Logging.html#logDebug(scala.Function0, java.lang.Throwable)">logDebug</A>, <A HREF="../../../../org/apache/spark/Logging.html#logError(scala.Function0)">logError</A>, <A HREF="../../../../org/apache/spark/Logging.html#logError(scala.Function0, java.lang.Throwable)">logError</A>, <A HREF="../../../../org/apache/spark/Logging.html#logInfo(scala.Function0)">logInfo</A>, <A HREF="../../../../org/apache/spark/Logging.html#logInfo(scala.Function0, java.lang.Throwable)">logInfo</A>, <A HREF="../../../../org/apache/spark/Logging.html#logName()">logName</A>, <A HREF="../../../../org/apache/spark/Logging.html#logTrace(scala.Function0)">logTrace</A>, <A HREF="../../../../org/apache/spark/Logging.html#logTrace(scala.Function0, java.lang.Throwable)">logTrace</A>, <A HREF="../../../../org/apache/spark/Logging.html#logWarning(scala.Function0)">logWarning</A>, <A HREF="../../../../org/apache/spark/Logging.html#logWarning(scala.Function0, java.lang.Throwable)">logWarning</A></CODE></TD>
</TR>
</TABLE>
&nbsp;
<P>

<!-- ========= CONSTRUCTOR DETAIL ======== -->

<A NAME="constructor_detail"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="1"><FONT SIZE="+2">
<B>Constructor Detail</B></FONT></TH>
</TR>
</TABLE>

<A NAME="RDD(org.apache.spark.SparkContext, scala.collection.Seq, scala.reflect.ClassTag)"><!-- --></A><H3>
RDD</H3>
<PRE>
public <B>RDD</B>(<A HREF="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</A>&nbsp;_sc,
           scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/Dependency.html" title="class in org.apache.spark">Dependency</A>&lt;?&gt;&gt;&nbsp;deps,
           scala.reflect.ClassTag&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;evidence$1)</PRE>
<DL>
</DL>
<HR>

<A NAME="RDD(org.apache.spark.rdd.RDD, scala.reflect.ClassTag)"><!-- --></A><H3>
RDD</H3>
<PRE>
public <B>RDD</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;?&gt;&nbsp;oneParent,
           scala.reflect.ClassTag&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;evidence$2)</PRE>
<DL>
<DD>Construct an RDD with just a one-to-one dependency on one parent
<P>
</DL>

<!-- ============ METHOD DETAIL ========== -->

<A NAME="method_detail"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="1"><FONT SIZE="+2">
<B>Method Detail</B></FONT></TH>
</TR>
</TABLE>

<A NAME="rddToPairRDDFunctions(org.apache.spark.rdd.RDD, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.math.Ordering)"><!-- --></A><H3>
rddToPairRDDFunctions</H3>
<PRE>
public static &lt;K,V&gt; <A HREF="../../../../org/apache/spark/rdd/PairRDDFunctions.html" title="class in org.apache.spark.rdd">PairRDDFunctions</A>&lt;K,V&gt; <B>rddToPairRDDFunctions</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,V&gt;&gt;&nbsp;rdd,
                                                                scala.reflect.ClassTag&lt;K&gt;&nbsp;kt,
                                                                scala.reflect.ClassTag&lt;V&gt;&nbsp;vt,
                                                                scala.math.Ordering&lt;K&gt;&nbsp;ord)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="rddToAsyncRDDActions(org.apache.spark.rdd.RDD, scala.reflect.ClassTag)"><!-- --></A><H3>
rddToAsyncRDDActions</H3>
<PRE>
public static &lt;T&gt; <A HREF="../../../../org/apache/spark/rdd/AsyncRDDActions.html" title="class in org.apache.spark.rdd">AsyncRDDActions</A>&lt;T&gt; <B>rddToAsyncRDDActions</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;T&gt;&nbsp;rdd,
                                                          scala.reflect.ClassTag&lt;T&gt;&nbsp;evidence$36)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="rddToSequenceFileRDDFunctions(org.apache.spark.rdd.RDD, scala.reflect.ClassTag, scala.reflect.ClassTag, , )"><!-- --></A><H3>
rddToSequenceFileRDDFunctions</H3>
<PRE>
public static &lt;K,V&gt; <A HREF="../../../../org/apache/spark/rdd/SequenceFileRDDFunctions.html" title="class in org.apache.spark.rdd">SequenceFileRDDFunctions</A>&lt;K,V&gt; <B>rddToSequenceFileRDDFunctions</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,V&gt;&gt;&nbsp;rdd,
                                                                                scala.reflect.ClassTag&lt;K&gt;&nbsp;kt,
                                                                                scala.reflect.ClassTag&lt;V&gt;&nbsp;vt,
                                                                                <any>&nbsp;keyWritableFactory,
                                                                                <any>&nbsp;valueWritableFactory)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="rddToOrderedRDDFunctions(org.apache.spark.rdd.RDD, scala.math.Ordering, scala.reflect.ClassTag, scala.reflect.ClassTag)"><!-- --></A><H3>
rddToOrderedRDDFunctions</H3>
<PRE>
public static &lt;K,V&gt; <A HREF="../../../../org/apache/spark/rdd/OrderedRDDFunctions.html" title="class in org.apache.spark.rdd">OrderedRDDFunctions</A>&lt;K,V,scala.Tuple2&lt;K,V&gt;&gt; <B>rddToOrderedRDDFunctions</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,V&gt;&gt;&nbsp;rdd,
                                                                                        scala.math.Ordering&lt;K&gt;&nbsp;evidence$37,
                                                                                        scala.reflect.ClassTag&lt;K&gt;&nbsp;evidence$38,
                                                                                        scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$39)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="doubleRDDToDoubleRDDFunctions(org.apache.spark.rdd.RDD)"><!-- --></A><H3>
doubleRDDToDoubleRDDFunctions</H3>
<PRE>
public static <A HREF="../../../../org/apache/spark/rdd/DoubleRDDFunctions.html" title="class in org.apache.spark.rdd">DoubleRDDFunctions</A> <B>doubleRDDToDoubleRDDFunctions</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;Object&gt;&nbsp;rdd)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="numericRDDToDoubleRDDFunctions(org.apache.spark.rdd.RDD, scala.math.Numeric)"><!-- --></A><H3>
numericRDDToDoubleRDDFunctions</H3>
<PRE>
public static &lt;T&gt; <A HREF="../../../../org/apache/spark/rdd/DoubleRDDFunctions.html" title="class in org.apache.spark.rdd">DoubleRDDFunctions</A> <B>numericRDDToDoubleRDDFunctions</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;T&gt;&nbsp;rdd,
                                                                    scala.math.Numeric&lt;T&gt;&nbsp;num)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="compute(org.apache.spark.Partition, org.apache.spark.TaskContext)"><!-- --></A><H3>
compute</H3>
<PRE>
public abstract scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>compute</B>(<A HREF="../../../../org/apache/spark/Partition.html" title="interface in org.apache.spark">Partition</A>&nbsp;split,
                                                     <A HREF="../../../../org/apache/spark/TaskContext.html" title="class in org.apache.spark">TaskContext</A>&nbsp;context)</PRE>
<DL>
<DD>:: DeveloperApi ::
 Implemented by subclasses to compute a given partition.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>split</CODE> - (undocumented)<DD><CODE>context</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="partitioner()"><!-- --></A><H3>
partitioner</H3>
<PRE>
public scala.Option&lt;<A HREF="../../../../org/apache/spark/Partitioner.html" title="class in org.apache.spark">Partitioner</A>&gt; <B>partitioner</B>()</PRE>
<DL>
<DD>Optionally overridden by subclasses to specify how they are partitioned.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="sparkContext()"><!-- --></A><H3>
sparkContext</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</A> <B>sparkContext</B>()</PRE>
<DL>
<DD>The SparkContext that created this RDD.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="id()"><!-- --></A><H3>
id</H3>
<PRE>
public int <B>id</B>()</PRE>
<DL>
<DD>A unique ID for this RDD (within its SparkContext).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="name()"><!-- --></A><H3>
name</H3>
<PRE>
public String <B>name</B>()</PRE>
<DL>
<DD>A friendly name for this RDD
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="setName(java.lang.String)"><!-- --></A><H3>
setName</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>setName</B>(String&nbsp;_name)</PRE>
<DL>
<DD>Assign a name to this RDD
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="persist(org.apache.spark.storage.StorageLevel)"><!-- --></A><H3>
persist</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>persist</B>(<A HREF="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A>&nbsp;newLevel)</PRE>
<DL>
<DD>Set this RDD's storage level to persist its values across operations after the first time
 it is computed. This can only be used to assign a new storage level if the RDD does not
 have a storage level set yet..
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>newLevel</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="persist()"><!-- --></A><H3>
persist</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>persist</B>()</PRE>
<DL>
<DD>Persist this RDD with the default storage level (`MEMORY_ONLY`).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="cache()"><!-- --></A><H3>
cache</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>cache</B>()</PRE>
<DL>
<DD>Persist this RDD with the default storage level (`MEMORY_ONLY`).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="unpersist(boolean)"><!-- --></A><H3>
unpersist</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>unpersist</B>(boolean&nbsp;blocking)</PRE>
<DL>
<DD>Mark the RDD as non-persistent, and remove all blocks for it from memory and disk.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>blocking</CODE> - Whether to block until all blocks are deleted.
<DT><B>Returns:</B><DD>This RDD.</DL>
</DD>
</DL>
<HR>

<A NAME="getStorageLevel()"><!-- --></A><H3>
getStorageLevel</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/storage/StorageLevel.html" title="class in org.apache.spark.storage">StorageLevel</A> <B>getStorageLevel</B>()</PRE>
<DL>
<DD>Get the RDD's current storage level, or StorageLevel.NONE if none is set.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="dependencies()"><!-- --></A><H3>
dependencies</H3>
<PRE>
public final scala.collection.Seq&lt;<A HREF="../../../../org/apache/spark/Dependency.html" title="class in org.apache.spark">Dependency</A>&lt;?&gt;&gt; <B>dependencies</B>()</PRE>
<DL>
<DD>Get the list of dependencies of this RDD, taking into account whether the
 RDD is checkpointed or not.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="partitions()"><!-- --></A><H3>
partitions</H3>
<PRE>
public final <A HREF="../../../../org/apache/spark/Partition.html" title="interface in org.apache.spark">Partition</A>[] <B>partitions</B>()</PRE>
<DL>
<DD>Get the array of partitions of this RDD, taking into account whether the
 RDD is checkpointed or not.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="preferredLocations(org.apache.spark.Partition)"><!-- --></A><H3>
preferredLocations</H3>
<PRE>
public final scala.collection.Seq&lt;String&gt; <B>preferredLocations</B>(<A HREF="../../../../org/apache/spark/Partition.html" title="interface in org.apache.spark">Partition</A>&nbsp;split)</PRE>
<DL>
<DD>Get the preferred locations of a partition, taking into account whether the
 RDD is checkpointed.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>split</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="iterator(org.apache.spark.Partition, org.apache.spark.TaskContext)"><!-- --></A><H3>
iterator</H3>
<PRE>
public final scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>iterator</B>(<A HREF="../../../../org/apache/spark/Partition.html" title="interface in org.apache.spark">Partition</A>&nbsp;split,
                                                   <A HREF="../../../../org/apache/spark/TaskContext.html" title="class in org.apache.spark">TaskContext</A>&nbsp;context)</PRE>
<DL>
<DD>Internal method to this RDD; will read from cache if applicable, or otherwise compute it.
 This should ''not'' be called by users directly, but is available for implementors of custom
 subclasses of RDD.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>split</CODE> - (undocumented)<DD><CODE>context</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="map(scala.Function1, scala.reflect.ClassTag)"><!-- --></A><H3>
map</H3>
<PRE>
public &lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt; <B>map</B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,U&gt;&nbsp;f,
                      scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$3)</PRE>
<DL>
<DD>Return a new RDD by applying a function to all elements of this RDD.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>evidence$3</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="flatMap(scala.Function1, scala.reflect.ClassTag)"><!-- --></A><H3>
flatMap</H3>
<PRE>
public &lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt; <B>flatMap</B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,scala.collection.TraversableOnce&lt;U&gt;&gt;&nbsp;f,
                          scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$4)</PRE>
<DL>
<DD>Return a new RDD by first applying a function to all elements of this
  RDD, and then flattening the results.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>evidence$4</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="filter(scala.Function1)"><!-- --></A><H3>
filter</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>filter</B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,Object&gt;&nbsp;f)</PRE>
<DL>
<DD>Return a new RDD containing only the elements that satisfy a predicate.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="distinct(int, scala.math.Ordering)"><!-- --></A><H3>
distinct</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>distinct</B>(int&nbsp;numPartitions,
                       scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</PRE>
<DL>
<DD>Return a new RDD containing the distinct elements in this RDD.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>numPartitions</CODE> - (undocumented)<DD><CODE>ord</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="distinct()"><!-- --></A><H3>
distinct</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>distinct</B>()</PRE>
<DL>
<DD>Return a new RDD containing the distinct elements in this RDD.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="repartition(int, scala.math.Ordering)"><!-- --></A><H3>
repartition</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>repartition</B>(int&nbsp;numPartitions,
                          scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</PRE>
<DL>
<DD>Return a new RDD that has exactly numPartitions partitions.
 <p>
 Can increase or decrease the level of parallelism in this RDD. Internally, this uses
 a shuffle to redistribute data.
 <p>
 If you are decreasing the number of partitions in this RDD, consider using <code>coalesce</code>,
 which can avoid performing a shuffle.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>numPartitions</CODE> - (undocumented)<DD><CODE>ord</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="coalesce(int, boolean, scala.math.Ordering)"><!-- --></A><H3>
coalesce</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>coalesce</B>(int&nbsp;numPartitions,
                       boolean&nbsp;shuffle,
                       scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</PRE>
<DL>
<DD>Return a new RDD that is reduced into <code>numPartitions</code> partitions.
 <p>
 This results in a narrow dependency, e.g. if you go from 1000 partitions
 to 100 partitions, there will not be a shuffle, instead each of the 100
 new partitions will claim 10 of the current partitions.
 <p>
 However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
 this may result in your computation taking place on fewer nodes than
 you like (e.g. one node in the case of numPartitions = 1). To avoid this,
 you can pass shuffle = true. This will add a shuffle step, but means the
 current upstream partitions will be executed in parallel (per whatever
 the current partitioning is).
 <p>
 Note: With shuffle = true, you can actually coalesce to a larger number
 of partitions. This is useful if you have a small number of partitions,
 say 100, potentially with a few partitions being abnormally large. Calling
 coalesce(1000, shuffle = true) will result in 1000 partitions with the
 data distributed using a hash partitioner.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>numPartitions</CODE> - (undocumented)<DD><CODE>shuffle</CODE> - (undocumented)<DD><CODE>ord</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="sample(boolean, double, long)"><!-- --></A><H3>
sample</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>sample</B>(boolean&nbsp;withReplacement,
                     double&nbsp;fraction,
                     long&nbsp;seed)</PRE>
<DL>
<DD>Return a sampled subset of this RDD.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>withReplacement</CODE> - can elements be sampled multiple times (replaced when sampled out)<DD><CODE>fraction</CODE> - expected size of the sample as a fraction of this RDD's size
  without replacement: probability that each element is chosen; fraction must be [0, 1]
  with replacement: expected number of times each element is chosen; fraction must be >= 0<DD><CODE>seed</CODE> - seed for the random number generator
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="randomSplit(double[], long)"><!-- --></A><H3>
randomSplit</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;[] <B>randomSplit</B>(double[]&nbsp;weights,
                            long&nbsp;seed)</PRE>
<DL>
<DD>Randomly splits this RDD with the provided weights.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>weights</CODE> - weights for splits, will be normalized if they don't sum to 1<DD><CODE>seed</CODE> - random seed
 <p>
<DT><B>Returns:</B><DD>split RDDs in an array</DL>
</DD>
</DL>
<HR>

<A NAME="takeSample(boolean, int, long)"><!-- --></A><H3>
takeSample</H3>
<PRE>
public Object <B>takeSample</B>(boolean&nbsp;withReplacement,
                         int&nbsp;num,
                         long&nbsp;seed)</PRE>
<DL>
<DD>Return a fixed-size sampled subset of this RDD in an array
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>withReplacement</CODE> - whether sampling is done with replacement<DD><CODE>num</CODE> - size of the returned sample<DD><CODE>seed</CODE> - seed for the random number generator
<DT><B>Returns:</B><DD>sample of specified size in an array</DL>
</DD>
</DL>
<HR>

<A NAME="union(org.apache.spark.rdd.RDD)"><!-- --></A><H3>
union</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>union</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other)</PRE>
<DL>
<DD>Return the union of this RDD and another one. Any identical elements will appear multiple
 times (use <code>.distinct()</code> to eliminate them).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>other</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="sortBy(scala.Function1, boolean, int, scala.math.Ordering, scala.reflect.ClassTag)"><!-- --></A><H3>
sortBy</H3>
<PRE>
public &lt;K&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>sortBy</B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,K&gt;&nbsp;f,
                         boolean&nbsp;ascending,
                         int&nbsp;numPartitions,
                         scala.math.Ordering&lt;K&gt;&nbsp;ord,
                         scala.reflect.ClassTag&lt;K&gt;&nbsp;ctag)</PRE>
<DL>
<DD>Return this RDD sorted by the given key function.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>ascending</CODE> - (undocumented)<DD><CODE>numPartitions</CODE> - (undocumented)<DD><CODE>ord</CODE> - (undocumented)<DD><CODE>ctag</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="intersection(org.apache.spark.rdd.RDD)"><!-- --></A><H3>
intersection</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>intersection</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other)</PRE>
<DL>
<DD>Return the intersection of this RDD and another one. The output will not contain any duplicate
 elements, even if the input RDDs did.
 <p>
 Note that this method performs a shuffle internally.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>other</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="intersection(org.apache.spark.rdd.RDD, org.apache.spark.Partitioner, scala.math.Ordering)"><!-- --></A><H3>
intersection</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>intersection</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other,
                           <A HREF="../../../../org/apache/spark/Partitioner.html" title="class in org.apache.spark">Partitioner</A>&nbsp;partitioner,
                           scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</PRE>
<DL>
<DD>Return the intersection of this RDD and another one. The output will not contain any duplicate
 elements, even if the input RDDs did.
 <p>
 Note that this method performs a shuffle internally.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>partitioner</CODE> - Partitioner to use for the resulting RDD<DD><CODE>other</CODE> - (undocumented)<DD><CODE>ord</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="intersection(org.apache.spark.rdd.RDD, int)"><!-- --></A><H3>
intersection</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>intersection</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other,
                           int&nbsp;numPartitions)</PRE>
<DL>
<DD>Return the intersection of this RDD and another one. The output will not contain any duplicate
 elements, even if the input RDDs did.  Performs a hash partition across the cluster
 <p>
 Note that this method performs a shuffle internally.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>numPartitions</CODE> - How many partitions to use in the resulting RDD<DD><CODE>other</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="glom()"><!-- --></A><H3>
glom</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;Object&gt; <B>glom</B>()</PRE>
<DL>
<DD>Return an RDD created by coalescing all elements within each partition into an array.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="cartesian(org.apache.spark.rdd.RDD, scala.reflect.ClassTag)"><!-- --></A><H3>
cartesian</H3>
<PRE>
public &lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,U&gt;&gt; <B>cartesian</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;&nbsp;other,
                                            scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$5)</PRE>
<DL>
<DD>Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of
 elements (a, b) where a is in <code>this</code> and b is in <code>other</code>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>other</CODE> - (undocumented)<DD><CODE>evidence$5</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="groupBy(scala.Function1, scala.reflect.ClassTag)"><!-- --></A><H3>
groupBy</H3>
<PRE>
public &lt;K&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,scala.collection.Iterable&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&gt;&gt; <B>groupBy</B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,K&gt;&nbsp;f,
                                                                     scala.reflect.ClassTag&lt;K&gt;&nbsp;kt)</PRE>
<DL>
<DD>Return an RDD of grouped items. Each group consists of a key and a sequence of elements
 mapping to that key. The ordering of elements within each group is not guaranteed, and
 may even differ each time the resulting RDD is evaluated.
 <p>
 Note: This operation may be very expensive. If you are grouping in order to perform an
 aggregation (such as a sum or average) over each key, using <CODE>PairRDDFunctions.aggregateByKey</CODE>
 or <CODE>PairRDDFunctions.reduceByKey</CODE> will provide much better performance.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>kt</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="groupBy(scala.Function1, int, scala.reflect.ClassTag)"><!-- --></A><H3>
groupBy</H3>
<PRE>
public &lt;K&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,scala.collection.Iterable&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&gt;&gt; <B>groupBy</B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,K&gt;&nbsp;f,
                                                                     int&nbsp;numPartitions,
                                                                     scala.reflect.ClassTag&lt;K&gt;&nbsp;kt)</PRE>
<DL>
<DD>Return an RDD of grouped elements. Each group consists of a key and a sequence of elements
 mapping to that key. The ordering of elements within each group is not guaranteed, and
 may even differ each time the resulting RDD is evaluated.
 <p>
 Note: This operation may be very expensive. If you are grouping in order to perform an
 aggregation (such as a sum or average) over each key, using <CODE>PairRDDFunctions.aggregateByKey</CODE>
 or <CODE>PairRDDFunctions.reduceByKey</CODE> will provide much better performance.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>numPartitions</CODE> - (undocumented)<DD><CODE>kt</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="groupBy(scala.Function1, org.apache.spark.Partitioner, scala.reflect.ClassTag, scala.math.Ordering)"><!-- --></A><H3>
groupBy</H3>
<PRE>
public &lt;K&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,scala.collection.Iterable&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&gt;&gt; <B>groupBy</B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,K&gt;&nbsp;f,
                                                                     <A HREF="../../../../org/apache/spark/Partitioner.html" title="class in org.apache.spark">Partitioner</A>&nbsp;p,
                                                                     scala.reflect.ClassTag&lt;K&gt;&nbsp;kt,
                                                                     scala.math.Ordering&lt;K&gt;&nbsp;ord)</PRE>
<DL>
<DD>Return an RDD of grouped items. Each group consists of a key and a sequence of elements
 mapping to that key. The ordering of elements within each group is not guaranteed, and
 may even differ each time the resulting RDD is evaluated.
 <p>
 Note: This operation may be very expensive. If you are grouping in order to perform an
 aggregation (such as a sum or average) over each key, using <CODE>PairRDDFunctions.aggregateByKey</CODE>
 or <CODE>PairRDDFunctions.reduceByKey</CODE> will provide much better performance.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>p</CODE> - (undocumented)<DD><CODE>kt</CODE> - (undocumented)<DD><CODE>ord</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="pipe(java.lang.String)"><!-- --></A><H3>
pipe</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;String&gt; <B>pipe</B>(String&nbsp;command)</PRE>
<DL>
<DD>Return an RDD created by piping elements to a forked external process.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>command</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="pipe(java.lang.String, scala.collection.Map)"><!-- --></A><H3>
pipe</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;String&gt; <B>pipe</B>(String&nbsp;command,
                        scala.collection.Map&lt;String,String&gt;&nbsp;env)</PRE>
<DL>
<DD>Return an RDD created by piping elements to a forked external process.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>command</CODE> - (undocumented)<DD><CODE>env</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="pipe(scala.collection.Seq, scala.collection.Map, scala.Function1, scala.Function2, boolean)"><!-- --></A><H3>
pipe</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;String&gt; <B>pipe</B>(scala.collection.Seq&lt;String&gt;&nbsp;command,
                        scala.collection.Map&lt;String,String&gt;&nbsp;env,
                        scala.Function1&lt;scala.Function1&lt;String,scala.runtime.BoxedUnit&gt;,scala.runtime.BoxedUnit&gt;&nbsp;printPipeContext,
                        scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,scala.Function1&lt;String,scala.runtime.BoxedUnit&gt;,scala.runtime.BoxedUnit&gt;&nbsp;printRDDElement,
                        boolean&nbsp;separateWorkingDir)</PRE>
<DL>
<DD>Return an RDD created by piping elements to a forked external process.
 The print behavior can be customized by providing two functions.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>command</CODE> - command to run in forked process.<DD><CODE>env</CODE> - environment variables to set.<DD><CODE>printPipeContext</CODE> - Before piping elements, this function is called as an opportunity
                         to pipe context data. Print line function (like out.println) will be
                         passed as printPipeContext's parameter.<DD><CODE>printRDDElement</CODE> - Use this function to customize how to pipe elements. This function
                        will be called with each RDD element as the 1st parameter, and the
                        print line function (like out.println()) as the 2nd parameter.
                        An example of pipe the RDD data of groupBy() in a streaming way,
                        instead of constructing a huge String to concat all the elements:
                        def printRDDElement(record:(String, Seq[String]), f:String=&amp;gt;Unit) =
                          for (e &amp;lt;- record._2){f(e)}<DD><CODE>separateWorkingDir</CODE> - Use separate working directories for each task.
<DT><B>Returns:</B><DD>the result RDD</DL>
</DD>
</DL>
<HR>

<A NAME="mapPartitions(scala.Function1, boolean, scala.reflect.ClassTag)"><!-- --></A><H3>
mapPartitions</H3>
<PRE>
public &lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt; <B>mapPartitions</B>(scala.Function1&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;U&gt;&gt;&nbsp;f,
                                boolean&nbsp;preservesPartitioning,
                                scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$6)</PRE>
<DL>
<DD>Return a new RDD by applying a function to each partition of this RDD.
 <p>
 <code>preservesPartitioning</code> indicates whether the input function preserves the partitioner, which
 should be <code>false</code> unless this is a pair RDD and the input function doesn't modify the keys.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>preservesPartitioning</CODE> - (undocumented)<DD><CODE>evidence$6</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="mapPartitionsWithIndex(scala.Function2, boolean, scala.reflect.ClassTag)"><!-- --></A><H3>
mapPartitionsWithIndex</H3>
<PRE>
public &lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt; <B>mapPartitionsWithIndex</B>(scala.Function2&lt;Object,scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;U&gt;&gt;&nbsp;f,
                                         boolean&nbsp;preservesPartitioning,
                                         scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$7)</PRE>
<DL>
<DD>Return a new RDD by applying a function to each partition of this RDD, while tracking the index
 of the original partition.
 <p>
 <code>preservesPartitioning</code> indicates whether the input function preserves the partitioner, which
 should be <code>false</code> unless this is a pair RDD and the input function doesn't modify the keys.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>preservesPartitioning</CODE> - (undocumented)<DD><CODE>evidence$7</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="mapPartitionsWithContext(scala.Function2, boolean, scala.reflect.ClassTag)"><!-- --></A><H3>
mapPartitionsWithContext</H3>
<PRE>
public &lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt; <B>mapPartitionsWithContext</B>(scala.Function2&lt;<A HREF="../../../../org/apache/spark/TaskContext.html" title="class in org.apache.spark">TaskContext</A>,scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;U&gt;&gt;&nbsp;f,
                                           boolean&nbsp;preservesPartitioning,
                                           scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$8)</PRE>
<DL>
<DD>:: DeveloperApi ::
 Return a new RDD by applying a function to each partition of this RDD. This is a variant of
 mapPartitions that also passes the TaskContext into the closure.
 <p>
 <code>preservesPartitioning</code> indicates whether the input function preserves the partitioner, which
 should be <code>false</code> unless this is a pair RDD and the input function doesn't modify the keys.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>preservesPartitioning</CODE> - (undocumented)<DD><CODE>evidence$8</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="mapPartitionsWithSplit(scala.Function2, boolean, scala.reflect.ClassTag)"><!-- --></A><H3>
mapPartitionsWithSplit</H3>
<PRE>
public &lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt; <B>mapPartitionsWithSplit</B>(scala.Function2&lt;Object,scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;U&gt;&gt;&nbsp;f,
                                         boolean&nbsp;preservesPartitioning,
                                         scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$9)</PRE>
<DL>
<DD>Return a new RDD by applying a function to each partition of this RDD, while tracking the index
 of the original partition.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>preservesPartitioning</CODE> - (undocumented)<DD><CODE>evidence$9</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="mapWith(scala.Function1, boolean, scala.Function2, scala.reflect.ClassTag)"><!-- --></A><H3>
mapWith</H3>
<PRE>
public &lt;A,U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt; <B>mapWith</B>(scala.Function1&lt;Object,A&gt;&nbsp;constructA,
                            boolean&nbsp;preservesPartitioning,
                            scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,A,U&gt;&nbsp;f,
                            scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$10)</PRE>
<DL>
<DD>Maps f over this RDD, where f takes an additional parameter of type A.  This
 additional parameter is produced by constructA, which is called in each
 partition with the index of that partition.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>constructA</CODE> - (undocumented)<DD><CODE>preservesPartitioning</CODE> - (undocumented)<DD><CODE>f</CODE> - (undocumented)<DD><CODE>evidence$10</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="flatMapWith(scala.Function1, boolean, scala.Function2, scala.reflect.ClassTag)"><!-- --></A><H3>
flatMapWith</H3>
<PRE>
public &lt;A,U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt; <B>flatMapWith</B>(scala.Function1&lt;Object,A&gt;&nbsp;constructA,
                                boolean&nbsp;preservesPartitioning,
                                scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,A,scala.collection.Seq&lt;U&gt;&gt;&nbsp;f,
                                scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$11)</PRE>
<DL>
<DD>FlatMaps f over this RDD, where f takes an additional parameter of type A.  This
 additional parameter is produced by constructA, which is called in each
 partition with the index of that partition.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>constructA</CODE> - (undocumented)<DD><CODE>preservesPartitioning</CODE> - (undocumented)<DD><CODE>f</CODE> - (undocumented)<DD><CODE>evidence$11</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="foreachWith(scala.Function1, scala.Function2)"><!-- --></A><H3>
foreachWith</H3>
<PRE>
public &lt;A&gt; void <B>foreachWith</B>(scala.Function1&lt;Object,A&gt;&nbsp;constructA,
                            scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,A,scala.runtime.BoxedUnit&gt;&nbsp;f)</PRE>
<DL>
<DD>Applies f to each element of this RDD, where f takes an additional parameter of type A.
 This additional parameter is produced by constructA, which is called in each
 partition with the index of that partition.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>constructA</CODE> - (undocumented)<DD><CODE>f</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="filterWith(scala.Function1, scala.Function2)"><!-- --></A><H3>
filterWith</H3>
<PRE>
public &lt;A&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>filterWith</B>(scala.Function1&lt;Object,A&gt;&nbsp;constructA,
                             scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,A,Object&gt;&nbsp;p)</PRE>
<DL>
<DD>Filters this RDD with p, where p takes an additional parameter of type A.  This
 additional parameter is produced by constructA, which is called in each
 partition with the index of that partition.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>constructA</CODE> - (undocumented)<DD><CODE>p</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="zip(org.apache.spark.rdd.RDD, scala.reflect.ClassTag)"><!-- --></A><H3>
zip</H3>
<PRE>
public &lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,U&gt;&gt; <B>zip</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt;&nbsp;other,
                                      scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$12)</PRE>
<DL>
<DD>Zips this RDD with another one, returning key-value pairs with the first element in each RDD,
 second element in each RDD, etc. Assumes that the two RDDs have the *same number of
 partitions* and the *same number of elements in each partition* (e.g. one was made through
 a map on the other).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>other</CODE> - (undocumented)<DD><CODE>evidence$12</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="zipPartitions(org.apache.spark.rdd.RDD, boolean, scala.Function2, scala.reflect.ClassTag, scala.reflect.ClassTag)"><!-- --></A><H3>
zipPartitions</H3>
<PRE>
public &lt;B,V&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;V&gt; <B>zipPartitions</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;B&gt;&nbsp;rdd2,
                                  boolean&nbsp;preservesPartitioning,
                                  scala.Function2&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;B&gt;,scala.collection.Iterator&lt;V&gt;&gt;&nbsp;f,
                                  scala.reflect.ClassTag&lt;B&gt;&nbsp;evidence$13,
                                  scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$14)</PRE>
<DL>
<DD>Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by
 applying a function to the zipped partitions. Assumes that all the RDDs have the
 *same number of partitions*, but does *not* require them to have the same number
 of elements in each partition.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>rdd2</CODE> - (undocumented)<DD><CODE>preservesPartitioning</CODE> - (undocumented)<DD><CODE>f</CODE> - (undocumented)<DD><CODE>evidence$13</CODE> - (undocumented)<DD><CODE>evidence$14</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="zipPartitions(org.apache.spark.rdd.RDD, scala.Function2, scala.reflect.ClassTag, scala.reflect.ClassTag)"><!-- --></A><H3>
zipPartitions</H3>
<PRE>
public &lt;B,V&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;V&gt; <B>zipPartitions</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;B&gt;&nbsp;rdd2,
                                  scala.Function2&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;B&gt;,scala.collection.Iterator&lt;V&gt;&gt;&nbsp;f,
                                  scala.reflect.ClassTag&lt;B&gt;&nbsp;evidence$15,
                                  scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$16)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="zipPartitions(org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, boolean, scala.Function3, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)"><!-- --></A><H3>
zipPartitions</H3>
<PRE>
public &lt;B,C,V&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;V&gt; <B>zipPartitions</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;B&gt;&nbsp;rdd2,
                                    <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;C&gt;&nbsp;rdd3,
                                    boolean&nbsp;preservesPartitioning,
                                    scala.Function3&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;B&gt;,scala.collection.Iterator&lt;C&gt;,scala.collection.Iterator&lt;V&gt;&gt;&nbsp;f,
                                    scala.reflect.ClassTag&lt;B&gt;&nbsp;evidence$17,
                                    scala.reflect.ClassTag&lt;C&gt;&nbsp;evidence$18,
                                    scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$19)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="zipPartitions(org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, scala.Function3, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)"><!-- --></A><H3>
zipPartitions</H3>
<PRE>
public &lt;B,C,V&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;V&gt; <B>zipPartitions</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;B&gt;&nbsp;rdd2,
                                    <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;C&gt;&nbsp;rdd3,
                                    scala.Function3&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;B&gt;,scala.collection.Iterator&lt;C&gt;,scala.collection.Iterator&lt;V&gt;&gt;&nbsp;f,
                                    scala.reflect.ClassTag&lt;B&gt;&nbsp;evidence$20,
                                    scala.reflect.ClassTag&lt;C&gt;&nbsp;evidence$21,
                                    scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$22)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="zipPartitions(org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, boolean, scala.Function4, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)"><!-- --></A><H3>
zipPartitions</H3>
<PRE>
public &lt;B,C,D,V&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;V&gt; <B>zipPartitions</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;B&gt;&nbsp;rdd2,
                                      <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;C&gt;&nbsp;rdd3,
                                      <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;D&gt;&nbsp;rdd4,
                                      boolean&nbsp;preservesPartitioning,
                                      scala.Function4&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;B&gt;,scala.collection.Iterator&lt;C&gt;,scala.collection.Iterator&lt;D&gt;,scala.collection.Iterator&lt;V&gt;&gt;&nbsp;f,
                                      scala.reflect.ClassTag&lt;B&gt;&nbsp;evidence$23,
                                      scala.reflect.ClassTag&lt;C&gt;&nbsp;evidence$24,
                                      scala.reflect.ClassTag&lt;D&gt;&nbsp;evidence$25,
                                      scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$26)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="zipPartitions(org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, org.apache.spark.rdd.RDD, scala.Function4, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag, scala.reflect.ClassTag)"><!-- --></A><H3>
zipPartitions</H3>
<PRE>
public &lt;B,C,D,V&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;V&gt; <B>zipPartitions</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;B&gt;&nbsp;rdd2,
                                      <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;C&gt;&nbsp;rdd3,
                                      <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;D&gt;&nbsp;rdd4,
                                      scala.Function4&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.collection.Iterator&lt;B&gt;,scala.collection.Iterator&lt;C&gt;,scala.collection.Iterator&lt;D&gt;,scala.collection.Iterator&lt;V&gt;&gt;&nbsp;f,
                                      scala.reflect.ClassTag&lt;B&gt;&nbsp;evidence$27,
                                      scala.reflect.ClassTag&lt;C&gt;&nbsp;evidence$28,
                                      scala.reflect.ClassTag&lt;D&gt;&nbsp;evidence$29,
                                      scala.reflect.ClassTag&lt;V&gt;&nbsp;evidence$30)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="foreach(scala.Function1)"><!-- --></A><H3>
foreach</H3>
<PRE>
public void <B>foreach</B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,scala.runtime.BoxedUnit&gt;&nbsp;f)</PRE>
<DL>
<DD>Applies a function f to all elements of this RDD.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="foreachPartition(scala.Function1)"><!-- --></A><H3>
foreachPartition</H3>
<PRE>
public void <B>foreachPartition</B>(scala.Function1&lt;scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;,scala.runtime.BoxedUnit&gt;&nbsp;f)</PRE>
<DL>
<DD>Applies a function f to each partition of this RDD.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="collect()"><!-- --></A><H3>
collect</H3>
<PRE>
public Object <B>collect</B>()</PRE>
<DL>
<DD>Return an array that contains all of the elements in this RDD.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="toLocalIterator()"><!-- --></A><H3>
toLocalIterator</H3>
<PRE>
public scala.collection.Iterator&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>toLocalIterator</B>()</PRE>
<DL>
<DD>Return an iterator that contains all of the elements in this RDD.
 <p>
 The iterator will consume as much memory as the largest partition in this RDD.
 <p>
 Note: this results in multiple Spark jobs, and if the input RDD is the result
 of a wide transformation (e.g. join with different partitioners), to avoid
 recomputing the input RDD should be cached first.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="toArray()"><!-- --></A><H3>
toArray</H3>
<PRE>
public Object <B>toArray</B>()</PRE>
<DL>
<DD>Return an array that contains all of the elements in this RDD.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="collect(scala.PartialFunction, scala.reflect.ClassTag)"><!-- --></A><H3>
collect</H3>
<PRE>
public &lt;U&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;U&gt; <B>collect</B>(scala.PartialFunction&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,U&gt;&nbsp;f,
                          scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$31)</PRE>
<DL>
<DD>Return an RDD that contains all matching values by applying <code>f</code>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)<DD><CODE>evidence$31</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="subtract(org.apache.spark.rdd.RDD)"><!-- --></A><H3>
subtract</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>subtract</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other)</PRE>
<DL>
<DD>Return an RDD with the elements from <code>this</code> that are not in <code>other</code>.
 <p>
 Uses <code>this</code> partitioner/partition size, because even if <code>other</code> is huge, the resulting
 RDD will be &amp;lt;= us.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>other</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="subtract(org.apache.spark.rdd.RDD, int)"><!-- --></A><H3>
subtract</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>subtract</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other,
                       int&nbsp;numPartitions)</PRE>
<DL>
<DD>Return an RDD with the elements from <code>this</code> that are not in <code>other</code>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>other</CODE> - (undocumented)<DD><CODE>numPartitions</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="subtract(org.apache.spark.rdd.RDD, org.apache.spark.Partitioner, scala.math.Ordering)"><!-- --></A><H3>
subtract</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>subtract</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;other,
                       <A HREF="../../../../org/apache/spark/Partitioner.html" title="class in org.apache.spark">Partitioner</A>&nbsp;p,
                       scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</PRE>
<DL>
<DD>Return an RDD with the elements from <code>this</code> that are not in <code>other</code>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>other</CODE> - (undocumented)<DD><CODE>p</CODE> - (undocumented)<DD><CODE>ord</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="reduce(scala.Function2)"><!-- --></A><H3>
reduce</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A> <B>reduce</B>(scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;f)</PRE>
<DL>
<DD>Reduces the elements of this RDD using the specified commutative and
 associative binary operator.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="treeReduce(scala.Function2, int)"><!-- --></A><H3>
treeReduce</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A> <B>treeReduce</B>(scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;f,
                    int&nbsp;depth)</PRE>
<DL>
<DD>Reduces the elements of this RDD in a multi-level tree pattern.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>depth</CODE> - suggested depth of the tree (default: 2)<DD><CODE>f</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>See Also:</B><DD><A HREF="../../../../org/apache/spark/rdd/RDD.html#reduce(scala.Function2)"><CODE>reduce(scala.Function2<T, T, T>)</CODE></A></DL>
</DD>
</DL>
<HR>

<A NAME="fold(java.lang.Object,scala.Function2)"><!-- --></A><A NAME="fold(T, scala.Function2)"><!-- --></A><H3>
fold</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A> <B>fold</B>(<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&nbsp;zeroValue,
              scala.Function2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;op)</PRE>
<DL>
<DD>Aggregate the elements of each partition, and then the results for all the partitions, using a
 given associative and commutative function and a neutral "zero value". The function
 op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object
 allocation; however, it should not modify t2.
 <p>
 This behaves somewhat differently from fold operations implemented for non-distributed
 collections in functional languages like Scala. This fold operation may be applied to
 partitions individually, and then fold those results into the final result, rather than
 apply the fold to each element sequentially in some defined ordering. For functions
 that are not commutative, the result may differ from that of a fold applied to a
 non-distributed collection.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>zeroValue</CODE> - (undocumented)<DD><CODE>op</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="aggregate(java.lang.Object,scala.Function2,scala.Function2,scala.reflect.ClassTag)"><!-- --></A><A NAME="aggregate(U, scala.Function2, scala.Function2, scala.reflect.ClassTag)"><!-- --></A><H3>
aggregate</H3>
<PRE>
public &lt;U&gt; U <B>aggregate</B>(U&nbsp;zeroValue,
                       scala.Function2&lt;U,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,U&gt;&nbsp;seqOp,
                       scala.Function2&lt;U,U,U&gt;&nbsp;combOp,
                       scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$32)</PRE>
<DL>
<DD>Aggregate the elements of each partition, and then the results for all the partitions, using
 given combine functions and a neutral "zero value". This function can return a different result
 type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U
 and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are
 allowed to modify and return their first argument instead of creating a new U to avoid memory
 allocation.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>zeroValue</CODE> - (undocumented)<DD><CODE>seqOp</CODE> - (undocumented)<DD><CODE>combOp</CODE> - (undocumented)<DD><CODE>evidence$32</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="treeAggregate(java.lang.Object,scala.Function2,scala.Function2,int,scala.reflect.ClassTag)"><!-- --></A><A NAME="treeAggregate(U, scala.Function2, scala.Function2, int, scala.reflect.ClassTag)"><!-- --></A><H3>
treeAggregate</H3>
<PRE>
public &lt;U&gt; U <B>treeAggregate</B>(U&nbsp;zeroValue,
                           scala.Function2&lt;U,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,U&gt;&nbsp;seqOp,
                           scala.Function2&lt;U,U,U&gt;&nbsp;combOp,
                           int&nbsp;depth,
                           scala.reflect.ClassTag&lt;U&gt;&nbsp;evidence$33)</PRE>
<DL>
<DD>Aggregates the elements of this RDD in a multi-level tree pattern.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>depth</CODE> - suggested depth of the tree (default: 2)<DD><CODE>zeroValue</CODE> - (undocumented)<DD><CODE>seqOp</CODE> - (undocumented)<DD><CODE>combOp</CODE> - (undocumented)<DD><CODE>evidence$33</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)<DT><B>See Also:</B><DD><A HREF="../../../../org/apache/spark/rdd/RDD.html#aggregate(U, scala.Function2, scala.Function2, scala.reflect.ClassTag)"><CODE>aggregate(U, scala.Function2<U, T, U>, scala.Function2<U, U, U>, scala.reflect.ClassTag<U>)</CODE></A></DL>
</DD>
</DL>
<HR>

<A NAME="count()"><!-- --></A><H3>
count</H3>
<PRE>
public long <B>count</B>()</PRE>
<DL>
<DD>Return the number of elements in the RDD.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="countApprox(long, double)"><!-- --></A><H3>
countApprox</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/partial/PartialResult.html" title="class in org.apache.spark.partial">PartialResult</A>&lt;<A HREF="../../../../org/apache/spark/partial/BoundedDouble.html" title="class in org.apache.spark.partial">BoundedDouble</A>&gt; <B>countApprox</B>(long&nbsp;timeout,
                                                double&nbsp;confidence)</PRE>
<DL>
<DD>:: Experimental ::
 Approximate version of count() that returns a potentially incomplete result
 within a timeout, even if not all tasks have finished.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>timeout</CODE> - (undocumented)<DD><CODE>confidence</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="countByValue(scala.math.Ordering)"><!-- --></A><H3>
countByValue</H3>
<PRE>
public scala.collection.Map&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,Object&gt; <B>countByValue</B>(scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</PRE>
<DL>
<DD>Return the count of each unique value in this RDD as a local map of (value, count) pairs.
 <p>
 Note that this method should only be used if the resulting map is expected to be small, as
 the whole thing is loaded into the driver's memory.
 To handle very large results, consider using rdd.map(x =&amp;gt; (x, 1L)).reduceByKey(_ + _), which
 returns an RDD[T, Long] instead of a map.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>ord</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="countByValueApprox(long, double, scala.math.Ordering)"><!-- --></A><H3>
countByValueApprox</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/partial/PartialResult.html" title="class in org.apache.spark.partial">PartialResult</A>&lt;scala.collection.Map&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,<A HREF="../../../../org/apache/spark/partial/BoundedDouble.html" title="class in org.apache.spark.partial">BoundedDouble</A>&gt;&gt; <B>countByValueApprox</B>(long&nbsp;timeout,
                                                                               double&nbsp;confidence,
                                                                               scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</PRE>
<DL>
<DD>:: Experimental ::
 Approximate version of countByValue().
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>timeout</CODE> - (undocumented)<DD><CODE>confidence</CODE> - (undocumented)<DD><CODE>ord</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="countApproxDistinct(int, int)"><!-- --></A><H3>
countApproxDistinct</H3>
<PRE>
public long <B>countApproxDistinct</B>(int&nbsp;p,
                                int&nbsp;sp)</PRE>
<DL>
<DD>:: Experimental ::
 Return approximate number of distinct elements in the RDD.
 <p>
 The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice:
 Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available
 <a href="http://dx.doi.org/10.1145/2452376.2452456">here</a>.
 <p>
 The relative accuracy is approximately <code>1.054 / sqrt(2^p)</code>. Setting a nonzero <code>sp &amp;gt; p</code>
 would trigger sparse representation of registers, which may reduce the memory consumption
 and increase accuracy when the cardinality is small.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>p</CODE> - The precision value for the normal set.
          <code>p</code> must be a value between 4 and <code>sp</code> if <code>sp</code> is not zero (32 max).<DD><CODE>sp</CODE> - The precision value for the sparse set, between 0 and 32.
           If <code>sp</code> equals 0, the sparse representation is skipped.
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="countApproxDistinct(double)"><!-- --></A><H3>
countApproxDistinct</H3>
<PRE>
public long <B>countApproxDistinct</B>(double&nbsp;relativeSD)</PRE>
<DL>
<DD>Return approximate number of distinct elements in the RDD.
 <p>
 The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice:
 Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available
 <a href="http://dx.doi.org/10.1145/2452376.2452456">here</a>.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>relativeSD</CODE> - Relative accuracy. Smaller values create counters that require more space.
                   It must be greater than 0.000017.
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="zipWithIndex()"><!-- --></A><H3>
zipWithIndex</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,Object&gt;&gt; <B>zipWithIndex</B>()</PRE>
<DL>
<DD>Zips this RDD with its element indices. The ordering is first based on the partition index
 and then the ordering of items within each partition. So the first item in the first
 partition gets index 0, and the last item in the last partition receives the largest index.
 <p>
 This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type.
 This method needs to trigger a spark job when this RDD contains more than one partitions.
 <p>
 Note that some RDDs, such as those returned by groupBy(), do not guarantee order of
 elements in a partition. The index assigned to each element is therefore not guaranteed,
 and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee
 the same index assignments, you should sort the RDD with sortByKey() or save it to a file.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="zipWithUniqueId()"><!-- --></A><H3>
zipWithUniqueId</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,Object&gt;&gt; <B>zipWithUniqueId</B>()</PRE>
<DL>
<DD>Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k,
 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method
 won't trigger a spark job, which is different from <A HREF="../../../../org/apache/spark/rdd/RDD.html#zipWithIndex()"><CODE>zipWithIndex()</CODE></A>.
 <p>
 Note that some RDDs, such as those returned by groupBy(), do not guarantee order of
 elements in a partition. The unique ID assigned to each element is therefore not guaranteed,
 and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee
 the same index assignments, you should sort the RDD with sortByKey() or save it to a file.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="take(int)"><!-- --></A><H3>
take</H3>
<PRE>
public Object <B>take</B>(int&nbsp;num)</PRE>
<DL>
<DD>Take the first num elements of the RDD. It works by first scanning one partition, and use the
 results from that partition to estimate the number of additional partitions needed to satisfy
 the limit.
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>num</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="first()"><!-- --></A><H3>
first</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A> <B>first</B>()</PRE>
<DL>
<DD>Return the first element in this RDD.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="top(int, scala.math.Ordering)"><!-- --></A><H3>
top</H3>
<PRE>
public Object <B>top</B>(int&nbsp;num,
                  scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="takeOrdered(int, scala.math.Ordering)"><!-- --></A><H3>
takeOrdered</H3>
<PRE>
public Object <B>takeOrdered</B>(int&nbsp;num,
                          scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</PRE>
<DL>
<DD>Returns the first k (smallest) elements from this RDD as defined by the specified
 implicit Ordering[T] and maintains the ordering. This does the opposite of <CODE>top</CODE>.
 For example:
 <pre><code>
   sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1)
   // returns Array(2)

   sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2)
   // returns Array(2, 3)
 </code></pre>
 <p>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>num</CODE> - k, the number of elements to return<DD><CODE>ord</CODE> - the implicit ordering for T
<DT><B>Returns:</B><DD>an array of top elements</DL>
</DD>
</DL>
<HR>

<A NAME="max(scala.math.Ordering)"><!-- --></A><H3>
max</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A> <B>max</B>(scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</PRE>
<DL>
<DD>Returns the max of this RDD as defined by the implicit Ordering[T].
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>ord</CODE> - (undocumented)
<DT><B>Returns:</B><DD>the maximum element of the RDD</DL>
</DD>
</DL>
<HR>

<A NAME="min(scala.math.Ordering)"><!-- --></A><H3>
min</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A> <B>min</B>(scala.math.Ordering&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&nbsp;ord)</PRE>
<DL>
<DD>Returns the min of this RDD as defined by the implicit Ordering[T].
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>ord</CODE> - (undocumented)
<DT><B>Returns:</B><DD>the minimum element of the RDD</DL>
</DD>
</DL>
<HR>

<A NAME="isEmpty()"><!-- --></A><H3>
isEmpty</H3>
<PRE>
public boolean <B>isEmpty</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>true if and only if the RDD contains no elements at all. Note that an RDD
         may be empty even when it has at least 1 partition.</DL>
</DD>
</DL>
<HR>

<A NAME="saveAsTextFile(java.lang.String)"><!-- --></A><H3>
saveAsTextFile</H3>
<PRE>
public void <B>saveAsTextFile</B>(String&nbsp;path)</PRE>
<DL>
<DD>Save this RDD as a text file, using string representations of elements.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>path</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="saveAsTextFile(java.lang.String, java.lang.Class)"><!-- --></A><H3>
saveAsTextFile</H3>
<PRE>
public void <B>saveAsTextFile</B>(String&nbsp;path,
                           Class&lt;? extends org.apache.hadoop.io.compress.CompressionCodec&gt;&nbsp;codec)</PRE>
<DL>
<DD>Save this RDD as a compressed text file, using string representations of elements.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>path</CODE> - (undocumented)<DD><CODE>codec</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="saveAsObjectFile(java.lang.String)"><!-- --></A><H3>
saveAsObjectFile</H3>
<PRE>
public void <B>saveAsObjectFile</B>(String&nbsp;path)</PRE>
<DL>
<DD>Save this RDD as a SequenceFile of serialized objects.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>path</CODE> - (undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="keyBy(scala.Function1)"><!-- --></A><H3>
keyBy</H3>
<PRE>
public &lt;K&gt; <A HREF="../../../../org/apache/spark/rdd/RDD.html" title="class in org.apache.spark.rdd">RDD</A>&lt;scala.Tuple2&lt;K,<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&gt; <B>keyBy</B>(scala.Function1&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>,K&gt;&nbsp;f)</PRE>
<DL>
<DD>Creates tuples of the elements in this RDD by applying <code>f</code>.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>f</CODE> - (undocumented)
<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="checkpoint()"><!-- --></A><H3>
checkpoint</H3>
<PRE>
public void <B>checkpoint</B>()</PRE>
<DL>
<DD>Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint
 directory set with SparkContext.setCheckpointDir() and all references to its parent
 RDDs will be removed. This function must be called before any job has been
 executed on this RDD. It is strongly recommended that this RDD is persisted in
 memory, otherwise saving it on a file will require recomputation.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="isCheckpointed()"><!-- --></A><H3>
isCheckpointed</H3>
<PRE>
public boolean <B>isCheckpointed</B>()</PRE>
<DL>
<DD>Return whether this RDD has been checkpointed or not
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="getCheckpointFile()"><!-- --></A><H3>
getCheckpointFile</H3>
<PRE>
public scala.Option&lt;String&gt; <B>getCheckpointFile</B>()</PRE>
<DL>
<DD>Gets the name of the file to which this RDD was checkpointed
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="creationSite()"><!-- --></A><H3>
creationSite</H3>
<PRE>
public org.apache.spark.util.CallSite <B>creationSite</B>()</PRE>
<DL>
<DD>User code that created this RDD (e.g. `textFile`, `parallelize`).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="scope()"><!-- --></A><H3>
scope</H3>
<PRE>
public scala.Option&lt;org.apache.spark.rdd.RDDOperationScope&gt; <B>scope</B>()</PRE>
<DL>
<DD>The scope associated with the operation that created this RDD.
 <p>
 This is more flexible than the call site and can be defined hierarchically. For more
 detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the
 user instantiates this RDD himself without using any Spark operations.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>(undocumented)</DL>
</DD>
</DL>
<HR>

<A NAME="checkpointData()"><!-- --></A><H3>
checkpointData</H3>
<PRE>
public scala.Option&lt;org.apache.spark.rdd.RDDCheckpointData&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt;&gt; <B>checkpointData</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="context()"><!-- --></A><H3>
context</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark">SparkContext</A> <B>context</B>()</PRE>
<DL>
<DD>The <A HREF="../../../../org/apache/spark/SparkContext.html" title="class in org.apache.spark"><CODE>SparkContext</CODE></A> that this RDD was created on.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="toDebugString()"><!-- --></A><H3>
toDebugString</H3>
<PRE>
public String <B>toDebugString</B>()</PRE>
<DL>
<DD>A description of this RDD and its recursive dependencies for debugging.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="toString()"><!-- --></A><H3>
toString</H3>
<PRE>
public String <B>toString</B>()</PRE>
<DL>
<DD><DL>
<DT><B>Overrides:</B><DD><CODE>toString</CODE> in class <CODE>Object</CODE></DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="toJavaRDD()"><!-- --></A><H3>
toJavaRDD</H3>
<PRE>
public <A HREF="../../../../org/apache/spark/api/java/JavaRDD.html" title="class in org.apache.spark.api.java">JavaRDD</A>&lt;<A HREF="../../../../org/apache/spark/rdd/RDD.html" title="type parameter in RDD">T</A>&gt; <B>toJavaRDD</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<!-- ========= END OF CLASS DATA ========= -->
<HR>


<!-- ======= START OF BOTTOM NAVBAR ====== -->
<A NAME="navbar_bottom"><!-- --></A>
<A HREF="#skip-navbar_bottom" title="Skip navigation links"></A>
<TABLE BORDER="0" WIDTH="100%" CELLPADDING="1" CELLSPACING="0" SUMMARY="">
<TR>
<TD COLSPAN=2 BGCOLOR="#EEEEFF" CLASS="NavBarCell1">
<A NAME="navbar_bottom_firstrow"><!-- --></A>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="3" SUMMARY="">
  <TR ALIGN="center" VALIGN="top">
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../overview-summary.html"><FONT CLASS="NavBarFont1"><B>Overview</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-summary.html"><FONT CLASS="NavBarFont1"><B>Package</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#FFFFFF" CLASS="NavBarCell1Rev"> &nbsp;<FONT CLASS="NavBarFont1Rev"><B>Class</B></FONT>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-tree.html"><FONT CLASS="NavBarFont1"><B>Tree</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../deprecated-list.html"><FONT CLASS="NavBarFont1"><B>Deprecated</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../index-all.html"><FONT CLASS="NavBarFont1"><B>Index</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../help-doc.html"><FONT CLASS="NavBarFont1"><B>Help</B></FONT></A>&nbsp;</TD>
  </TR>
</TABLE>
</TD>
<TD ALIGN="right" VALIGN="top" ROWSPAN=3><EM>
</EM>
</TD>
</TR>

<TR>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
&nbsp;<A HREF="../../../../org/apache/spark/rdd/PartitionPruningRDD.html" title="class in org.apache.spark.rdd"><B>PREV CLASS</B></A>&nbsp;
&nbsp;<A HREF="../../../../org/apache/spark/rdd/SequenceFileRDDFunctions.html" title="class in org.apache.spark.rdd"><B>NEXT CLASS</B></A></FONT></TD>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
  <A HREF="../../../../index.html?org/apache/spark/rdd/RDD.html" target="_top"><B>FRAMES</B></A>  &nbsp;
&nbsp;<A HREF="RDD.html" target="_top"><B>NO FRAMES</B></A>  &nbsp;
&nbsp;<SCRIPT type="text/javascript">
  <!--
  if(window==top) {
    document.writeln('<A HREF="../../../../allclasses-noframe.html"><B>All Classes</B></A>');
  }
  //-->
</SCRIPT>
<NOSCRIPT>
  <A HREF="../../../../allclasses-noframe.html"><B>All Classes</B></A>
</NOSCRIPT>


</FONT></TD>
</TR>
<TR>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_summary">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_summary">METHOD</A></FONT></TD>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
DETAIL:&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_detail">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_detail">METHOD</A></FONT></TD>
</TR>
</TABLE>
<A NAME="skip-navbar_bottom"></A>
<!-- ======== END OF BOTTOM NAVBAR ======= -->

<HR>

</BODY>
</HTML>
