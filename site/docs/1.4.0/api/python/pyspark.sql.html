<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>pyspark.sql module &mdash; PySpark 1.4.0-SNAPSHOT documentation</title>
    
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.4.0-SNAPSHOT',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="PySpark 1.4.0-SNAPSHOT documentation" href="index.html" />
    <link rel="next" title="pyspark.streaming module" href="pyspark.streaming.html" />
    <link rel="prev" title="pyspark.mllib package" href="pyspark.mllib.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="pyspark.streaming.html" title="pyspark.streaming module"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="pyspark.mllib.html" title="pyspark.mllib package"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">PySpark 1.4.0-SNAPSHOT documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="pyspark-sql-module">
<h1>pyspark.sql module<a class="headerlink" href="#pyspark-sql-module" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-pyspark.sql">
<span id="module-context"></span><h2>Module Context<a class="headerlink" href="#module-pyspark.sql" title="Permalink to this headline">¶</a></h2>
<p>Important classes of Spark SQL and DataFrames:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#pyspark.sql.SQLContext" title="pyspark.sql.SQLContext"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.SQLContext</span></tt></a>
Main entry point for <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> and SQL functionality.</li>
<li><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataFrame</span></tt></a>
A distributed collection of data grouped into named columns.</li>
<li><a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.Column</span></tt></a>
A column expression in a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</li>
<li><a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.Row</span></tt></a>
A row of data in a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</li>
<li><a class="reference internal" href="#pyspark.sql.HiveContext" title="pyspark.sql.HiveContext"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.HiveContext</span></tt></a>
Main entry point for accessing data stored in Apache Hive.</li>
<li><a class="reference internal" href="#pyspark.sql.GroupedData" title="pyspark.sql.GroupedData"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.GroupedData</span></tt></a>
Aggregation methods, returned by <a class="reference internal" href="#pyspark.sql.DataFrame.groupBy" title="pyspark.sql.DataFrame.groupBy"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.groupBy()</span></tt></a>.</li>
<li><a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions" title="pyspark.sql.DataFrameNaFunctions"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataFrameNaFunctions</span></tt></a>
Methods for handling missing data (null values).</li>
<li><a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions" title="pyspark.sql.DataFrameStatFunctions"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.DataFrameStatFunctions</span></tt></a>
Methods for statistics functionality.</li>
<li><a class="reference internal" href="#module-pyspark.sql.functions" title="pyspark.sql.functions"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.functions</span></tt></a>
List of built-in functions available for <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</li>
<li><a class="reference internal" href="#module-pyspark.sql.types" title="pyspark.sql.types"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.types</span></tt></a>
List of data types available.</li>
<li><a class="reference internal" href="#pyspark.sql.Window" title="pyspark.sql.Window"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.sql.Window</span></tt></a>
For working with window functions.</li>
</ul>
</div></blockquote>
<dl class="class">
<dt id="pyspark.sql.SQLContext">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">SQLContext</tt><big>(</big><em>sparkContext</em>, <em>sqlContext=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Main entry point for Spark SQL functionality.</p>
<p>A SQLContext can be used create <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>, register <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as
tables, execute SQL over tables, cache tables, and read parquet files.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sparkContext</strong> &#8211; The <tt class="xref py py-class docutils literal"><span class="pre">SparkContext</span></tt> backing this SQLContext.</li>
<li><strong>sqlContext</strong> &#8211; An optional JVM Scala SQLContext. If set, we do not instantiate a new
SQLContext in the JVM, instead we make all calls to this object.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="pyspark.sql.SQLContext.applySchema">
<tt class="descname">applySchema</tt><big>(</big><em>rdd</em>, <em>schema</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.applySchema" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 1.3, use <a class="reference internal" href="#pyspark.sql.SQLContext.createDataFrame" title="pyspark.sql.SQLContext.createDataFrame"><tt class="xref py py-func docutils literal"><span class="pre">createDataFrame()</span></tt></a> instead.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.cacheTable">
<tt class="descname">cacheTable</tt><big>(</big><em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.cacheTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Caches the specified table in-memory.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.clearCache">
<tt class="descname">clearCache</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.clearCache" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes all cached tables from the in-memory cache.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.createDataFrame">
<tt class="descname">createDataFrame</tt><big>(</big><em>data</em>, <em>schema=None</em>, <em>samplingRatio=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.createDataFrame" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> from an <tt class="xref py py-class docutils literal"><span class="pre">RDD</span></tt> of <tt class="xref py py-class docutils literal"><span class="pre">tuple</span></tt>/<tt class="xref py py-class docutils literal"><span class="pre">list</span></tt>,
list or <tt class="xref py py-class docutils literal"><span class="pre">pandas.DataFrame</span></tt>.</p>
<p>When <tt class="docutils literal"><span class="pre">schema</span></tt> is a list of column names, the type of each column
will be inferred from <tt class="docutils literal"><span class="pre">data</span></tt>.</p>
<p>When <tt class="docutils literal"><span class="pre">schema</span></tt> is <tt class="docutils literal"><span class="pre">None</span></tt>, it will try to infer the schema (column names and types)
from <tt class="docutils literal"><span class="pre">data</span></tt>, which should be an RDD of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>,
or <tt class="xref py py-class docutils literal"><span class="pre">namedtuple</span></tt>, or <tt class="xref py py-class docutils literal"><span class="pre">dict</span></tt>.</p>
<p>If schema inference is needed, <tt class="docutils literal"><span class="pre">samplingRatio</span></tt> is used to determined the ratio of
rows used for schema inference. The first row will be used if <tt class="docutils literal"><span class="pre">samplingRatio</span></tt> is <tt class="docutils literal"><span class="pre">None</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>data</strong> &#8211; an RDD of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>/<tt class="xref py py-class docutils literal"><span class="pre">tuple</span></tt>/<tt class="xref py py-class docutils literal"><span class="pre">list</span></tt>/<tt class="xref py py-class docutils literal"><span class="pre">dict</span></tt>,
<tt class="xref py py-class docutils literal"><span class="pre">list</span></tt>, or <tt class="xref py py-class docutils literal"><span class="pre">pandas.DataFrame</span></tt>.</li>
<li><strong>schema</strong> &#8211; a <tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt> or list of column names. default None.</li>
<li><strong>samplingRatio</strong> &#8211; the sample ratio of rows used for inferring</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></p>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="p">[(</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(_1=u&#39;Alice&#39;, _2=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="p">[{</span><span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=1, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(_1=u&#39;Alice&#39;, _2=1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="p">[</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">person</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">Person</span><span class="p">(</span><span class="o">*</span><span class="n">r</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">person</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
<span class="gp">... </span>   <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
<span class="gp">... </span>   <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>  
<span class="go">[Row(name=u&#39;Alice&#39;, age=1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.createExternalTable">
<tt class="descname">createExternalTable</tt><big>(</big><em>tableName</em>, <em>path=None</em>, <em>source=None</em>, <em>schema=None</em>, <em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.createExternalTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates an external table based on the dataset in a data source.</p>
<p>It returns the DataFrame associated with the external table.</p>
<p>The data source is specified by the <tt class="docutils literal"><span class="pre">source</span></tt> and a set of <tt class="docutils literal"><span class="pre">options</span></tt>.
If <tt class="docutils literal"><span class="pre">source</span></tt> is not specified, the default data source configured by
<tt class="docutils literal"><span class="pre">spark.sql.sources.default</span></tt> will be used.</p>
<p>Optionally, a schema can be provided as the schema of the returned <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> and
created external table.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.getConf">
<tt class="descname">getConf</tt><big>(</big><em>key</em>, <em>defaultValue</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.getConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of Spark SQL configuration property for the given key.</p>
<p>If the key is not set, returns defaultValue.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.inferSchema">
<tt class="descname">inferSchema</tt><big>(</big><em>rdd</em>, <em>samplingRatio=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.inferSchema" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 1.3, use <a class="reference internal" href="#pyspark.sql.SQLContext.createDataFrame" title="pyspark.sql.SQLContext.createDataFrame"><tt class="xref py py-func docutils literal"><span class="pre">createDataFrame()</span></tt></a> instead.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.jsonFile">
<tt class="descname">jsonFile</tt><big>(</big><em>path</em>, <em>schema=None</em>, <em>samplingRatio=1.0</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.jsonFile" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a text file storing one JSON object per line as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 1.4, use <a class="reference internal" href="#pyspark.sql.DataFrameReader.json" title="pyspark.sql.DataFrameReader.json"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameReader.json()</span></tt></a> instead.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">jsonFile</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/people.json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.jsonRDD">
<tt class="descname">jsonRDD</tt><big>(</big><em>rdd</em>, <em>schema=None</em>, <em>samplingRatio=1.0</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.jsonRDD" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads an RDD storing one JSON object per string as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<p>If the schema is provided, applies the given schema to this JSON dataset.
Otherwise, it samples the dataset with ratio <tt class="docutils literal"><span class="pre">samplingRatio</span></tt> to determine the schema.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">jsonRDD</span><span class="p">(</span><span class="n">json</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df1</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="go">Row(field1=1, field2=u&#39;row1&#39;, field3=Row(field4=11, field5=None), field6=None)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">jsonRDD</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="go">Row(field1=1, field2=u&#39;row1&#39;, field3=Row(field4=11, field5=None), field6=None)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;field2&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">()),</span>
<span class="gp">... </span>    <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;field3&quot;</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s">&quot;field5&quot;</span><span class="p">,</span> <span class="n">ArrayType</span><span class="p">(</span><span class="n">IntegerType</span><span class="p">()))]))</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">jsonRDD</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="go">Row(field2=u&#39;row1&#39;, field3=Row(field5=None))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.load">
<tt class="descname">load</tt><big>(</big><em>path=None</em>, <em>source=None</em>, <em>schema=None</em>, <em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the dataset in a data source as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 1.4, use <a class="reference internal" href="#pyspark.sql.DataFrameReader.load" title="pyspark.sql.DataFrameReader.load"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameReader.load()</span></tt></a> instead.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.parquetFile">
<tt class="descname">parquetFile</tt><big>(</big><em>*paths</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.parquetFile" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a Parquet file, returning the result as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 1.4, use <a class="reference internal" href="#pyspark.sql.DataFrameReader.parquet" title="pyspark.sql.DataFrameReader.parquet"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameReader.parquet()</span></tt></a> instead.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">parquetFile</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/parquet_partitioned&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;name&#39;, &#39;string&#39;), (&#39;year&#39;, &#39;int&#39;), (&#39;month&#39;, &#39;int&#39;), (&#39;day&#39;, &#39;int&#39;)]</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.range">
<tt class="descname">range</tt><big>(</big><em>start</em>, <em>end=None</em>, <em>step=1</em>, <em>numPartitions=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.range" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> with single LongType column named <cite>id</cite>,
containing elements in a range from <cite>start</cite> to <cite>end</cite> (exclusive) with
step value <cite>step</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>start</strong> &#8211; the start value</li>
<li><strong>end</strong> &#8211; the end value (exclusive)</li>
<li><strong>step</strong> &#8211; the incremental step (default: 1)</li>
<li><strong>numPartitions</strong> &#8211; the number of partitions of the DataFrame</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></p>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=1), Row(id=3), Row(id=5)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=0), Row(id=1), Row(id=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SQLContext.read">
<tt class="descname">read</tt><a class="headerlink" href="#pyspark.sql.SQLContext.read" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrameReader" title="pyspark.sql.DataFrameReader"><tt class="xref py py-class docutils literal"><span class="pre">DataFrameReader</span></tt></a> that can be used to read data
in as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrameReader" title="pyspark.sql.DataFrameReader"><tt class="xref py py-class docutils literal"><span class="pre">DataFrameReader</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.registerDataFrameAsTable">
<tt class="descname">registerDataFrameAsTable</tt><big>(</big><em>df</em>, <em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.registerDataFrameAsTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers the given <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as a temporary table in the catalog.</p>
<p>Temporary tables exist only during the lifetime of this instance of <a class="reference internal" href="#pyspark.sql.SQLContext" title="pyspark.sql.SQLContext"><tt class="xref py py-class docutils literal"><span class="pre">SQLContext</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.registerFunction">
<tt class="descname">registerFunction</tt><big>(</big><em>name</em>, <em>f</em>, <em>returnType=StringType</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.registerFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a lambda function as a UDF so it can be used in SQL statements.</p>
<p>In addition to a name and the function itself, the return type can be optionally specified.
When the return type is not given it default to a string and conversion will automatically
be done.  For any other return type, the produced object must match the specified type.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> &#8211; name of the UDF</li>
<li><strong>samplingRatio</strong> &#8211; lambda function</li>
<li><strong>returnType</strong> &#8211; a <tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt> object</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerFunction</span><span class="p">(</span><span class="s">&quot;stringLengthString&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT stringLengthString(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c0=u&#39;4&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerFunction</span><span class="p">(</span><span class="s">&quot;stringLengthInt&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT stringLengthInt(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c0=4)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">udf</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s">&quot;stringLengthInt&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT stringLengthInt(&#39;test&#39;)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c0=4)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.2.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.setConf">
<tt class="descname">setConf</tt><big>(</big><em>key</em>, <em>value</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.setConf" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the given Spark SQL configuration property.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.sql">
<tt class="descname">sql</tt><big>(</big><em>sqlQuery</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.sql" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> representing the result of the given query.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT field1 AS f1, field2 as f2 from table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(f1=1, f2=u&#39;row1&#39;), Row(f1=2, f2=u&#39;row2&#39;), Row(f1=3, f2=u&#39;row3&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.table">
<tt class="descname">table</tt><big>(</big><em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.table" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the specified table as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.0.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.tableNames">
<tt class="descname">tableNames</tt><big>(</big><em>dbName=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.tableNames" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of names of tables in the database <tt class="docutils literal"><span class="pre">dbName</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dbName</strong> &#8211; string, name of the database to use. Default to the current database.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of table names, in string</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s">&quot;table1&quot;</span> <span class="ow">in</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">tableNames</span><span class="p">()</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s">&quot;table1&quot;</span> <span class="ow">in</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">tableNames</span><span class="p">(</span><span class="s">&quot;db&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.tables">
<tt class="descname">tables</tt><big>(</big><em>dbName=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.tables" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> containing names of tables in the given database.</p>
<p>If <tt class="docutils literal"><span class="pre">dbName</span></tt> is not specified, the current database will be used.</p>
<p>The returned DataFrame has two columns: <tt class="docutils literal"><span class="pre">tableName</span></tt> and <tt class="docutils literal"><span class="pre">isTemporary</span></tt>
(a column with <tt class="xref py py-class docutils literal"><span class="pre">BooleanType</span></tt> indicating if a table is a temporary one or not).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>dbName</strong> &#8211; string, name of the database to use.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">registerDataFrameAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">&quot;table1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">tables</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s">&quot;tableName = &#39;table1&#39;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="go">Row(tableName=u&#39;table1&#39;, isTemporary=True)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.SQLContext.udf">
<tt class="descname">udf</tt><a class="headerlink" href="#pyspark.sql.SQLContext.udf" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <tt class="xref py py-class docutils literal"><span class="pre">UDFRegistration</span></tt> for UDF registration.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><tt class="xref py py-class docutils literal"><span class="pre">UDFRegistration</span></tt></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.SQLContext.uncacheTable">
<tt class="descname">uncacheTable</tt><big>(</big><em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.SQLContext.uncacheTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes the specified table from the in-memory cache.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.0.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.HiveContext">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">HiveContext</tt><big>(</big><em>sparkContext</em>, <em>hiveContext=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.HiveContext" title="Permalink to this definition">¶</a></dt>
<dd><p>A variant of Spark SQL that integrates with data stored in Hive.</p>
<p>Configuration for Hive is read from <tt class="docutils literal"><span class="pre">hive-site.xml</span></tt> on the classpath.
It supports running both SQL and HiveQL commands.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sparkContext</strong> &#8211; The SparkContext to wrap.</li>
<li><strong>hiveContext</strong> &#8211; An optional JVM Scala HiveContext. If set, we do not instantiate a new
<a class="reference internal" href="#pyspark.sql.HiveContext" title="pyspark.sql.HiveContext"><tt class="xref py py-class docutils literal"><span class="pre">HiveContext</span></tt></a> in the JVM, instead we make all calls to this object.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="pyspark.sql.HiveContext.refreshTable">
<tt class="descname">refreshTable</tt><big>(</big><em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.HiveContext.refreshTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Invalidate and refresh all the cached the metadata of the given
table. For performance reasons, Spark SQL or the external data source
library it uses might cache certain metadata about a table, such as the
location of blocks. When those change outside of Spark SQL, users should
call this function to invalidate the cache.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrame">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DataFrame</tt><big>(</big><em>jdf</em>, <em>sql_ctx</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame" title="Permalink to this definition">¶</a></dt>
<dd><p>A distributed collection of data grouped into named columns.</p>
<p>A <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> is equivalent to a relational table in Spark SQL,
and can be created using various functions in <a class="reference internal" href="#pyspark.sql.SQLContext" title="pyspark.sql.SQLContext"><tt class="xref py py-class docutils literal"><span class="pre">SQLContext</span></tt></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">people</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">&quot;...&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Once created, it can be manipulated using the various domain-specific-language
(DSL) functions defined in: <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>, <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a>.</p>
<p>To select a column from the data frame, use the apply method:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ageCol</span> <span class="o">=</span> <span class="n">people</span><span class="o">.</span><span class="n">age</span>
</pre></div>
</div>
<p>A more concrete example:</p>
<div class="highlight-python"><div class="highlight"><pre># To create DataFrame using SQLContext
people = sqlContext.read.parquet(&quot;...&quot;)
department = sqlContext.read.parquet(&quot;...&quot;)

people.filter(people.age &gt; 30).join(department, people.deptId == department.id))           .groupBy(department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;})
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrame.agg">
<tt class="descname">agg</tt><big>(</big><em>*exprs</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.agg" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate on the entire <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> without groups
(shorthand for <tt class="docutils literal"><span class="pre">df.groupBy.agg()</span></tt>).</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s">&quot;age&quot;</span><span class="p">:</span> <span class="s">&quot;max&quot;</span><span class="p">})</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(MAX(age)=5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(MIN(age)=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.alias">
<tt class="descname">alias</tt><big>(</big><em>alias</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.alias" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> with an alias set.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_as1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;df_as1&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df_as2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;df_as2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">joined_df</span> <span class="o">=</span> <span class="n">df_as1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df_as2</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">&quot;df_as1.name&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">col</span><span class="p">(</span><span class="s">&quot;df_as2.name&quot;</span><span class="p">),</span> <span class="s">&#39;inner&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">joined_df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">&quot;df_as1.name&quot;</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s">&quot;df_as2.name&quot;</span><span class="p">),</span> <span class="n">col</span><span class="p">(</span><span class="s">&quot;df_as2.age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, name=u&#39;Alice&#39;, age=2), Row(name=u&#39;Bob&#39;, name=u&#39;Bob&#39;, age=5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.cache">
<tt class="descname">cache</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.cache" title="Permalink to this definition">¶</a></dt>
<dd><p>Persists with the default storage level (<tt class="xref py py-class docutils literal"><span class="pre">MEMORY_ONLY_SER</span></tt>).</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.coalesce">
<tt class="descname">coalesce</tt><big>(</big><em>numPartitions</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.coalesce" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> that has exactly <cite>numPartitions</cite> partitions.</p>
<p>Similar to coalesce defined on an <tt class="xref py py-class docutils literal"><span class="pre">RDD</span></tt>, this operation results in a
narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,
there will not be a shuffle, instead each of the 100 new partitions will
claim 10 of the current partitions.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">coalesce</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.collect">
<tt class="descname">collect</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.collect" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all the records as a list of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;), Row(age=5, name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.columns">
<tt class="descname">columns</tt><a class="headerlink" href="#pyspark.sql.DataFrame.columns" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all column names as a list.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">columns</span>
<span class="go">[u&#39;age&#39;, u&#39;name&#39;]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.corr">
<tt class="descname">corr</tt><big>(</big><em>col1</em>, <em>col2</em>, <em>method=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.corr" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the correlation of two columns of a DataFrame as a double value.
Currently only supports the Pearson Correlation Coefficient.
<a class="reference internal" href="#pyspark.sql.DataFrame.corr" title="pyspark.sql.DataFrame.corr"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.corr()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.corr" title="pyspark.sql.DataFrameStatFunctions.corr"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.corr()</span></tt></a> are aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col1</strong> &#8211; The name of the first column</li>
<li><strong>col2</strong> &#8211; The name of the second column</li>
<li><strong>method</strong> &#8211; The correlation method. Currently only supports &#8220;pearson&#8221;</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.count">
<tt class="descname">count</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of rows in this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">2</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.cov">
<tt class="descname">cov</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.cov" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the sample covariance for the given columns, specified by their names, as a
double value. <a class="reference internal" href="#pyspark.sql.DataFrame.cov" title="pyspark.sql.DataFrame.cov"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.cov()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.cov" title="pyspark.sql.DataFrameStatFunctions.cov"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.cov()</span></tt></a> are aliases.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col1</strong> &#8211; The name of the first column</li>
<li><strong>col2</strong> &#8211; The name of the second column</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.crosstab">
<tt class="descname">crosstab</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.crosstab" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a pair-wise frequency table of the given columns. Also known as a contingency
table. The number of distinct values for each column should be less than 1e4. At most 1e6
non-zero pair frequencies will be returned.
The first column of each row will be the distinct values of <cite>col1</cite> and the column names
will be the distinct values of <cite>col2</cite>. The name of the first column will be <cite>$col1_$col2</cite>.
Pairs that have no occurrences will have <cite>null</cite> as their counts.
<a class="reference internal" href="#pyspark.sql.DataFrame.crosstab" title="pyspark.sql.DataFrame.crosstab"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.crosstab()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.crosstab" title="pyspark.sql.DataFrameStatFunctions.crosstab"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.crosstab()</span></tt></a> are aliases.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col1</strong> &#8211; The name of the first column. Distinct items will make the first item of
each row.</li>
<li><strong>col2</strong> &#8211; The name of the second column. Distinct items will make the column names
of the DataFrame.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.cube">
<tt class="descname">cube</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.cube" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a multi-dimensional cube for the current <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> using
the specified columns, so we can run aggregation on them.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">cube</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+----+-----+</span>
<span class="go">| name| age|count|</span>
<span class="go">+-----+----+-----+</span>
<span class="go">| null|   2|    1|</span>
<span class="go">|Alice|null|    1|</span>
<span class="go">|  Bob|   5|    1|</span>
<span class="go">|  Bob|null|    1|</span>
<span class="go">| null|   5|    1|</span>
<span class="go">| null|null|    2|</span>
<span class="go">|Alice|   2|    1|</span>
<span class="go">+-----+----+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.describe">
<tt class="descname">describe</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.describe" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes statistics for numeric columns.</p>
<p>This include count, mean, stddev, min, and max. If no columns are
given, this function computes statistics for all numerical columns.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function is meant for exploratory data analysis, as we make no         guarantee about the backward compatibility of the schema of the resulting DataFrame.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-------+---+</span>
<span class="go">|summary|age|</span>
<span class="go">+-------+---+</span>
<span class="go">|  count|  2|</span>
<span class="go">|   mean|3.5|</span>
<span class="go">| stddev|1.5|</span>
<span class="go">|    min|  2|</span>
<span class="go">|    max|  5|</span>
<span class="go">+-------+---+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">([</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-------+---+-----+</span>
<span class="go">|summary|age| name|</span>
<span class="go">+-------+---+-----+</span>
<span class="go">|  count|  2|    2|</span>
<span class="go">|   mean|3.5| null|</span>
<span class="go">| stddev|1.5| null|</span>
<span class="go">|    min|  2|Alice|</span>
<span class="go">|    max|  5|  Bob|</span>
<span class="go">+-------+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.distinct">
<tt class="descname">distinct</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.distinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> containing the distinct rows in this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">distinct</span><span class="p">()</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">2</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.drop">
<tt class="descname">drop</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.drop" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> that drops the specified column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>col</strong> &#8211; a string name of the column to drop, or a
<a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> to drop.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;), Row(name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;), Row(name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s">&#39;inner&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, height=85, name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s">&#39;inner&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;, height=85)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.dropDuplicates">
<tt class="descname">dropDuplicates</tt><big>(</big><em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.dropDuplicates" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> with duplicate rows removed,
optionally only considering certain columns.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span>             <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">),</span>             <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">),</span>             <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">)])</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">|  5|    80|Alice|</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">([</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">|  5|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.drop_duplicates">
<tt class="descname">drop_duplicates</tt><big>(</big><em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.drop_duplicates" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> with duplicate rows removed,
optionally only considering certain columns.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span>             <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">),</span>             <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">),</span>             <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">80</span><span class="p">)])</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">|  5|    80|Alice|</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">([</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">|  5|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.dropna">
<tt class="descname">dropna</tt><big>(</big><em>how='any'</em>, <em>thresh=None</em>, <em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.dropna" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> omitting rows with null values.
<a class="reference internal" href="#pyspark.sql.DataFrame.dropna" title="pyspark.sql.DataFrame.dropna"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.dropna()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.drop" title="pyspark.sql.DataFrameNaFunctions.drop"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameNaFunctions.drop()</span></tt></a> are aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>how</strong> &#8211; &#8216;any&#8217; or &#8216;all&#8217;.
If &#8216;any&#8217;, drop a row if it contains any nulls.
If &#8216;all&#8217;, drop a row only if all its values are null.</li>
<li><strong>thresh</strong> &#8211; int, default None
If specified, drop rows that have less than <cite>thresh</cite> non-null values.
This overwrites the <cite>how</cite> parameter.</li>
<li><strong>subset</strong> &#8211; optional list of column names to consider.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">drop</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.dtypes">
<tt class="descname">dtypes</tt><a class="headerlink" href="#pyspark.sql.DataFrame.dtypes" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns all column names and their data types as a list.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;int&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.explain">
<tt class="descname">explain</tt><big>(</big><em>extended=False</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.explain" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints the (logical and physical) plans to the console for debugging purpose.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>extended</strong> &#8211; boolean, default <tt class="docutils literal"><span class="pre">False</span></tt>. If <tt class="docutils literal"><span class="pre">False</span></tt>, prints only the physical plan.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">explain</span><span class="p">()</span>
<span class="go">PhysicalRDD [age#0,name#1], MapPartitionsRDD[...] at applySchemaToPythonRDD at          NativeMethodAccessorImpl.java:...</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">explain</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="go">== Parsed Logical Plan ==</span>
<span class="gp">...</span>
<span class="go">== Analyzed Logical Plan ==</span>
<span class="gp">...</span>
<span class="go">== Optimized Logical Plan ==</span>
<span class="gp">...</span>
<span class="go">== Physical Plan ==</span>
<span class="gp">...</span>
<span class="go">== RDD ==</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.fillna">
<tt class="descname">fillna</tt><big>(</big><em>value</em>, <em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.fillna" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace null values, alias for <tt class="docutils literal"><span class="pre">na.fill()</span></tt>.
<a class="reference internal" href="#pyspark.sql.DataFrame.fillna" title="pyspark.sql.DataFrame.fillna"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.fillna()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.fill" title="pyspark.sql.DataFrameNaFunctions.fill"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameNaFunctions.fill()</span></tt></a> are aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>value</strong> &#8211; int, long, float, string, or dict.
Value to replace null values with.
If the value is a dict, then <cite>subset</cite> is ignored and <cite>value</cite> must be a mapping
from column name (string) to replacement value. The replacement value must be
an int, long, float, or string.</li>
<li><strong>subset</strong> &#8211; optional list of column names to consider.
Columns specified in subset that do not have matching data type are ignored.
For example, if <cite>value</cite> is a string, and subset contains a non-string column,
then the non-string column is simply ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">|  5|    50|  Bob|</span>
<span class="go">| 50|    50|  Tom|</span>
<span class="go">| 50|    50| null|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">({</span><span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;unknown&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-------+</span>
<span class="go">|age|height|   name|</span>
<span class="go">+---+------+-------+</span>
<span class="go">| 10|    80|  Alice|</span>
<span class="go">|  5|  null|    Bob|</span>
<span class="go">| 50|  null|    Tom|</span>
<span class="go">| 50|  null|unknown|</span>
<span class="go">+---+------+-------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.filter">
<tt class="descname">filter</tt><big>(</big><em>condition</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.filter" title="Permalink to this definition">¶</a></dt>
<dd><p>Filters rows using the given condition.</p>
<p><a class="reference internal" href="#pyspark.sql.DataFrame.where" title="pyspark.sql.DataFrame.where"><tt class="xref py py-func docutils literal"><span class="pre">where()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.filter" title="pyspark.sql.DataFrame.filter"><tt class="xref py py-func docutils literal"><span class="pre">filter()</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>condition</strong> &#8211; a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> of <a class="reference internal" href="#pyspark.sql.types.BooleanType" title="pyspark.sql.types.BooleanType"><tt class="xref py py-class docutils literal"><span class="pre">types.BooleanType</span></tt></a>
or a string of SQL expression.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s">&quot;age &gt; 3&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s">&quot;age = 2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.first">
<tt class="descname">first</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.first" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first row as a <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="go">Row(age=2, name=u&#39;Alice&#39;)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.flatMap">
<tt class="descname">flatMap</tt><big>(</big><em>f</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.flatMap" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <tt class="xref py py-class docutils literal"><span class="pre">RDD</span></tt> by first applying the <tt class="docutils literal"><span class="pre">f</span></tt> function to each <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>,
and then flattening the results.</p>
<p>This is a shorthand for <tt class="docutils literal"><span class="pre">df.rdd.flatMap()</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[u&#39;A&#39;, u&#39;l&#39;, u&#39;i&#39;, u&#39;c&#39;, u&#39;e&#39;, u&#39;B&#39;, u&#39;o&#39;, u&#39;b&#39;]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.foreach">
<tt class="descname">foreach</tt><big>(</big><em>f</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.foreach" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the <tt class="docutils literal"><span class="pre">f</span></tt> function to all <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a> of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<p>This is a shorthand for <tt class="docutils literal"><span class="pre">df.rdd.foreach()</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">person</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="n">person</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.foreachPartition">
<tt class="descname">foreachPartition</tt><big>(</big><em>f</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.foreachPartition" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the <tt class="docutils literal"><span class="pre">f</span></tt> function to each partition of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<p>This a shorthand for <tt class="docutils literal"><span class="pre">df.rdd.foreachPartition()</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">people</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">person</span> <span class="ow">in</span> <span class="n">people</span><span class="p">:</span>
<span class="gp">... </span>        <span class="k">print</span><span class="p">(</span><span class="n">person</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">foreachPartition</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.freqItems">
<tt class="descname">freqItems</tt><big>(</big><em>cols</em>, <em>support=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.freqItems" title="Permalink to this definition">¶</a></dt>
<dd><p>Finding frequent items for columns, possibly with false positives. Using the
frequent element count algorithm described in
&#8220;<a class="reference external" href="http://dx.doi.org/10.1145/762471.762473">http://dx.doi.org/10.1145/762471.762473</a>, proposed by Karp, Schenker, and Papadimitriou&#8221;.
<a class="reference internal" href="#pyspark.sql.DataFrame.freqItems" title="pyspark.sql.DataFrame.freqItems"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.freqItems()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.freqItems" title="pyspark.sql.DataFrameStatFunctions.freqItems"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.freqItems()</span></tt></a> are aliases.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function is meant for exploratory data analysis, as we make no         guarantee about the backward compatibility of the schema of the resulting DataFrame.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>cols</strong> &#8211; Names of the columns to calculate frequent items for as a list or tuple of
strings.</li>
<li><strong>support</strong> &#8211; The frequency with which to consider an item &#8216;frequent&#8217;. Default is 1%.
The support must be greater than 1e-4.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.groupBy">
<tt class="descname">groupBy</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.groupBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Groups the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> using the specified columns,
so we can run aggregation on them. See <a class="reference internal" href="#pyspark.sql.GroupedData" title="pyspark.sql.GroupedData"><tt class="xref py py-class docutils literal"><span class="pre">GroupedData</span></tt></a>
for all the available aggregate functions.</p>
<p><a class="reference internal" href="#pyspark.sql.DataFrame.groupby" title="pyspark.sql.DataFrame.groupby"><tt class="xref py py-func docutils literal"><span class="pre">groupby()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.groupBy" title="pyspark.sql.DataFrame.groupBy"><tt class="xref py py-func docutils literal"><span class="pre">groupBy()</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of columns to group by.
Each element should be a column name (string) or an expression (<a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a>).</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">avg</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(AVG(age)=3.5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="s">&#39;mean&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, AVG(age)=2.0), Row(name=u&#39;Bob&#39;, AVG(age)=5.0)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">avg</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, AVG(age)=2.0), Row(name=u&#39;Bob&#39;, AVG(age)=5.0)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">([</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">])</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Bob&#39;, age=5, count=1), Row(name=u&#39;Alice&#39;, age=2, count=1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.groupby">
<tt class="descname">groupby</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.groupby" title="Permalink to this definition">¶</a></dt>
<dd><p>Groups the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> using the specified columns,
so we can run aggregation on them. See <a class="reference internal" href="#pyspark.sql.GroupedData" title="pyspark.sql.GroupedData"><tt class="xref py py-class docutils literal"><span class="pre">GroupedData</span></tt></a>
for all the available aggregate functions.</p>
<p><a class="reference internal" href="#pyspark.sql.DataFrame.groupby" title="pyspark.sql.DataFrame.groupby"><tt class="xref py py-func docutils literal"><span class="pre">groupby()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.groupBy" title="pyspark.sql.DataFrame.groupBy"><tt class="xref py py-func docutils literal"><span class="pre">groupBy()</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of columns to group by.
Each element should be a column name (string) or an expression (<a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a>).</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">avg</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(AVG(age)=3.5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="s">&#39;mean&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, AVG(age)=2.0), Row(name=u&#39;Bob&#39;, AVG(age)=5.0)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">avg</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, AVG(age)=2.0), Row(name=u&#39;Bob&#39;, AVG(age)=5.0)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">([</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">])</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Bob&#39;, age=5, count=1), Row(name=u&#39;Alice&#39;, age=2, count=1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.head">
<tt class="descname">head</tt><big>(</big><em>n=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.head" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first <tt class="docutils literal"><span class="pre">n</span></tt> rows.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>n</strong> &#8211; int, default 1. Number of rows to return.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">If n is greater than 1, return a list of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>.
If n is 1, return a single Row.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="go">Row(age=2, name=u&#39;Alice&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.insertInto">
<tt class="descname">insertInto</tt><big>(</big><em>tableName</em>, <em>overwrite=False</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.insertInto" title="Permalink to this definition">¶</a></dt>
<dd><p>Inserts the contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> into the specified table.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 1.4, use <a class="reference internal" href="#pyspark.sql.DataFrameWriter.insertInto" title="pyspark.sql.DataFrameWriter.insertInto"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameWriter.insertInto()</span></tt></a> instead.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.intersect">
<tt class="descname">intersect</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.intersect" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> containing rows only in
both this frame and another frame.</p>
<p>This is equivalent to <cite>INTERSECT</cite> in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.isLocal">
<tt class="descname">isLocal</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.isLocal" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <tt class="docutils literal"><span class="pre">True</span></tt> if the <a class="reference internal" href="#pyspark.sql.DataFrame.collect" title="pyspark.sql.DataFrame.collect"><tt class="xref py py-func docutils literal"><span class="pre">collect()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrame.take" title="pyspark.sql.DataFrame.take"><tt class="xref py py-func docutils literal"><span class="pre">take()</span></tt></a> methods can be run locally
(without any Spark executors).</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.join">
<tt class="descname">join</tt><big>(</big><em>other</em>, <em>joinExprs=None</em>, <em>joinType=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Joins with another <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>, using the given join expression.</p>
<p>The following performs a full outer join between <tt class="docutils literal"><span class="pre">df1</span></tt> and <tt class="docutils literal"><span class="pre">df2</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>other</strong> &#8211; Right side of the join</li>
<li><strong>joinExprs</strong> &#8211; a string for join column name, or a join expression (Column).
If joinExprs is a string indicating the name of the join column,
the column must exist on both sides, and this performs an inner equi-join.</li>
<li><strong>joinType</strong> &#8211; str, default &#8216;inner&#8217;.
One of <cite>inner</cite>, <cite>outer</cite>, <cite>left_outer</cite>, <cite>right_outer</cite>, <cite>semijoin</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s">&#39;outer&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df2</span><span class="o">.</span><span class="n">height</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=None, height=80), Row(name=u&#39;Alice&#39;, height=None), Row(name=u&#39;Bob&#39;, height=85)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df2</span><span class="o">.</span><span class="n">height</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Bob&#39;, height=85)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.limit">
<tt class="descname">limit</tt><big>(</big><em>num</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.limit" title="Permalink to this definition">¶</a></dt>
<dd><p>Limits the result count to the number specified.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.map">
<tt class="descname">map</tt><big>(</big><em>f</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.map" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <tt class="xref py py-class docutils literal"><span class="pre">RDD</span></tt> by applying a the <tt class="docutils literal"><span class="pre">f</span></tt> function to each <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>.</p>
<p>This is a shorthand for <tt class="docutils literal"><span class="pre">df.rdd.map()</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[u&#39;Alice&#39;, u&#39;Bob&#39;]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.mapPartitions">
<tt class="descname">mapPartitions</tt><big>(</big><em>f</em>, <em>preservesPartitioning=False</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.mapPartitions" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <tt class="xref py py-class docutils literal"><span class="pre">RDD</span></tt> by applying the <tt class="docutils literal"><span class="pre">f</span></tt> function to each partition.</p>
<p>This is a shorthand for <tt class="docutils literal"><span class="pre">df.rdd.mapPartitions()</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span> <span class="k">yield</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="go">4</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.na">
<tt class="descname">na</tt><a class="headerlink" href="#pyspark.sql.DataFrame.na" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions" title="pyspark.sql.DataFrameNaFunctions"><tt class="xref py py-class docutils literal"><span class="pre">DataFrameNaFunctions</span></tt></a> for handling missing values.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.orderBy">
<tt class="descname">orderBy</tt><big>(</big><em>*cols</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.orderBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> sorted by the specified column(s).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>cols</strong> &#8211; list of <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> or column names to sort by.</li>
<li><strong>ascending</strong> &#8211; boolean or list of boolean (default True).
Sort ascending vs. descending. Specify list for multiple sort orders.
If a list is specified, length of the list must equal length of the <cite>cols</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">asc</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;), Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">desc</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">),</span> <span class="s">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">([</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="s">&quot;name&quot;</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.persist">
<tt class="descname">persist</tt><big>(</big><em>storageLevel=StorageLevel(False</em>, <em>True</em>, <em>False</em>, <em>False</em>, <em>1)</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.persist" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the storage level to persist its values across operations
after the first time it is computed. This can only be used to assign
a new storage level if the RDD does not have a storage level set yet.
If no storage level is specified defaults to (<tt class="xref py py-class docutils literal"><span class="pre">MEMORY_ONLY_SER</span></tt>).</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.printSchema">
<tt class="descname">printSchema</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.printSchema" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints out the schema in the tree format.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="go">root</span>
<span class="go"> |-- age: integer (nullable = true)</span>
<span class="go"> |-- name: string (nullable = true)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.randomSplit">
<tt class="descname">randomSplit</tt><big>(</big><em>weights</em>, <em>seed=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.randomSplit" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly splits this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> with the provided weights.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>weights</strong> &#8211; list of doubles as weights with which to split the DataFrame. Weights will
be normalized if they don&#8217;t sum up to 1.0.</li>
<li><strong>seed</strong> &#8211; The seed for sampling.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span> <span class="o">=</span> <span class="n">df4</span><span class="o">.</span><span class="n">randomSplit</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="mi">24</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">splits</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.rdd">
<tt class="descname">rdd</tt><a class="headerlink" href="#pyspark.sql.DataFrame.rdd" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the content as an <a class="reference internal" href="pyspark.html#pyspark.RDD" title="pyspark.RDD"><tt class="xref py py-class docutils literal"><span class="pre">pyspark.RDD</span></tt></a> of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.registerAsTable">
<tt class="descname">registerAsTable</tt><big>(</big><em>name</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.registerAsTable" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 1.4, use <a class="reference internal" href="#pyspark.sql.DataFrame.registerTempTable" title="pyspark.sql.DataFrame.registerTempTable"><tt class="xref py py-func docutils literal"><span class="pre">registerTempTable()</span></tt></a> instead.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.registerTempTable">
<tt class="descname">registerTempTable</tt><big>(</big><em>name</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.registerTempTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers this RDD as a temporary table using the given name.</p>
<p>The lifetime of this temporary table is tied to the <a class="reference internal" href="#pyspark.sql.SQLContext" title="pyspark.sql.SQLContext"><tt class="xref py py-class docutils literal"><span class="pre">SQLContext</span></tt></a>
that was used to create this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s">&quot;people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;select * from people&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span> <span class="o">==</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.repartition">
<tt class="descname">repartition</tt><big>(</big><em>numPartitions</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.repartition" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> that has exactly <tt class="docutils literal"><span class="pre">numPartitions</span></tt> partitions.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
<span class="go">10</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.replace">
<tt class="descname">replace</tt><big>(</big><em>to_replace</em>, <em>value</em>, <em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> replacing a value with another value.
<a class="reference internal" href="#pyspark.sql.DataFrame.replace" title="pyspark.sql.DataFrame.replace"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.replace()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.replace" title="pyspark.sql.DataFrameNaFunctions.replace"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameNaFunctions.replace()</span></tt></a> are
aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>to_replace</strong> &#8211; int, long, float, string, or list.
Value to be replaced.
If the value is a dict, then <cite>value</cite> is ignored and <cite>to_replace</cite> must be a
mapping from column name (string) to replacement value. The value to be
replaced must be an int, long, float, or string.</li>
<li><strong>value</strong> &#8211; int, long, float, string, or list.
Value to use to replace holes.
The replacement value must be an int, long, float, or string. If <cite>value</cite> is a
list or tuple, <cite>value</cite> should be of the same length with <cite>to_replace</cite>.</li>
<li><strong>subset</strong> &#8211; optional list of column names to consider.
Columns specified in subset that do not have matching data type are ignored.
For example, if <cite>value</cite> is a string, and subset contains a non-string column,
then the non-string column is simply ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+-----+</span>
<span class="go">| age|height| name|</span>
<span class="go">+----+------+-----+</span>
<span class="go">|  20|    80|Alice|</span>
<span class="go">|   5|  null|  Bob|</span>
<span class="go">|null|  null|  Tom|</span>
<span class="go">|null|  null| null|</span>
<span class="go">+----+------+-----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="s">&#39;Bob&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;A&#39;</span><span class="p">,</span> <span class="s">&#39;B&#39;</span><span class="p">],</span> <span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+----+</span>
<span class="go">| age|height|name|</span>
<span class="go">+----+------+----+</span>
<span class="go">|  10|    80|   A|</span>
<span class="go">|   5|  null|   B|</span>
<span class="go">|null|  null| Tom|</span>
<span class="go">|null|  null|null|</span>
<span class="go">+----+------+----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.rollup">
<tt class="descname">rollup</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.rollup" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a multi-dimensional rollup for the current <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> using
the specified columns, so we can run aggregation on them.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">rollup</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+----+-----+</span>
<span class="go">| name| age|count|</span>
<span class="go">+-----+----+-----+</span>
<span class="go">|Alice|null|    1|</span>
<span class="go">|  Bob|   5|    1|</span>
<span class="go">|  Bob|null|    1|</span>
<span class="go">| null|null|    2|</span>
<span class="go">|Alice|   2|    1|</span>
<span class="go">+-----+----+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.sample">
<tt class="descname">sample</tt><big>(</big><em>withReplacement</em>, <em>fraction</em>, <em>seed=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sampled subset of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">False</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="go">1</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.save">
<tt class="descname">save</tt><big>(</big><em>path=None</em>, <em>source=None</em>, <em>mode='error'</em>, <em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the contents of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to a data source.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 1.4, use <a class="reference internal" href="#pyspark.sql.DataFrameWriter.save" title="pyspark.sql.DataFrameWriter.save"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameWriter.save()</span></tt></a> instead.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.saveAsParquetFile">
<tt class="descname">saveAsParquetFile</tt><big>(</big><em>path</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.saveAsParquetFile" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the contents as a Parquet file, preserving the schema.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 1.4, use <a class="reference internal" href="#pyspark.sql.DataFrameWriter.parquet" title="pyspark.sql.DataFrameWriter.parquet"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameWriter.parquet()</span></tt></a> instead.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.saveAsTable">
<tt class="descname">saveAsTable</tt><big>(</big><em>tableName</em>, <em>source=None</em>, <em>mode='error'</em>, <em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.saveAsTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to a data source as a table.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Deprecated in 1.4, use <a class="reference internal" href="#pyspark.sql.DataFrameWriter.saveAsTable" title="pyspark.sql.DataFrameWriter.saveAsTable"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameWriter.saveAsTable()</span></tt></a> instead.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.schema">
<tt class="descname">schema</tt><a class="headerlink" href="#pyspark.sql.DataFrame.schema" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the schema of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as a <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><tt class="xref py py-class docutils literal"><span class="pre">types.StructType</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">schema</span>
<span class="go">StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.select">
<tt class="descname">select</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.select" title="Permalink to this definition">¶</a></dt>
<dd><p>Projects a set of expressions and returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string) or expressions (<a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a>).
If one of the column names is &#8216;*&#8217;, that column is expanded to include all columns
in the current DataFrame.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&#39;*&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;), Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&#39;name&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=2), Row(name=u&#39;Bob&#39;, age=5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">+</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, age=12), Row(name=u&#39;Bob&#39;, age=15)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.selectExpr">
<tt class="descname">selectExpr</tt><big>(</big><em>*expr</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.selectExpr" title="Permalink to this definition">¶</a></dt>
<dd><p>Projects a set of SQL expressions and returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<p>This is a variant of <a class="reference internal" href="#pyspark.sql.DataFrame.select" title="pyspark.sql.DataFrame.select"><tt class="xref py py-func docutils literal"><span class="pre">select()</span></tt></a> that accepts SQL expressions.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">&quot;age * 2&quot;</span><span class="p">,</span> <span class="s">&quot;abs(age)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row((age * 2)=4, Abs(age)=2), Row((age * 2)=10, Abs(age)=5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.show">
<tt class="descname">show</tt><big>(</big><em>n=20</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.show" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints the first <tt class="docutils literal"><span class="pre">n</span></tt> rows to the console.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span>
<span class="go">DataFrame[age: int, name: string]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|age| name|</span>
<span class="go">+---+-----+</span>
<span class="go">|  2|Alice|</span>
<span class="go">|  5|  Bob|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.sort">
<tt class="descname">sort</tt><big>(</big><em>*cols</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.sort" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> sorted by the specified column(s).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>cols</strong> &#8211; list of <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> or column names to sort by.</li>
<li><strong>ascending</strong> &#8211; boolean or list of boolean (default True).
Sort ascending vs. descending. Specify list for multiple sort orders.
If a list is specified, length of the list must equal length of the <cite>cols</cite>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">desc</span><span class="p">())</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">asc</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;), Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">desc</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">),</span> <span class="s">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">([</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="s">&quot;name&quot;</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;), Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.stat">
<tt class="descname">stat</tt><a class="headerlink" href="#pyspark.sql.DataFrame.stat" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions" title="pyspark.sql.DataFrameStatFunctions"><tt class="xref py py-class docutils literal"><span class="pre">DataFrameStatFunctions</span></tt></a> for statistic functions.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.subtract">
<tt class="descname">subtract</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.subtract" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> containing rows in this frame
but not in another frame.</p>
<p>This is equivalent to <cite>EXCEPT</cite> in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.take">
<tt class="descname">take</tt><big>(</big><em>num</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.take" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first <tt class="docutils literal"><span class="pre">num</span></tt> rows as a <tt class="xref py py-class docutils literal"><span class="pre">list</span></tt> of <a class="reference internal" href="#pyspark.sql.Row" title="pyspark.sql.Row"><tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;), Row(age=5, name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.toJSON">
<tt class="descname">toJSON</tt><big>(</big><em>use_unicode=True</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.toJSON" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> into a <tt class="xref py py-class docutils literal"><span class="pre">RDD</span></tt> of string.</p>
<p>Each row is turned into a JSON document as one element in the returned RDD.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">toJSON</span><span class="p">()</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="go">u&#39;{&quot;age&quot;:2,&quot;name&quot;:&quot;Alice&quot;}&#39;</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.toPandas">
<tt class="descname">toPandas</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.toPandas" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as Pandas <tt class="docutils literal"><span class="pre">pandas.DataFrame</span></tt>.</p>
<p>This is only available if Pandas is installed and available.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>  
<span class="go">   age   name</span>
<span class="go">0    2  Alice</span>
<span class="go">1    5    Bob</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.unionAll">
<tt class="descname">unionAll</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.unionAll" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> containing union of rows in this
frame and another frame.</p>
<p>This is equivalent to <cite>UNION ALL</cite> in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.unpersist">
<tt class="descname">unpersist</tt><big>(</big><em>blocking=True</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.unpersist" title="Permalink to this definition">¶</a></dt>
<dd><p>Marks the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as non-persistent, and remove all blocks for it from
memory and disk.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.where">
<tt class="descname">where</tt><big>(</big><em>condition</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.where" title="Permalink to this definition">¶</a></dt>
<dd><p>Filters rows using the given condition.</p>
<p><a class="reference internal" href="#pyspark.sql.DataFrame.where" title="pyspark.sql.DataFrame.where"><tt class="xref py py-func docutils literal"><span class="pre">where()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.DataFrame.filter" title="pyspark.sql.DataFrame.filter"><tt class="xref py py-func docutils literal"><span class="pre">filter()</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>condition</strong> &#8211; a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> of <a class="reference internal" href="#pyspark.sql.types.BooleanType" title="pyspark.sql.types.BooleanType"><tt class="xref py py-class docutils literal"><span class="pre">types.BooleanType</span></tt></a>
or a string of SQL expression.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s">&quot;age &gt; 3&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s">&quot;age = 2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.withColumn">
<tt class="descname">withColumn</tt><big>(</big><em>colName</em>, <em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.withColumn" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> by adding a column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>colName</strong> &#8211; string, name of the new column.</li>
<li><strong>col</strong> &#8211; a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> expression for the new column.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">&#39;age2&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;, age2=4), Row(age=5, name=u&#39;Bob&#39;, age2=7)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrame.withColumnRenamed">
<tt class="descname">withColumnRenamed</tt><big>(</big><em>existing</em>, <em>new</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrame.withColumnRenamed" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> by renaming an existing column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>existing</strong> &#8211; string, name of the existing column to rename.</li>
<li><strong>col</strong> &#8211; string, new name of the column.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;age2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age2=2, name=u&#39;Alice&#39;), Row(age2=5, name=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pyspark.sql.DataFrame.write">
<tt class="descname">write</tt><a class="headerlink" href="#pyspark.sql.DataFrame.write" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface for saving the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> out into external storage.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pyspark.sql.DataFrameWriter" title="pyspark.sql.DataFrameWriter"><tt class="xref py py-class docutils literal"><span class="pre">DataFrameWriter</span></tt></a></td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.GroupedData">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">GroupedData</tt><big>(</big><em>jdf</em>, <em>sql_ctx</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData" title="Permalink to this definition">¶</a></dt>
<dd><p>A set of methods for aggregations on a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>,
created by <a class="reference internal" href="#pyspark.sql.DataFrame.groupBy" title="pyspark.sql.DataFrame.groupBy"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.groupBy()</span></tt></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.GroupedData.agg">
<tt class="descname">agg</tt><big>(</big><em>*exprs</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.agg" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute aggregates and returns the result as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<p>The available aggregate functions are <cite>avg</cite>, <cite>max</cite>, <cite>min</cite>, <cite>sum</cite>, <cite>count</cite>.</p>
<p>If <tt class="docutils literal"><span class="pre">exprs</span></tt> is a single <tt class="xref py py-class docutils literal"><span class="pre">dict</span></tt> mapping from string to string, then the key
is the column to perform aggregation on, and the value is the aggregate function.</p>
<p>Alternatively, <tt class="docutils literal"><span class="pre">exprs</span></tt> can also be a list of aggregate <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> expressions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>exprs</strong> &#8211; a dict mapping from column name (string) to aggregate functions (string),
or a list of <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a>.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">gdf</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gdf</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s">&quot;*&quot;</span><span class="p">:</span> <span class="s">&quot;count&quot;</span><span class="p">})</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, COUNT(1)=1), Row(name=u&#39;Bob&#39;, COUNT(1)=1)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gdf</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(name=u&#39;Alice&#39;, MIN(age)=2), Row(name=u&#39;Bob&#39;, MIN(age)=5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.avg">
<tt class="descname">avg</tt><big>(</big><em>*args</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.avg" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes average values for each numeric columns for each group.</p>
<p><a class="reference internal" href="#pyspark.sql.GroupedData.mean" title="pyspark.sql.GroupedData.mean"><tt class="xref py py-func docutils literal"><span class="pre">mean()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.GroupedData.avg" title="pyspark.sql.GroupedData.avg"><tt class="xref py py-func docutils literal"><span class="pre">avg()</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string). Non-numeric columns are ignored.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">avg</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(AVG(age)=3.5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">avg</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(AVG(age)=3.5, AVG(height)=82.5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.count">
<tt class="descname">count</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Counts the number of records for each group.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, count=1), Row(age=5, count=1)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.max">
<tt class="descname">max</tt><big>(</big><em>*args</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.max" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the max value for each numeric columns for each group.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(MAX(age)=5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(MAX(age)=5, MAX(height)=85)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.mean">
<tt class="descname">mean</tt><big>(</big><em>*args</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes average values for each numeric columns for each group.</p>
<p><a class="reference internal" href="#pyspark.sql.GroupedData.mean" title="pyspark.sql.GroupedData.mean"><tt class="xref py py-func docutils literal"><span class="pre">mean()</span></tt></a> is an alias for <a class="reference internal" href="#pyspark.sql.GroupedData.avg" title="pyspark.sql.GroupedData.avg"><tt class="xref py py-func docutils literal"><span class="pre">avg()</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string). Non-numeric columns are ignored.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(AVG(age)=3.5)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(AVG(age)=3.5, AVG(height)=82.5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.min">
<tt class="descname">min</tt><big>(</big><em>*args</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.min" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the min value for each numeric column for each group.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string). Non-numeric columns are ignored.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(MIN(age)=2)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(MIN(age)=2, MIN(height)=80)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.GroupedData.sum">
<tt class="descname">sum</tt><big>(</big><em>*args</em><big>)</big><a class="headerlink" href="#pyspark.sql.GroupedData.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the sum for each numeric columns for each group.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string). Non-numeric columns are ignored.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(SUM(age)=7)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df3</span><span class="o">.</span><span class="n">groupBy</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;height&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(SUM(age)=7, SUM(height)=165)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.Column">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">Column</tt><big>(</big><em>jc</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column" title="Permalink to this definition">¶</a></dt>
<dd><p>A column in a DataFrame.</p>
<p><a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> instances can be created by:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># 1. Select a column out of a DataFrame</span>

<span class="n">df</span><span class="o">.</span><span class="n">colName</span>
<span class="n">df</span><span class="p">[</span><span class="s">&quot;colName&quot;</span><span class="p">]</span>

<span class="c"># 2. Create from an expression</span>
<span class="n">df</span><span class="o">.</span><span class="n">colName</span> <span class="o">+</span> <span class="mi">1</span>
<span class="mi">1</span> <span class="o">/</span> <span class="n">df</span><span class="o">.</span><span class="n">colName</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.Column.alias">
<tt class="descname">alias</tt><big>(</big><em>*alias</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.alias" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns this column aliased with a new name or names (in the case of expressions that
return more than one column, such as explode).</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;age2&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age2=2), Row(age2=5)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.asc">
<tt class="descname">asc</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.Column.asc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the ascending order of the given column name.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.astype">
<tt class="descname">astype</tt><big>(</big><em>dataType</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.astype" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert the column into type <tt class="docutils literal"><span class="pre">dataType</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s">&quot;string&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;ages&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(ages=u&#39;2&#39;), Row(ages=u&#39;5&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">StringType</span><span class="p">())</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;ages&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(ages=u&#39;2&#39;), Row(ages=u&#39;5&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.between">
<tt class="descname">between</tt><big>(</big><em>lowerBound</em>, <em>upperBound</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.between" title="Permalink to this definition">¶</a></dt>
<dd><p>A boolean expression that is evaluated to true if the value of this
expression is between the given columns.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+--------------------------+</span>
<span class="go">| name|((age &gt;= 2) &amp;&amp; (age &lt;= 4))|</span>
<span class="go">+-----+--------------------------+</span>
<span class="go">|Alice|                      true|</span>
<span class="go">|  Bob|                     false|</span>
<span class="go">+-----+--------------------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.bitwiseAND">
<tt class="descname">bitwiseAND</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.bitwiseAND" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.bitwiseOR">
<tt class="descname">bitwiseOR</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.bitwiseOR" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.bitwiseXOR">
<tt class="descname">bitwiseXOR</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.bitwiseXOR" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.cast">
<tt class="descname">cast</tt><big>(</big><em>dataType</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.cast" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert the column into type <tt class="docutils literal"><span class="pre">dataType</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s">&quot;string&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;ages&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(ages=u&#39;2&#39;), Row(ages=u&#39;5&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">StringType</span><span class="p">())</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;ages&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(ages=u&#39;2&#39;), Row(ages=u&#39;5&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.desc">
<tt class="descname">desc</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.Column.desc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the descending order of the given column name.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.endswith">
<tt class="descname">endswith</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.endswith" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.getField">
<tt class="descname">getField</tt><big>(</big><em>name</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.getField" title="Permalink to this definition">¶</a></dt>
<dd><p>An expression that gets a field by name in a StructField.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="s">&quot;b&quot;</span><span class="p">))])</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">getField</span><span class="p">(</span><span class="s">&quot;b&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+</span>
<span class="go">|r[b]|</span>
<span class="go">+----+</span>
<span class="go">|   b|</span>
<span class="go">+----+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+</span>
<span class="go">|r[a]|</span>
<span class="go">+----+</span>
<span class="go">|   1|</span>
<span class="go">+----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.getItem">
<tt class="descname">getItem</tt><big>(</big><em>key</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.getItem" title="Permalink to this definition">¶</a></dt>
<dd><p>An expression that gets an item at position <tt class="docutils literal"><span class="pre">ordinal</span></tt> out of a list,
or gets an item by key out of a dict.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">{</span><span class="s">&quot;key&quot;</span><span class="p">:</span> <span class="s">&quot;value&quot;</span><span class="p">})])</span><span class="o">.</span><span class="n">toDF</span><span class="p">([</span><span class="s">&quot;l&quot;</span><span class="p">,</span> <span class="s">&quot;d&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">l</span><span class="o">.</span><span class="n">getItem</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">getItem</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+</span>
<span class="go">|l[0]|d[key]|</span>
<span class="go">+----+------+</span>
<span class="go">|   1| value|</span>
<span class="go">+----+------+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">df</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="s">&quot;key&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+</span>
<span class="go">|l[0]|d[key]|</span>
<span class="go">+----+------+</span>
<span class="go">|   1| value|</span>
<span class="go">+----+------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.inSet">
<tt class="descname">inSet</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.inSet" title="Permalink to this definition">¶</a></dt>
<dd><p>A boolean expression that is evaluated to true if the value of this
expression is contained by the evaluated values of the arguments.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">inSet</span><span class="p">(</span><span class="s">&quot;Bob&quot;</span><span class="p">,</span> <span class="s">&quot;Mike&quot;</span><span class="p">)]</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=5, name=u&#39;Bob&#39;)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="o">.</span><span class="n">inSet</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])]</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=2, name=u&#39;Alice&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.isNotNull">
<tt class="descname">isNotNull</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.Column.isNotNull" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the current expression is not null.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.isNull">
<tt class="descname">isNull</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.Column.isNull" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the current expression is null.</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.like">
<tt class="descname">like</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.like" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.otherwise">
<tt class="descname">otherwise</tt><big>(</big><em>value</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.otherwise" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates a list of conditions and returns one of multiple possible result expressions.
If <a class="reference internal" href="#pyspark.sql.Column.otherwise" title="pyspark.sql.Column.otherwise"><tt class="xref py py-func docutils literal"><span class="pre">Column.otherwise()</span></tt></a> is not invoked, None is returned for unmatched conditions.</p>
<p>See <a class="reference internal" href="#pyspark.sql.functions.when" title="pyspark.sql.functions.when"><tt class="xref py py-func docutils literal"><span class="pre">pyspark.sql.functions.when()</span></tt></a> for example usage.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>value</strong> &#8211; a literal value, or a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> expression.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+---------------------------------+</span>
<span class="go">| name|CASE WHEN (age &gt; 3) THEN 1 ELSE 0|</span>
<span class="go">+-----+---------------------------------+</span>
<span class="go">|Alice|                                0|</span>
<span class="go">|  Bob|                                1|</span>
<span class="go">+-----+---------------------------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.over">
<tt class="descname">over</tt><big>(</big><em>window</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.over" title="Permalink to this definition">¶</a></dt>
<dd><p>Define a windowing column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>window</strong> &#8211; a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><tt class="xref py py-class docutils literal"><span class="pre">WindowSpec</span></tt></a></td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">a Column</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Window</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rowsBetween</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">rank</span><span class="p">,</span> <span class="nb">min</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># df.select(rank().over(window), min(&#39;age&#39;).over(window))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Window functions is only supported with HiveContext in 1.4</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.rlike">
<tt class="descname">rlike</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.rlike" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.startswith">
<tt class="descname">startswith</tt><big>(</big><em>other</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.startswith" title="Permalink to this definition">¶</a></dt>
<dd><p>binary operator</p>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.substr">
<tt class="descname">substr</tt><big>(</big><em>startPos</em>, <em>length</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.substr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> which is a substring of the column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>startPos</strong> &#8211; start position (int or Column)</li>
<li><strong>length</strong> &#8211; length of the substring (int or Column)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">substr</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;col&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(col=u&#39;Ali&#39;), Row(col=u&#39;Bob&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.Column.when">
<tt class="descname">when</tt><big>(</big><em>condition</em>, <em>value</em><big>)</big><a class="headerlink" href="#pyspark.sql.Column.when" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates a list of conditions and returns one of multiple possible result expressions.
If <a class="reference internal" href="#pyspark.sql.Column.otherwise" title="pyspark.sql.Column.otherwise"><tt class="xref py py-func docutils literal"><span class="pre">Column.otherwise()</span></tt></a> is not invoked, None is returned for unmatched conditions.</p>
<p>See <a class="reference internal" href="#pyspark.sql.functions.when" title="pyspark.sql.functions.when"><tt class="xref py py-func docutils literal"><span class="pre">pyspark.sql.functions.when()</span></tt></a> for example usage.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>condition</strong> &#8211; a boolean <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> expression.</li>
<li><strong>value</strong> &#8211; a literal value, or a <a class="reference internal" href="#pyspark.sql.Column" title="pyspark.sql.Column"><tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt></a> expression.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-----+--------------------------------------------------------+</span>
<span class="go">| name|CASE WHEN (age &gt; 4) THEN 1 WHEN (age &lt; 3) THEN -1 ELSE 0|</span>
<span class="go">+-----+--------------------------------------------------------+</span>
<span class="go">|Alice|                                                      -1|</span>
<span class="go">|  Bob|                                                       1|</span>
<span class="go">+-----+--------------------------------------------------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.Row">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">Row</tt><a class="headerlink" href="#pyspark.sql.Row" title="Permalink to this definition">¶</a></dt>
<dd><p>A row in <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>. The fields in it can be accessed like attributes.</p>
<p>Row can be used to create a row object by using named arguments,
the fields will be sorted by names.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">row</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">&quot;Alice&quot;</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span>
<span class="go">Row(age=11, name=&#39;Alice&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">row</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">row</span><span class="o">.</span><span class="n">age</span>
<span class="go">(&#39;Alice&#39;, 11)</span>
</pre></div>
</div>
<p>Row also can be used to create another Row like class, then it
could be used to create Row objects, such as</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="s">&quot;age&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span>
<span class="go">&lt;Row(name, age)&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Person</span><span class="p">(</span><span class="s">&quot;Alice&quot;</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="go">Row(name=&#39;Alice&#39;, age=11)</span>
</pre></div>
</div>
<dl class="method">
<dt id="pyspark.sql.Row.asDict">
<tt class="descname">asDict</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.Row.asDict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return as an dict</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrameNaFunctions">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DataFrameNaFunctions</tt><big>(</big><em>df</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameNaFunctions" title="Permalink to this definition">¶</a></dt>
<dd><p>Functionality for working with missing data in <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrameNaFunctions.drop">
<tt class="descname">drop</tt><big>(</big><em>how='any'</em>, <em>thresh=None</em>, <em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameNaFunctions.drop" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> omitting rows with null values.
<a class="reference internal" href="#pyspark.sql.DataFrame.dropna" title="pyspark.sql.DataFrame.dropna"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.dropna()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.drop" title="pyspark.sql.DataFrameNaFunctions.drop"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameNaFunctions.drop()</span></tt></a> are aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>how</strong> &#8211; &#8216;any&#8217; or &#8216;all&#8217;.
If &#8216;any&#8217;, drop a row if it contains any nulls.
If &#8216;all&#8217;, drop a row only if all its values are null.</li>
<li><strong>thresh</strong> &#8211; int, default None
If specified, drop rows that have less than <cite>thresh</cite> non-null values.
This overwrites the <cite>how</cite> parameter.</li>
<li><strong>subset</strong> &#8211; optional list of column names to consider.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">drop</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameNaFunctions.fill">
<tt class="descname">fill</tt><big>(</big><em>value</em>, <em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameNaFunctions.fill" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace null values, alias for <tt class="docutils literal"><span class="pre">na.fill()</span></tt>.
<a class="reference internal" href="#pyspark.sql.DataFrame.fillna" title="pyspark.sql.DataFrame.fillna"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.fillna()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.fill" title="pyspark.sql.DataFrameNaFunctions.fill"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameNaFunctions.fill()</span></tt></a> are aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>value</strong> &#8211; int, long, float, string, or dict.
Value to replace null values with.
If the value is a dict, then <cite>subset</cite> is ignored and <cite>value</cite> must be a mapping
from column name (string) to replacement value. The replacement value must be
an int, long, float, or string.</li>
<li><strong>subset</strong> &#8211; optional list of column names to consider.
Columns specified in subset that do not have matching data type are ignored.
For example, if <cite>value</cite> is a string, and subset contains a non-string column,
then the non-string column is simply ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-----+</span>
<span class="go">|age|height| name|</span>
<span class="go">+---+------+-----+</span>
<span class="go">| 10|    80|Alice|</span>
<span class="go">|  5|    50|  Bob|</span>
<span class="go">| 50|    50|  Tom|</span>
<span class="go">| 50|    50| null|</span>
<span class="go">+---+------+-----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">({</span><span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;unknown&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+------+-------+</span>
<span class="go">|age|height|   name|</span>
<span class="go">+---+------+-------+</span>
<span class="go">| 10|    80|  Alice|</span>
<span class="go">|  5|  null|    Bob|</span>
<span class="go">| 50|  null|    Tom|</span>
<span class="go">| 50|  null|unknown|</span>
<span class="go">+---+------+-------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.1.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameNaFunctions.replace">
<tt class="descname">replace</tt><big>(</big><em>to_replace</em>, <em>value</em>, <em>subset=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameNaFunctions.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> replacing a value with another value.
<a class="reference internal" href="#pyspark.sql.DataFrame.replace" title="pyspark.sql.DataFrame.replace"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.replace()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameNaFunctions.replace" title="pyspark.sql.DataFrameNaFunctions.replace"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameNaFunctions.replace()</span></tt></a> are
aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>to_replace</strong> &#8211; int, long, float, string, or list.
Value to be replaced.
If the value is a dict, then <cite>value</cite> is ignored and <cite>to_replace</cite> must be a
mapping from column name (string) to replacement value. The value to be
replaced must be an int, long, float, or string.</li>
<li><strong>value</strong> &#8211; int, long, float, string, or list.
Value to use to replace holes.
The replacement value must be an int, long, float, or string. If <cite>value</cite> is a
list or tuple, <cite>value</cite> should be of the same length with <cite>to_replace</cite>.</li>
<li><strong>subset</strong> &#8211; optional list of column names to consider.
Columns specified in subset that do not have matching data type are ignored.
For example, if <cite>value</cite> is a string, and subset contains a non-string column,
then the non-string column is simply ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+-----+</span>
<span class="go">| age|height| name|</span>
<span class="go">+----+------+-----+</span>
<span class="go">|  20|    80|Alice|</span>
<span class="go">|   5|  null|  Bob|</span>
<span class="go">|null|  null|  Tom|</span>
<span class="go">|null|  null| null|</span>
<span class="go">+----+------+-----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df4</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s">&#39;Alice&#39;</span><span class="p">,</span> <span class="s">&#39;Bob&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;A&#39;</span><span class="p">,</span> <span class="s">&#39;B&#39;</span><span class="p">],</span> <span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+------+----+</span>
<span class="go">| age|height|name|</span>
<span class="go">+----+------+----+</span>
<span class="go">|  10|    80|   A|</span>
<span class="go">|   5|  null|   B|</span>
<span class="go">|null|  null| Tom|</span>
<span class="go">|null|  null|null|</span>
<span class="go">+----+------+----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrameStatFunctions">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DataFrameStatFunctions</tt><big>(</big><em>df</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions" title="Permalink to this definition">¶</a></dt>
<dd><p>Functionality for statistic functions with <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.corr">
<tt class="descname">corr</tt><big>(</big><em>col1</em>, <em>col2</em>, <em>method=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.corr" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the correlation of two columns of a DataFrame as a double value.
Currently only supports the Pearson Correlation Coefficient.
<a class="reference internal" href="#pyspark.sql.DataFrame.corr" title="pyspark.sql.DataFrame.corr"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.corr()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.corr" title="pyspark.sql.DataFrameStatFunctions.corr"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.corr()</span></tt></a> are aliases of each other.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col1</strong> &#8211; The name of the first column</li>
<li><strong>col2</strong> &#8211; The name of the second column</li>
<li><strong>method</strong> &#8211; The correlation method. Currently only supports &#8220;pearson&#8221;</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.cov">
<tt class="descname">cov</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.cov" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the sample covariance for the given columns, specified by their names, as a
double value. <a class="reference internal" href="#pyspark.sql.DataFrame.cov" title="pyspark.sql.DataFrame.cov"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.cov()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.cov" title="pyspark.sql.DataFrameStatFunctions.cov"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.cov()</span></tt></a> are aliases.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col1</strong> &#8211; The name of the first column</li>
<li><strong>col2</strong> &#8211; The name of the second column</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.crosstab">
<tt class="descname">crosstab</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.crosstab" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a pair-wise frequency table of the given columns. Also known as a contingency
table. The number of distinct values for each column should be less than 1e4. At most 1e6
non-zero pair frequencies will be returned.
The first column of each row will be the distinct values of <cite>col1</cite> and the column names
will be the distinct values of <cite>col2</cite>. The name of the first column will be <cite>$col1_$col2</cite>.
Pairs that have no occurrences will have <cite>null</cite> as their counts.
<a class="reference internal" href="#pyspark.sql.DataFrame.crosstab" title="pyspark.sql.DataFrame.crosstab"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.crosstab()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.crosstab" title="pyspark.sql.DataFrameStatFunctions.crosstab"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.crosstab()</span></tt></a> are aliases.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col1</strong> &#8211; The name of the first column. Distinct items will make the first item of
each row.</li>
<li><strong>col2</strong> &#8211; The name of the second column. Distinct items will make the column names
of the DataFrame.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameStatFunctions.freqItems">
<tt class="descname">freqItems</tt><big>(</big><em>cols</em>, <em>support=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameStatFunctions.freqItems" title="Permalink to this definition">¶</a></dt>
<dd><p>Finding frequent items for columns, possibly with false positives. Using the
frequent element count algorithm described in
&#8220;<a class="reference external" href="http://dx.doi.org/10.1145/762471.762473">http://dx.doi.org/10.1145/762471.762473</a>, proposed by Karp, Schenker, and Papadimitriou&#8221;.
<a class="reference internal" href="#pyspark.sql.DataFrame.freqItems" title="pyspark.sql.DataFrame.freqItems"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.freqItems()</span></tt></a> and <a class="reference internal" href="#pyspark.sql.DataFrameStatFunctions.freqItems" title="pyspark.sql.DataFrameStatFunctions.freqItems"><tt class="xref py py-func docutils literal"><span class="pre">DataFrameStatFunctions.freqItems()</span></tt></a> are aliases.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This function is meant for exploratory data analysis, as we make no         guarantee about the backward compatibility of the schema of the resulting DataFrame.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>cols</strong> &#8211; Names of the columns to calculate frequent items for as a list or tuple of
strings.</li>
<li><strong>support</strong> &#8211; The frequency with which to consider an item &#8216;frequent&#8217;. Default is 1%.
The support must be greater than 1e-4.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.Window">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">Window</tt><a class="headerlink" href="#pyspark.sql.Window" title="Permalink to this definition">¶</a></dt>
<dd><p>Utility functions for defining window in DataFrames.</p>
<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># PARTITION BY country ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s">&quot;country&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">&quot;date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rowsBetween</span><span class="p">(</span><span class="o">-</span><span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># PARTITION BY country ORDER BY date RANGE BETWEEN 3 PRECEDING AND 3 FOLLOWING</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s">&quot;date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s">&quot;country&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">rangeBetween</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<dl class="staticmethod">
<dt id="pyspark.sql.Window.orderBy">
<em class="property">static </em><tt class="descname">orderBy</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.Window.orderBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><tt class="xref py py-class docutils literal"><span class="pre">WindowSpec</span></tt></a> with the partitioning defined.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="staticmethod">
<dt id="pyspark.sql.Window.partitionBy">
<em class="property">static </em><tt class="descname">partitionBy</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.Window.partitionBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><tt class="xref py py-class docutils literal"><span class="pre">WindowSpec</span></tt></a> with the partitioning defined.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.WindowSpec">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">WindowSpec</tt><big>(</big><em>jspec</em><big>)</big><a class="headerlink" href="#pyspark.sql.WindowSpec" title="Permalink to this definition">¶</a></dt>
<dd><p>A window specification that defines the partitioning, ordering,
and frame boundaries.</p>
<p>Use the static methods in <a class="reference internal" href="#pyspark.sql.Window" title="pyspark.sql.Window"><tt class="xref py py-class docutils literal"><span class="pre">Window</span></tt></a> to create a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><tt class="xref py py-class docutils literal"><span class="pre">WindowSpec</span></tt></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Experimental</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.WindowSpec.orderBy">
<tt class="descname">orderBy</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.WindowSpec.orderBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the ordering columns in a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><tt class="xref py py-class docutils literal"><span class="pre">WindowSpec</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; names of columns or expressions</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.WindowSpec.partitionBy">
<tt class="descname">partitionBy</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.WindowSpec.partitionBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the partitioning columns in a <a class="reference internal" href="#pyspark.sql.WindowSpec" title="pyspark.sql.WindowSpec"><tt class="xref py py-class docutils literal"><span class="pre">WindowSpec</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; names of columns or expressions</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.WindowSpec.rangeBetween">
<tt class="descname">rangeBetween</tt><big>(</big><em>start</em>, <em>end</em><big>)</big><a class="headerlink" href="#pyspark.sql.WindowSpec.rangeBetween" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the frame boundaries, from <cite>start</cite> (inclusive) to <cite>end</cite> (inclusive).</p>
<p>Both <cite>start</cite> and <cite>end</cite> are relative from the current row. For example,
&#8220;0&#8221; means &#8220;current row&#8221;, while &#8220;-1&#8221; means one off before the current row,
and &#8220;5&#8221; means the five off after the current row.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>start</strong> &#8211; boundary start, inclusive.
The frame is unbounded if this is <tt class="docutils literal"><span class="pre">-sys.maxsize</span></tt> (or lower).</li>
<li><strong>end</strong> &#8211; boundary end, inclusive.
The frame is unbounded if this is <tt class="docutils literal"><span class="pre">sys.maxsize</span></tt> (or higher).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.WindowSpec.rowsBetween">
<tt class="descname">rowsBetween</tt><big>(</big><em>start</em>, <em>end</em><big>)</big><a class="headerlink" href="#pyspark.sql.WindowSpec.rowsBetween" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the frame boundaries, from <cite>start</cite> (inclusive) to <cite>end</cite> (inclusive).</p>
<p>Both <cite>start</cite> and <cite>end</cite> are relative positions from the current row.
For example, &#8220;0&#8221; means &#8220;current row&#8221;, while &#8220;-1&#8221; means the row before
the current row, and &#8220;5&#8221; means the fifth row after the current row.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>start</strong> &#8211; boundary start, inclusive.
The frame is unbounded if this is <tt class="docutils literal"><span class="pre">-sys.maxsize</span></tt> (or lower).</li>
<li><strong>end</strong> &#8211; boundary end, inclusive.
The frame is unbounded if this is <tt class="docutils literal"><span class="pre">sys.maxsize</span></tt> (or higher).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrameReader">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DataFrameReader</tt><big>(</big><em>sqlContext</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface used to load a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> from external storage systems
(e.g. file systems, key-value stores, etc). Use <a class="reference internal" href="#pyspark.sql.SQLContext.read" title="pyspark.sql.SQLContext.read"><tt class="xref py py-func docutils literal"><span class="pre">SQLContext.read()</span></tt></a>
to access this.</p>
<p>::Note: Experimental</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrameReader.format">
<tt class="descname">format</tt><big>(</big><em>source</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the input data source format.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>source</strong> &#8211; string, name of the data source, e.g. &#8216;json&#8217;, &#8216;parquet&#8217;.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">&#39;json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/people.json&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.jdbc">
<tt class="descname">jdbc</tt><big>(</big><em>url</em>, <em>table</em>, <em>column=None</em>, <em>lowerBound=None</em>, <em>upperBound=None</em>, <em>numPartitions=None</em>, <em>predicates=None</em>, <em>properties={}</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.jdbc" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> representing the database table accessible
via JDBC URL <cite>url</cite> named <cite>table</cite> and connection <cite>properties</cite>.</p>
<p>The <cite>column</cite> parameter could be used to partition the table, then it will
be retrieved in parallel based on the parameters passed to this function.</p>
<p>The <cite>predicates</cite> parameter gives a list expressions suitable for inclusion
in WHERE clauses; each one defines one partition of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<p>::Note: Don&#8217;t create too many partitions in parallel on a large cluster;
otherwise Spark might crash your external database systems.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>url</strong> &#8211; a JDBC URL</li>
<li><strong>table</strong> &#8211; name of table</li>
<li><strong>column</strong> &#8211; the column used to partition</li>
<li><strong>lowerBound</strong> &#8211; the lower bound of partition column</li>
<li><strong>upperBound</strong> &#8211; the upper bound of the partition column</li>
<li><strong>numPartitions</strong> &#8211; the number of partitions</li>
<li><strong>predicates</strong> &#8211; a list of expressions</li>
<li><strong>properties</strong> &#8211; JDBC database connection arguments, a list of arbitrary string
tag/value. Normally at least a &#8220;user&#8221; and &#8220;password&#8221; property
should be included.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">a DataFrame</p>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.json">
<tt class="descname">json</tt><big>(</big><em>path</em>, <em>schema=None</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.json" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a JSON file (one object per line) and returns the result as
a :class`DataFrame`.</p>
<p>If the <tt class="docutils literal"><span class="pre">schema</span></tt> parameter is not specified, this function goes
through the input once to determine the input schema.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; string, path to the JSON dataset.</li>
<li><strong>schema</strong> &#8211; an optional <tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt> for the input schema.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/people.json&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.load">
<tt class="descname">load</tt><big>(</big><em>path=None</em>, <em>format=None</em>, <em>schema=None</em>, <em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads data from a data source and returns it as a :class`DataFrame`.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; optional string for file-system backed data sources.</li>
<li><strong>format</strong> &#8211; optional string for format of the data source. Default to &#8216;parquet&#8217;.</li>
<li><strong>schema</strong> &#8211; optional <tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt> for the input schema.</li>
<li><strong>options</strong> &#8211; all other string options</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/parquet_partitioned&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;name&#39;, &#39;string&#39;), (&#39;year&#39;, &#39;int&#39;), (&#39;month&#39;, &#39;int&#39;), (&#39;day&#39;, &#39;int&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.options">
<tt class="descname">options</tt><big>(</big><em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.options" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds input options for the underlying data source.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.parquet">
<tt class="descname">parquet</tt><big>(</big><em>*path</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.parquet" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a Parquet file, returning the result as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/parquet_partitioned&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;name&#39;, &#39;string&#39;), (&#39;year&#39;, &#39;int&#39;), (&#39;month&#39;, &#39;int&#39;), (&#39;day&#39;, &#39;int&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.schema">
<tt class="descname">schema</tt><big>(</big><em>schema</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.schema" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the input schema.</p>
<p>Some data sources (e.g. JSON) can infer the input schema automatically from data.
By specifying the schema here, the underlying data source can skip the schema
inference step, and thus speed up data loading.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>schema</strong> &#8211; a StructType object</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameReader.table">
<tt class="descname">table</tt><big>(</big><em>tableName</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameReader.table" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the specified table as a <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>tableName</strong> &#8211; string, name of the table.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">&#39;python/test_support/sql/parquet_partitioned&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s">&#39;tmpTable&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqlContext</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s">&#39;tmpTable&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">dtypes</span>
<span class="go">[(&#39;name&#39;, &#39;string&#39;), (&#39;year&#39;, &#39;int&#39;), (&#39;month&#39;, &#39;int&#39;), (&#39;day&#39;, &#39;int&#39;)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.DataFrameWriter">
<em class="property">class </em><tt class="descclassname">pyspark.sql.</tt><tt class="descname">DataFrameWriter</tt><big>(</big><em>df</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter" title="Permalink to this definition">¶</a></dt>
<dd><p>Interface used to write a [[DataFrame]] to external storage systems
(e.g. file systems, key-value stores, etc). Use <a class="reference internal" href="#pyspark.sql.DataFrame.write" title="pyspark.sql.DataFrame.write"><tt class="xref py py-func docutils literal"><span class="pre">DataFrame.write()</span></tt></a>
to access this.</p>
<p>::Note: Experimental</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.format">
<tt class="descname">format</tt><big>(</big><em>source</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.format" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the underlying output data source.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>source</strong> &#8211; string, name of the data source, e.g. &#8216;json&#8217;, &#8216;parquet&#8217;.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s">&#39;json&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.insertInto">
<tt class="descname">insertInto</tt><big>(</big><em>tableName</em>, <em>overwrite=False</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.insertInto" title="Permalink to this definition">¶</a></dt>
<dd><p>Inserts the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to the specified table.</p>
<p>It requires that the schema of the class:<cite>DataFrame</cite> is the same as the
schema of the table.</p>
<p>Optionally overwriting any existing data.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.jdbc">
<tt class="descname">jdbc</tt><big>(</big><em>url</em>, <em>table</em>, <em>mode='error'</em>, <em>properties={}</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.jdbc" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to a external database table via JDBC.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Don&#8217;t create too many partitions in parallel on a large cluster;        otherwise Spark might crash your external database systems.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> &#8211; a JDBC URL of the form <tt class="docutils literal"><span class="pre">jdbc:subprotocol:subname</span></tt></li>
<li><strong>table</strong> &#8211; Name of the table in the external database.</li>
<li><strong>mode</strong> &#8211; <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><tt class="docutils literal"><span class="pre">append</span></tt>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><tt class="docutils literal"><span class="pre">overwrite</span></tt>: Overwrite existing data.</li>
<li><tt class="docutils literal"><span class="pre">ignore</span></tt>: Silently ignore this operation if data already exists.</li>
<li><tt class="docutils literal"><span class="pre">error</span></tt> (default case): Throw an exception if data already exists.</li>
</ul>
</li>
<li><strong>properties</strong> &#8211; JDBC database connection arguments, a list of
arbitrary string tag/value. Normally at least a
&#8220;user&#8221; and &#8220;password&#8221; property should be included.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.json">
<tt class="descname">json</tt><big>(</big><em>path</em>, <em>mode='error'</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.json" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> in JSON format at the specified path.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; the path in any Hadoop supported file system</li>
<li><strong>mode</strong> &#8211; <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><tt class="docutils literal"><span class="pre">append</span></tt>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><tt class="docutils literal"><span class="pre">overwrite</span></tt>: Overwrite existing data.</li>
<li><tt class="docutils literal"><span class="pre">ignore</span></tt>: Silently ignore this operation if data already exists.</li>
<li><tt class="docutils literal"><span class="pre">error</span></tt> (default case): Throw an exception if data already exists.</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.mode">
<tt class="descname">mode</tt><big>(</big><em>saveMode</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.mode" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies the behavior when data or table already exists.</p>
<p>Options include:</p>
<ul class="simple">
<li><cite>append</cite>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><cite>overwrite</cite>: Overwrite existing data.</li>
<li><cite>error</cite>: Throw an exception if data already exists.</li>
<li><cite>ignore</cite>: Silently ignore this operation if data already exists.</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s">&#39;append&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.options">
<tt class="descname">options</tt><big>(</big><em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.options" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds output options for the underlying data source.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.parquet">
<tt class="descname">parquet</tt><big>(</big><em>path</em>, <em>mode='error'</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.parquet" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> in Parquet format at the specified path.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; the path in any Hadoop supported file system</li>
<li><strong>mode</strong> &#8211; <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><tt class="docutils literal"><span class="pre">append</span></tt>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><tt class="docutils literal"><span class="pre">overwrite</span></tt>: Overwrite existing data.</li>
<li><tt class="docutils literal"><span class="pre">ignore</span></tt>: Silently ignore this operation if data already exists.</li>
<li><tt class="docutils literal"><span class="pre">error</span></tt> (default case): Throw an exception if data already exists.</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.partitionBy">
<tt class="descname">partitionBy</tt><big>(</big><em>*cols</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.partitionBy" title="Permalink to this definition">¶</a></dt>
<dd><p>Partitions the output by the given columns on the file system.</p>
<p>If specified, the output is laid out on the file system similar
to Hive&#8217;s partitioning scheme.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; name of columns</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s">&#39;year&#39;</span><span class="p">,</span> <span class="s">&#39;month&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.save">
<tt class="descname">save</tt><big>(</big><em>path=None</em>, <em>format=None</em>, <em>mode='error'</em>, <em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the contents of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to a data source.</p>
<p>The data source is specified by the <tt class="docutils literal"><span class="pre">format</span></tt> and a set of <tt class="docutils literal"><span class="pre">options</span></tt>.
If <tt class="docutils literal"><span class="pre">format</span></tt> is not specified, the default data source configured by
<tt class="docutils literal"><span class="pre">spark.sql.sources.default</span></tt> will be used.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path</strong> &#8211; the path in a Hadoop supported file system</li>
<li><strong>format</strong> &#8211; the format used to save</li>
<li><strong>mode</strong> &#8211; <p>specifies the behavior of the save operation when data already exists.</p>
<ul>
<li><tt class="docutils literal"><span class="pre">append</span></tt>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><tt class="docutils literal"><span class="pre">overwrite</span></tt>: Overwrite existing data.</li>
<li><tt class="docutils literal"><span class="pre">ignore</span></tt>: Silently ignore this operation if data already exists.</li>
<li><tt class="docutils literal"><span class="pre">error</span></tt> (default case): Throw an exception if data already exists.</li>
</ul>
</li>
<li><strong>options</strong> &#8211; all other string options</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s">&#39;append&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(),</span> <span class="s">&#39;data&#39;</span><span class="p">))</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="method">
<dt id="pyspark.sql.DataFrameWriter.saveAsTable">
<tt class="descname">saveAsTable</tt><big>(</big><em>name</em>, <em>format=None</em>, <em>mode='error'</em>, <em>**options</em><big>)</big><a class="headerlink" href="#pyspark.sql.DataFrameWriter.saveAsTable" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the content of the <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> as the specified table.</p>
<p>In the case the table already exists, behavior of this function depends on the
save mode, specified by the <cite>mode</cite> function (default to throwing an exception).
When <cite>mode</cite> is <cite>Overwrite</cite>, the schema of the [[DataFrame]] does not need to be
the same as that of the existing table.</p>
<ul class="simple">
<li><cite>append</cite>: Append contents of this <a class="reference internal" href="#pyspark.sql.DataFrame" title="pyspark.sql.DataFrame"><tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt></a> to existing data.</li>
<li><cite>overwrite</cite>: Overwrite existing data.</li>
<li><cite>error</cite>: Throw an exception if data already exists.</li>
<li><cite>ignore</cite>: Silently ignore this operation if data already exists.</li>
</ul>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> &#8211; the table name</li>
<li><strong>format</strong> &#8211; the format used to save</li>
<li><strong>mode</strong> &#8211; one of <cite>append</cite>, <cite>overwrite</cite>, <cite>error</cite>, <cite>ignore</cite> (default: error)</li>
<li><strong>options</strong> &#8211; all other string options</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pyspark.sql.types">
<span id="pyspark-sql-types-module"></span><h2>pyspark.sql.types module<a class="headerlink" href="#module-pyspark.sql.types" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pyspark.sql.types.DataType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">DataType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for data types.</p>
<dl class="method">
<dt id="pyspark.sql.types.DataType.json">
<tt class="descname">json</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.json"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.json" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DataType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="classmethod">
<dt id="pyspark.sql.types.DataType.typeName">
<em class="property">classmethod </em><tt class="descname">typeName</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DataType.typeName"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DataType.typeName" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.NullType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">NullType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#NullType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.NullType" title="Permalink to this definition">¶</a></dt>
<dd><p>Null type.</p>
<p>The data type representing None, used for the types that cannot be inferred.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.StringType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">StringType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#StringType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StringType" title="Permalink to this definition">¶</a></dt>
<dd><p>String data type.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.BinaryType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">BinaryType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#BinaryType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.BinaryType" title="Permalink to this definition">¶</a></dt>
<dd><p>Binary (byte array) data type.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.BooleanType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">BooleanType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#BooleanType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.BooleanType" title="Permalink to this definition">¶</a></dt>
<dd><p>Boolean data type.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.DateType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">DateType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#DateType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DateType" title="Permalink to this definition">¶</a></dt>
<dd><p>Date (datetime.date) data type.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.TimestampType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">TimestampType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#TimestampType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.TimestampType" title="Permalink to this definition">¶</a></dt>
<dd><p>Timestamp (datetime.datetime) data type.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.DecimalType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">DecimalType</tt><big>(</big><em>precision=None</em>, <em>scale=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DecimalType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DecimalType" title="Permalink to this definition">¶</a></dt>
<dd><p>Decimal (decimal.Decimal) data type.</p>
<dl class="method">
<dt id="pyspark.sql.types.DecimalType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DecimalType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DecimalType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.DecimalType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#DecimalType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DecimalType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.DoubleType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">DoubleType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#DoubleType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.DoubleType" title="Permalink to this definition">¶</a></dt>
<dd><p>Double data type, representing double precision floats.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.FloatType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">FloatType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#FloatType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.FloatType" title="Permalink to this definition">¶</a></dt>
<dd><p>Float data type, representing single precision floats.</p>
</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.ByteType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">ByteType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#ByteType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ByteType" title="Permalink to this definition">¶</a></dt>
<dd><p>Byte data type, i.e. a signed integer in a single byte.</p>
<dl class="method">
<dt id="pyspark.sql.types.ByteType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ByteType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ByteType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.IntegerType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">IntegerType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#IntegerType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.IntegerType" title="Permalink to this definition">¶</a></dt>
<dd><p>Int data type, i.e. a signed 32-bit integer.</p>
<dl class="method">
<dt id="pyspark.sql.types.IntegerType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#IntegerType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.IntegerType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.LongType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">LongType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#LongType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.LongType" title="Permalink to this definition">¶</a></dt>
<dd><p>Long data type, i.e. a signed 64-bit integer.</p>
<p>If the values are beyond the range of [-9223372036854775808, 9223372036854775807],
please use <a class="reference internal" href="#pyspark.sql.types.DecimalType" title="pyspark.sql.types.DecimalType"><tt class="xref py py-class docutils literal"><span class="pre">DecimalType</span></tt></a>.</p>
<dl class="method">
<dt id="pyspark.sql.types.LongType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#LongType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.LongType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.ShortType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">ShortType</tt><a class="reference internal" href="_modules/pyspark/sql/types.html#ShortType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ShortType" title="Permalink to this definition">¶</a></dt>
<dd><p>Short data type, i.e. a signed 16-bit integer.</p>
<dl class="method">
<dt id="pyspark.sql.types.ShortType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ShortType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ShortType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.ArrayType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">ArrayType</tt><big>(</big><em>elementType</em>, <em>containsNull=True</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType" title="Permalink to this definition">¶</a></dt>
<dd><p>Array data type.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>elementType</strong> &#8211; <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt></a> of each element in the array.</li>
<li><strong>containsNull</strong> &#8211; boolean, whether the array can contain null (None) values.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="classmethod">
<dt id="pyspark.sql.types.ArrayType.fromJson">
<em class="property">classmethod </em><tt class="descname">fromJson</tt><big>(</big><em>json</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.ArrayType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.ArrayType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#ArrayType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.ArrayType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.MapType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">MapType</tt><big>(</big><em>keyType</em>, <em>valueType</em>, <em>valueContainsNull=True</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType" title="Permalink to this definition">¶</a></dt>
<dd><p>Map data type.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>keyType</strong> &#8211; <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt></a> of the keys in the map.</li>
<li><strong>valueType</strong> &#8211; <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt></a> of the values in the map.</li>
<li><strong>valueContainsNull</strong> &#8211; indicates whether values can contain null (None) values.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Keys in a map data type are not allowed to be null (None).</p>
<dl class="classmethod">
<dt id="pyspark.sql.types.MapType.fromJson">
<em class="property">classmethod </em><tt class="descname">fromJson</tt><big>(</big><em>json</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.MapType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.MapType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#MapType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.MapType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.StructField">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">StructField</tt><big>(</big><em>name</em>, <em>dataType</em>, <em>nullable=True</em>, <em>metadata=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField" title="Permalink to this definition">¶</a></dt>
<dd><p>A field in <a class="reference internal" href="#pyspark.sql.types.StructType" title="pyspark.sql.types.StructType"><tt class="xref py py-class docutils literal"><span class="pre">StructType</span></tt></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> &#8211; string, name of the field.</li>
<li><strong>dataType</strong> &#8211; <a class="reference internal" href="#pyspark.sql.types.DataType" title="pyspark.sql.types.DataType"><tt class="xref py py-class docutils literal"><span class="pre">DataType</span></tt></a> of the field.</li>
<li><strong>nullable</strong> &#8211; boolean, whether the field can be null (None) or not.</li>
<li><strong>metadata</strong> &#8211; a dict from string to simple type that can be serialized to JSON automatically</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="classmethod">
<dt id="pyspark.sql.types.StructField.fromJson">
<em class="property">classmethod </em><tt class="descname">fromJson</tt><big>(</big><em>json</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructField.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructField.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructField.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructField.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pyspark.sql.types.StructType">
<em class="property">class </em><tt class="descclassname">pyspark.sql.types.</tt><tt class="descname">StructType</tt><big>(</big><em>fields</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType" title="Permalink to this definition">¶</a></dt>
<dd><p>Struct type, consisting of a list of <a class="reference internal" href="#pyspark.sql.types.StructField" title="pyspark.sql.types.StructField"><tt class="xref py py-class docutils literal"><span class="pre">StructField</span></tt></a>.</p>
<p>This is the data type representing a <tt class="xref py py-class docutils literal"><span class="pre">Row</span></tt>.</p>
<dl class="classmethod">
<dt id="pyspark.sql.types.StructType.fromJson">
<em class="property">classmethod </em><tt class="descname">fromJson</tt><big>(</big><em>json</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.fromJson"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.fromJson" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.jsonValue">
<tt class="descname">jsonValue</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.jsonValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.jsonValue" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pyspark.sql.types.StructType.simpleString">
<tt class="descname">simpleString</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/types.html#StructType.simpleString"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.types.StructType.simpleString" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pyspark.sql.functions">
<span id="pyspark-sql-functions-module"></span><h2>pyspark.sql.functions module<a class="headerlink" href="#module-pyspark.sql.functions" title="Permalink to this headline">¶</a></h2>
<p>A collections of builtin functions</p>
<dl class="function">
<dt id="pyspark.sql.functions.abs">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">abs</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.abs" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the absolute value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.acos">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">acos</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.acos" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the cosine inverse of the given value; the returned angle is in the range0.0 through pi.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.approxCountDistinct">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">approxCountDistinct</tt><big>(</big><em>col</em>, <em>rsd=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#approxCountDistinct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.approxCountDistinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> for approximate distinct count of <tt class="docutils literal"><span class="pre">col</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">approxCountDistinct</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.array">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">array</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#array"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.array" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new array column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string) or list of <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> expressions that have
the same data type.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;age&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;arr&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(arr=[2, 2]), Row(arr=[5, 5])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">array</span><span class="p">([</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">])</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;arr&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(arr=[2, 2]), Row(arr=[5, 5])]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.asc">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">asc</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.asc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the ascending order of the given column name.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.asin">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">asin</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.asin" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the sine inverse of the given value; the returned angle is in the range-pi/2 through pi/2.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.atan">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">atan</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.atan" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the tangent inverse of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.atan2">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">atan2</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.atan2" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the angle theta from the conversion of rectangular coordinates (x, y) topolar coordinates (r, theta).</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.avg">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">avg</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.avg" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the average of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.bitwiseNOT">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">bitwiseNOT</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.bitwiseNOT" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes bitwise not.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.cbrt">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">cbrt</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.cbrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the cube-root of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.ceil">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">ceil</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.ceil" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the ceiling of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.coalesce">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">coalesce</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#coalesce"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.coalesce" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first column that is not null.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cDf</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span> <span class="p">(</span><span class="s">&quot;a&quot;</span><span class="p">,</span> <span class="s">&quot;b&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cDf</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+----+</span>
<span class="go">|   a|   b|</span>
<span class="go">+----+----+</span>
<span class="go">|null|null|</span>
<span class="go">|   1|null|</span>
<span class="go">|null|   2|</span>
<span class="go">+----+----+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cDf</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">coalesce</span><span class="p">(</span><span class="n">cDf</span><span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">],</span> <span class="n">cDf</span><span class="p">[</span><span class="s">&quot;b&quot;</span><span class="p">]))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+-------------+</span>
<span class="go">|Coalesce(a,b)|</span>
<span class="go">+-------------+</span>
<span class="go">|         null|</span>
<span class="go">|            1|</span>
<span class="go">|            2|</span>
<span class="go">+-------------+</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cDf</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&#39;*&#39;</span><span class="p">,</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">cDf</span><span class="p">[</span><span class="s">&quot;a&quot;</span><span class="p">],</span> <span class="n">lit</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+----+----+---------------+</span>
<span class="go">|   a|   b|Coalesce(a,0.0)|</span>
<span class="go">+----+----+---------------+</span>
<span class="go">|null|null|            0.0|</span>
<span class="go">|   1|null|            1.0|</span>
<span class="go">|null|   2|            0.0|</span>
<span class="go">+----+----+---------------+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.col">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">col</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.col" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> based on the given column name.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.column">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">column</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.column" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> based on the given column name.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.cos">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">cos</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.cos" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the cosine of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.cosh">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">cosh</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.cosh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the hyperbolic cosine of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.count">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">count</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the number of items in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.countDistinct">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">countDistinct</tt><big>(</big><em>col</em>, <em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#countDistinct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.countDistinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> for distinct count of <tt class="docutils literal"><span class="pre">col</span></tt> or <tt class="docutils literal"><span class="pre">cols</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">countDistinct</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=2)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">countDistinct</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="s">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;c&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(c=2)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.cumeDist">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">cumeDist</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.functions.cumeDist" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the cumulative distribution of values within a window partition,
i.e. the fraction of rows that are below the current row.</p>
<p>This is equivalent to the CUME_DIST function in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.denseRank">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">denseRank</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.functions.denseRank" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the rank of rows within a window partition, without any gaps.</p>
<p>The difference between rank and denseRank is that denseRank leaves no gaps in ranking
sequence when there are ties. That is, if you were ranking a competition using denseRank
and had three people tie for second place, you would say that all three were in second
place and that the next person came in third.</p>
<p>This is equivalent to the DENSE_RANK function in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.desc">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">desc</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.desc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a sort expression based on the descending order of the given column name.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.exp">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">exp</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.exp" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the exponential of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.explode">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">explode</tt><big>(</big><em>col</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#explode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.explode" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new row for each element in the given array or map.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">intlist</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">mapfield</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;a&quot;</span><span class="p">:</span> <span class="s">&quot;b&quot;</span><span class="p">})])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">explode</span><span class="p">(</span><span class="n">eDF</span><span class="o">.</span><span class="n">intlist</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;anInt&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(anInt=1), Row(anInt=2), Row(anInt=3)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">eDF</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">explode</span><span class="p">(</span><span class="n">eDF</span><span class="o">.</span><span class="n">mapfield</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;key&quot;</span><span class="p">,</span> <span class="s">&quot;value&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">+---+-----+</span>
<span class="go">|key|value|</span>
<span class="go">+---+-----+</span>
<span class="go">|  a|    b|</span>
<span class="go">+---+-----+</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.expm1">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">expm1</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.expm1" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the exponential of the given value minus one.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.first">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">first</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.first" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the first value in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.floor">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">floor</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.floor" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the floor of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.hypot">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">hypot</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.hypot" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes <cite>sqrt(a^2^ + b^2^)</cite> without intermediate overflow or underflow.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lag">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">lag</tt><big>(</big><em>col</em>, <em>count=1</em>, <em>default=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#lag"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.lag" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the value that is <cite>offset</cite> rows before the current row, and
<cite>defaultValue</cite> if there is less than <cite>offset</cite> rows before the current row. For example,
an <cite>offset</cite> of one will return the previous row at any given point in the window partition.</p>
<p>This is equivalent to the LAG function in SQL.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col</strong> &#8211; name of column or expression</li>
<li><strong>count</strong> &#8211; number of row to extend</li>
<li><strong>default</strong> &#8211; default value</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.last">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">last</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.last" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the last value in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lead">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">lead</tt><big>(</big><em>col</em>, <em>count=1</em>, <em>default=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#lead"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.lead" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the value that is <cite>offset</cite> rows after the current row, and
<cite>defaultValue</cite> if there is less than <cite>offset</cite> rows after the current row. For example,
an <cite>offset</cite> of one will return the next row at any given point in the window partition.</p>
<p>This is equivalent to the LEAD function in SQL.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>col</strong> &#8211; name of column or expression</li>
<li><strong>count</strong> &#8211; number of row to extend</li>
<li><strong>default</strong> &#8211; default value</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lit">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">lit</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.lit" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> of literal value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.log">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">log</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the natural logarithm of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.log10">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">log10</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.log10" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the logarithm of the given value in Base 10.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.log1p">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">log1p</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.log1p" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the natural logarithm of the given value plus one.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.lower">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">lower</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.lower" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a string expression to upper case.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.max">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">max</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.max" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the maximum value of the expression in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.mean">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">mean</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the average of the values in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.min">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">min</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.min" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the minimum value of the expression in a group.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.monotonicallyIncreasingId">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">monotonicallyIncreasingId</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#monotonicallyIncreasingId"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.monotonicallyIncreasingId" title="Permalink to this definition">¶</a></dt>
<dd><p>A column that generates monotonically increasing 64-bit integers.</p>
<p>The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.
The current implementation puts the partition ID in the upper 31 bits, and the record number
within each partition in the lower 33 bits. The assumption is that the data frame has
less than 1 billion partitions, and each partition has less than 8 billion records.</p>
<p>As an example, consider a <tt class="xref py py-class docutils literal"><span class="pre">DataFrame</span></tt> with two partitions, each with 3 records.
This expression would return the following IDs:
0, 1, 2, 8589934592 (1L &lt;&lt; 33), 8589934593, 8589934594.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df0</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,)])</span><span class="o">.</span><span class="n">toDF</span><span class="p">([</span><span class="s">&#39;col1&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df0</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">monotonicallyIncreasingId</span><span class="p">()</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;id&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.ntile">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">ntile</tt><big>(</big><em>n</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#ntile"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.ntile" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns a group id from 1 to <cite>n</cite> (inclusive) in a round-robin fashion in
a window partition. Fow example, if <cite>n</cite> is 3, the first row will get 1, the second row will
get 2, the third row will get 3, and the fourth row will get 1...</p>
<p>This is equivalent to the NTILE function in SQL.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>n</strong> &#8211; an integer</td>
</tr>
</tbody>
</table>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.percentRank">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">percentRank</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.functions.percentRank" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the relative rank (i.e. percentile) of rows within a window partition.</p>
<p>This is equivalent to the PERCENT_RANK function in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.pow">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">pow</tt><big>(</big><em>col1</em>, <em>col2</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.pow" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the value of the first argument raised to the power of the second argument.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rand">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">rand</tt><big>(</big><em>seed=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#rand"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.rand" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a random column with i.i.d. samples from U[0.0, 1.0].</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.randn">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">randn</tt><big>(</big><em>seed=None</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#randn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.randn" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a column with i.i.d. samples from the standard normal distribution.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rank">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">rank</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.functions.rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns the rank of rows within a window partition.</p>
<p>The difference between rank and denseRank is that denseRank leaves no gaps in ranking
sequence when there are ties. That is, if you were ranking a competition using denseRank
and had three people tie for second place, you would say that all three were in second
place and that the next person came in third.</p>
<p>This is equivalent to the RANK function in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rint">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">rint</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.rint" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the double value that is closest in value to the argument and is equal to a mathematical integer.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.rowNumber">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">rowNumber</tt><big>(</big><big>)</big><a class="headerlink" href="#pyspark.sql.functions.rowNumber" title="Permalink to this definition">¶</a></dt>
<dd><p>Window function: returns a sequential number starting at 1 within a window partition.</p>
<p>This is equivalent to the ROW_NUMBER function in SQL.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.signum">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">signum</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.signum" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the signum of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sin">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sin</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.sin" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the sine of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sinh">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sinh</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.sinh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the hyperbolic sine of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sparkPartitionId">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sparkPartitionId</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#sparkPartitionId"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.sparkPartitionId" title="Permalink to this definition">¶</a></dt>
<dd><p>A column for partition ID of the Spark task.</p>
<p>Note that this is indeterministic because it depends on data partitioning and task scheduling.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">sparkPartitionId</span><span class="p">()</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;pid&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(pid=0), Row(pid=0)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sqrt">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sqrt</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.sqrt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the square root of the specified float value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.struct">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">struct</tt><big>(</big><em>*cols</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#struct"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.struct" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new struct column.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>cols</strong> &#8211; list of column names (string) or list of <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> expressions
that are named or aliased.</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">struct</span><span class="p">(</span><span class="s">&#39;age&#39;</span><span class="p">,</span> <span class="s">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;struct&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(struct=Row(age=2, name=u&#39;Alice&#39;)), Row(struct=Row(age=5, name=u&#39;Bob&#39;))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">struct</span><span class="p">([</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">])</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;struct&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(struct=Row(age=2, name=u&#39;Alice&#39;)), Row(struct=Row(age=5, name=u&#39;Bob&#39;))]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sum">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sum</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.sum" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the sum of all values in the expression.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.sumDistinct">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">sumDistinct</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.sumDistinct" title="Permalink to this definition">¶</a></dt>
<dd><p>Aggregate function: returns the sum of distinct values in the expression.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.tan">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">tan</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.tan" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the tangent of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.tanh">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">tanh</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the hyperbolic tangent of the given value.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.toDegrees">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">toDegrees</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.toDegrees" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an angle measured in radians to an approximately equivalent angle measured in degrees.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.toRadians">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">toRadians</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.toRadians" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts an angle measured in degrees to an approximately equivalent angle measured in radians.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.udf">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">udf</tt><big>(</big><em>f</em>, <em>returnType=StringType</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#udf"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.udf" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> expression representing a user defined function (UDF).</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">IntegerType</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slen</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">IntegerType</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">slen</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">name</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&#39;slen&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(slen=5), Row(slen=3)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.upper">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">upper</tt><big>(</big><em>col</em><big>)</big><a class="headerlink" href="#pyspark.sql.functions.upper" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a string expression to upper case.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.</span></p>
</div>
</dd></dl>

<dl class="function">
<dt id="pyspark.sql.functions.when">
<tt class="descclassname">pyspark.sql.functions.</tt><tt class="descname">when</tt><big>(</big><em>condition</em>, <em>value</em><big>)</big><a class="reference internal" href="_modules/pyspark/sql/functions.html#when"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pyspark.sql.functions.when" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates a list of conditions and returns one of multiple possible result expressions.
If <tt class="xref py py-func docutils literal"><span class="pre">Column.otherwise()</span></tt> is not invoked, None is returned for unmatched conditions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>condition</strong> &#8211; a boolean <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> expression.</li>
<li><strong>value</strong> &#8211; a literal value, or a <tt class="xref py py-class docutils literal"><span class="pre">Column</span></tt> expression.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&#39;age&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=3), Row(age=4)]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">age</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="go">[Row(age=3), Row(age=None)]</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/spark-logo-hd.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">pyspark.sql module</a><ul>
<li><a class="reference internal" href="#module-pyspark.sql">Module Context</a></li>
<li><a class="reference internal" href="#module-pyspark.sql.types">pyspark.sql.types module</a></li>
<li><a class="reference internal" href="#module-pyspark.sql.functions">pyspark.sql.functions module</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="pyspark.mllib.html"
                        title="previous chapter">pyspark.mllib package</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="pyspark.streaming.html"
                        title="next chapter">pyspark.streaming module</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/pyspark.sql.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="pyspark.streaming.html" title="pyspark.streaming module"
             >next</a></li>
        <li class="right" >
          <a href="pyspark.mllib.html" title="pyspark.mllib package"
             >previous</a> |</li>
        <li><a href="index.html">PySpark 1.4.0-SNAPSHOT documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright .
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
  </body>
</html>